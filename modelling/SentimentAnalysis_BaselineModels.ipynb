{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old labels\n",
    "#path = \"\"\n",
    "path =\"old_labels/\"\n",
    "test = pd.read_csv(path + \"test_subset.csv\")\n",
    "train_1 = pd.read_csv(path + \"train_first_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_2 = pd.read_csv(path + \"train_second_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_3 = pd.read_csv(path + \"train_third_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_1[\"label\"] = pd.to_numeric(train_1[\"label\"], errors='coerce')\n",
    "train = pd.concat([train_1,train_2,train_3],axis=0)\n",
    "val = pd.read_csv(path + \"val_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "\n",
    "test = test.reset_index()\n",
    "train=train.reset_index()\n",
    "val = val.reset_index()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new labels\n",
    "path = \"new_labels/\"\n",
    "test = pd.read_csv(path + \"test_subset.csv\")\n",
    "test[\"label\"] = pd.to_numeric(test[\"label\"], errors='coerce')\n",
    "train_1 = pd.read_csv(path + \"train_first_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_2 = pd.read_csv(path + \"train_second_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "extra_neg = pd.read_csv(path + \"extra_neg.csv\")[[\"Unnamed: 0\",'restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "val = pd.read_csv(path + \"val_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "\n",
    "index = extra_neg.loc[extra_neg[\"Unnamed: 0\"] == 0].index\n",
    "\n",
    "test = pd.concat([test,extra_neg.iloc[:index[1],:]],axis=0)\n",
    "train = pd.concat([train_1,train_2,extra_neg.iloc[index[1] + 1:index[2],:]],axis=0)\n",
    "val = pd.concat([val,extra_neg.iloc[index[2] + 1:,:]],axis=0)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data in [train_1,train_2,train_3,train,val,test]:\n",
    "    print(\"Total size = {}\".format(data.shape[0]))\n",
    "    print(\"Noise = {} \".format(data.loc[(data.label == 0.0) & (data.new_aspect_1.isnull()) & (data.new_aspect_2.isnull())].shape[0]))\n",
    "    print(\"Aspect with no adjectives = {} \".format(data.loc[(data.new_aspect_1.notnull()) & (data.new_aspect_2.isnull())].shape[0]))\n",
    "    print(\"Aspect with adjectives = {} \".format(data.loc[(data.new_aspect_1.notnull()) & (data.new_aspect_2.notnull())].shape[0]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Data Preprocessing\n",
    "\n",
    "1. Clean Phrase (Utils) \n",
    "2. Filter the phrase column where there is an empty string \n",
    "3. Combine the aspects into 1 & aggregate the sentiment scores by averaging then sign(>0 = 1,=0 =0 <0 = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean phrases\n",
    "train[\"phrase_lemma\"] = train.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "val[\"phrase_lemma\"] = val.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "test[\"phrase_lemma\"] = test.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "\n",
    "train[\"phrase_stem\"] = train.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "val[\"phrase_stem\"] = val.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "test[\"phrase_stem\"] = test.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "\n",
    "train.phrase = train.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))\n",
    "val.phrase = val.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))\n",
    "test.phrase = test.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows where there are no characters\n",
    "train = train.loc[(train.phrase.str.len() > 0)]\n",
    "val = val.loc[(val.phrase.str.len() > 0)]\n",
    "test = test.loc[(test.phrase.str.len() > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if nan label, replace with 0\n",
    "train.label = train.label.apply(lambda x: 0 if np.isnan(x) else x)\n",
    "val.label = val.label.apply(lambda x: 0 if np.isnan(x) else x)\n",
    "test.label = test.label.apply(lambda x: 0 if np.isnan(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgn(x):\n",
    "    if x < 0:\n",
    "        return -1\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out phrases with hashtags & those with character length <=5 then get the majority (might not make sense)\n",
    "# train\n",
    "trainagg = train.groupby([\"restaurant_code\",\"review_title\",\"account_name\",\"new_aspect_1\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                                                \"phrase_lemma\": \" \".join,\n",
    "                                                                                                \"phrase_stem\":\" \".join,\n",
    "                                                                                                \"label\": \"mean\"})\n",
    "trainagg = trainagg.reset_index()\n",
    "trainagg.label = trainagg.label.map(sgn)\n",
    "\n",
    "\n",
    "# test\n",
    "valagg = val.groupby([\"restaurant_code\",\"review_title\",\"account_name\",\"new_aspect_1\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                                                \"phrase_lemma\": \" \".join,\n",
    "                                                                                                \"phrase_stem\":\" \".join,\n",
    "                                                                                                \"label\": \"mean\"})\n",
    "valagg = valagg.reset_index()\n",
    "valagg.label = valagg.label.map(sgn)\n",
    "\n",
    "#val \n",
    "testagg = test.groupby([\"restaurant_code\",\"review_title\",\"account_name\",\"new_aspect_1\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                                                \"phrase_lemma\": \" \".join,\n",
    "                                                                                                \"phrase_stem\":\" \".join,\n",
    "                                                                                                \"label\": \"mean\"})\n",
    "testagg = testagg.reset_index()\n",
    "testagg.label = testagg.label.map(sgn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvalagg = pd.concat([trainagg, valagg],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "trainagg.to_csv(path + \"train_newpreproc.csv\")\n",
    "valagg.to_csv(path + \"val_newpreproc.csv\")\n",
    "testagg.to_csv(path + \"test_newpreproc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Text Vectorisation --> used on data2\n",
    "1. BoW\n",
    "2. Tf-idf\n",
    "3. Fasttext - can handle oov words https://www.quora.com/How-does-fastText-output-a-vector-for-a-word-that-is-not-in-the-pre-trained-model (not used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainagg = pd.read_csv(\"train_newpreproc.csv\")\n",
    "valagg = pd.read_csv(\"val_newpreproc.csv\")\n",
    "testagg = pd.read_csv(\"test_newpreproc.csv\")\n",
    "trainvalagg = pd.concat([trainagg, valagg],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 3))\n",
    "bow_train = bow.fit_transform(trainvalagg.phrase)\n",
    "bow_test = bow.transform(testagg.phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 3))\n",
    "tfidf_train = tfidf.fit_transform(trainvalagg.phrase)\n",
    "tfidf_test = tfidf.transform(testagg.phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Preprocess data for fasttext word embeddings training\n",
    "train_clean = [process_text(sentence) for sentence in tqdm(trainvalagg.phrase) if sentence.strip() !='']\n",
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "train_word_tokens = [word_tokenizer.tokenize(sent) for sent in tqdm(train_clean)]\n",
    "\n",
    "# Preprocess data for fasttext word embeddings testing data\n",
    "test_clean = [process_text(sentence) for sentence in tqdm(test2.phrase) if sentence.strip() !='']\n",
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "test_word_tokens = [word_tokenizer.tokenize(sent) for sent in tqdm(test_clean)]\n",
    "\n",
    "# Train Fasttext word embeddings on training data\n",
    "# Defining values for parameters\n",
    "embedding_size = 300\n",
    "window_size = 5\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n",
    " \n",
    "\n",
    "fast_Text_model = FastText(train_word_tokens,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      workers = 4,\n",
    "                      sg=1,\n",
    "                      iter=100)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Save fastText gensim model\n",
    "fast_Text_model.save(\"ftmodel/ft_model_train\")\n",
    "# Load saved gensim fastText model\n",
    "fast_Text_model = Word2Vec.load(\"ftmodel/ft_model_train\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# convert sentence --> average(each word embedding)\n",
    "fasttext_train = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in train_word_tokens]\n",
    "# convert sentence --> average(each word embedding)\n",
    "fasttext_test = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in test_word_tokens]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fasttext_train_processed = []\n",
    "for row in fasttext_train:\n",
    "    try:\n",
    "        fasttext_train_processed.append(list(row))\n",
    "    except:\n",
    "        fasttext_train_processed.append([0]*300)\n",
    "\n",
    "fasttext_test_processed = []\n",
    "for row in fasttext_test:\n",
    "    try:\n",
    "        fasttext_test_processed.append(list(row))\n",
    "    except:\n",
    "        fasttext_test_processed.append([0]*300)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fasttext_train_processed = np.array(fasttext_train_processed).reshape(-1,300)\n",
    "fasttext_test_processed = np.array(fasttext_test_processed).reshape(-1,300)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelling\n",
    "\n",
    "1. Naive Bayes/Support Vector Machines (NBSVM)/Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Baseline) 1. Naive Bayes -Support Vector Machines (NBSVM)- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.96      0.98       333\n",
      "           0       0.98      0.99      0.99       917\n",
      "           1       0.99      0.99      0.99      1236\n",
      "\n",
      "    accuracy                           0.99      2486\n",
      "   macro avg       0.99      0.98      0.99      2486\n",
      "weighted avg       0.99      0.99      0.99      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.7021    0.3173    0.4371       104\n",
      "           0     0.5000    0.7982    0.6148       218\n",
      "           1     0.8368    0.7020    0.7635       453\n",
      "\n",
      "    accuracy                         0.6774       775\n",
      "   macro avg     0.6797    0.6058    0.6051       775\n",
      "weighted avg     0.7240    0.6774    0.6779       775\n",
      "\n",
      "SVM + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.97      0.66      0.78       333\n",
      "           0       0.79      0.97      0.87       917\n",
      "           1       0.97      0.89      0.93      1236\n",
      "\n",
      "    accuracy                           0.89      2486\n",
      "   macro avg       0.91      0.84      0.86      2486\n",
      "weighted avg       0.91      0.89      0.89      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.6923    0.1731    0.2769       104\n",
      "           0     0.4176    0.8945    0.5693       218\n",
      "           1     0.8511    0.5298    0.6531       453\n",
      "\n",
      "    accuracy                         0.5845       775\n",
      "   macro avg     0.6536    0.5325    0.4998       775\n",
      "weighted avg     0.7078    0.5845    0.5790       775\n",
      "\n",
      "Naive Bayes + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.97      0.93      0.95       333\n",
      "           0       0.98      0.93      0.96       917\n",
      "           1       0.94      0.99      0.97      1236\n",
      "\n",
      "    accuracy                           0.96      2486\n",
      "   macro avg       0.97      0.95      0.96      2486\n",
      "weighted avg       0.96      0.96      0.96      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9167    0.2115    0.3437       104\n",
      "           0     0.5103    0.3394    0.4077       218\n",
      "           1     0.6617    0.8852    0.7573       453\n",
      "\n",
      "    accuracy                         0.6413       775\n",
      "   macro avg     0.6962    0.4787    0.5029       775\n",
      "weighted avg     0.6533    0.6413    0.6035       775\n",
      "\n",
      "Logistic Regression + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.52      0.68       333\n",
      "           0       0.92      0.97      0.95       917\n",
      "           1       0.91      1.00      0.95      1236\n",
      "\n",
      "    accuracy                           0.92      2486\n",
      "   macro avg       0.94      0.83      0.86      2486\n",
      "weighted avg       0.93      0.92      0.91      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.7812    0.2404    0.3676       104\n",
      "           0     0.5314    0.5826    0.5558       218\n",
      "           1     0.7361    0.8190    0.7753       453\n",
      "\n",
      "    accuracy                         0.6748       775\n",
      "   macro avg     0.6829    0.5473    0.5663       775\n",
      "weighted avg     0.6846    0.6748    0.6589       775\n",
      "\n",
      "SVM + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.96      0.98       333\n",
      "           0       0.99      0.99      0.99       917\n",
      "           1       0.99      1.00      0.99      1236\n",
      "\n",
      "    accuracy                           0.99      2486\n",
      "   macro avg       0.99      0.98      0.99      2486\n",
      "weighted avg       0.99      0.99      0.99      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.8462    0.2115    0.3385       104\n",
      "           0     0.5440    0.4541    0.4950       218\n",
      "           1     0.6966    0.8720    0.7745       453\n",
      "\n",
      "    accuracy                         0.6658       775\n",
      "   macro avg     0.6956    0.5125    0.5360       775\n",
      "weighted avg     0.6738    0.6658    0.6374       775\n",
      "\n",
      "Naive Bayes + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.20      0.34       333\n",
      "           0       0.94      0.93      0.94       917\n",
      "           1       0.82      1.00      0.90      1236\n",
      "\n",
      "    accuracy                           0.87      2486\n",
      "   macro avg       0.92      0.71      0.72      2486\n",
      "weighted avg       0.89      0.87      0.84      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     1.0000    0.0481    0.0917       104\n",
      "           0     0.5570    0.2018    0.2963       218\n",
      "           1     0.6194    0.9448    0.7483       453\n",
      "\n",
      "    accuracy                         0.6155       775\n",
      "   macro avg     0.7255    0.3982    0.3788       775\n",
      "weighted avg     0.6529    0.6155    0.5330       775\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "svm = SVC()\n",
    "\n",
    "all_train = [bow_train,tfidf_train]\n",
    "all_test = [bow_test,tfidf_test]\n",
    "all_vectorisers = [\"BoW\",\"TfIdf\"]\n",
    "\n",
    "for train,test,vectoriser in zip(all_train,all_test,all_vectorisers):\n",
    "    logreg.fit(train,trainvalagg.label)\n",
    "    \n",
    "    print(\"Logistic Regression + \" + vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, logreg.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, logreg.predict(test),digits=4)) \n",
    "    \n",
    "    svm.fit(train,trainvalagg.label)\n",
    "    \n",
    "    print(\"SVM + \" + vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, svm.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, svm.predict(test),digits=4)) \n",
    "\n",
    "    # Fasttest dont work w NaiveBayes\n",
    "    nb.fit(train,trainvalagg.label)\n",
    "    print(\"Naive Bayes + \"+ vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, nb.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, nb.predict(test),digits=4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5813514360313315"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(34/383)*0.2769  + (104/383)*0.5693 + (245/383)*0.6531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5330452903225806"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(104/775)*0.0917 + (218/775)*0.2963 + (453/775)*0.7483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
