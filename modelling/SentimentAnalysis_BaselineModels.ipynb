{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>restaurant_code</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_body</th>\n",
       "      <th>account_name</th>\n",
       "      <th>account_id</th>\n",
       "      <th>aspect</th>\n",
       "      <th>phrase</th>\n",
       "      <th>label</th>\n",
       "      <th>new_aspect_1</th>\n",
       "      <th>new_aspect_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>So glad the weekend is here and I can finally ...</td>\n",
       "      <td>Breakfast with the husband this morning at Gen...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>food</td>\n",
       "      <td>cheat, day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>So glad the weekend is here and I can finally ...</td>\n",
       "      <td>Breakfast with the husband this morning at Gen...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>food</td>\n",
       "      <td>glutinous, rice</td>\n",
       "      <td>0.0</td>\n",
       "      <td>food</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>My Saturday morning started with breakfast at ...</td>\n",
       "      <td>It is so inspiring talking to Uncle Kun as he ...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>food</td>\n",
       "      <td>cantonese, style, congee, well, seasoned, gene...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>food</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>My Saturday morning started with breakfast at ...</td>\n",
       "      <td>It is so inspiring talking to Uncle Kun as he ...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>portion</td>\n",
       "      <td>well, seasoned, generous, ingredients</td>\n",
       "      <td>1.0</td>\n",
       "      <td>portion</td>\n",
       "      <td>portion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>03 Jan 2018 my comfort food..</td>\n",
       "      <td>Lunar New Year is approaching.. 根叔 Gen Shu is ...</td>\n",
       "      <td>Bobcatsysop YK Chan</td>\n",
       "      <td>@BobcatSysOp</td>\n",
       "      <td>time</td>\n",
       "      <td>sold, around, lunch, time</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index restaurant_code                                       review_title  \\\n",
       "0      0        2lhaZp7B  So glad the weekend is here and I can finally ...   \n",
       "1      1        2lhaZp7B  So glad the weekend is here and I can finally ...   \n",
       "2      2        2lhaZp7B  My Saturday morning started with breakfast at ...   \n",
       "3      3        2lhaZp7B  My Saturday morning started with breakfast at ...   \n",
       "4      4        2lhaZp7B                      03 Jan 2018 my comfort food..   \n",
       "\n",
       "                                         review_body         account_name  \\\n",
       "0  Breakfast with the husband this morning at Gen...           Maureen Ow   \n",
       "1  Breakfast with the husband this morning at Gen...           Maureen Ow   \n",
       "2  It is so inspiring talking to Uncle Kun as he ...           Maureen Ow   \n",
       "3  It is so inspiring talking to Uncle Kun as he ...           Maureen Ow   \n",
       "4  Lunar New Year is approaching.. 根叔 Gen Shu is ...  Bobcatsysop YK Chan   \n",
       "\n",
       "     account_id   aspect                                             phrase  \\\n",
       "0        @72128     food                                         cheat, day   \n",
       "1        @72128     food                                    glutinous, rice   \n",
       "2        @72128     food  cantonese, style, congee, well, seasoned, gene...   \n",
       "3        @72128  portion              well, seasoned, generous, ingredients   \n",
       "4  @BobcatSysOp     time                          sold, around, lunch, time   \n",
       "\n",
       "   label new_aspect_1 new_aspect_2  \n",
       "0    0.0          NaN          NaN  \n",
       "1    0.0         food          NaN  \n",
       "2    1.0         food         food  \n",
       "3    1.0      portion      portion  \n",
       "4    0.0          NaN          NaN  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"\"\n",
    "#path = \"../data/old_labels/\"\n",
    "#path \"../data/new_labels/\"\n",
    "test = pd.read_csv(path + \"test_subset.csv\")\n",
    "test[\"label\"] = pd.to_numeric(test[\"label\"], errors='coerce')\n",
    "train_1 = pd.read_csv(path + \"train_first_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_2 = pd.read_csv(path + \"train_second_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "#train_3 = pd.read_csv(path + \"train_third_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "extra_neg = pd.read_csv(path + \"extra_neg.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "#train_1[\"label\"] = pd.to_numeric(train_1[\"label\"], errors='coerce')\n",
    "#train = pd.concat([train_1,train_2,train_3],axis=0)\n",
    "train = pd.concat([train_1,train_2,extra_neg],axis=0)\n",
    "val = pd.read_csv(path + \"val_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "\n",
    "test = test.reset_index()\n",
    "train=train.reset_index()\n",
    "val = val.reset_index()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data in [train_1,train_2,train_3,train,val,test]:\n",
    "    print(\"Total size = {}\".format(data.shape[0]))\n",
    "    print(\"Noise = {} \".format(data.loc[(data.label == 0.0) & (data.new_aspect_1.isnull()) & (data.new_aspect_2.isnull())].shape[0]))\n",
    "    print(\"Aspect with no adjectives = {} \".format(data.loc[(data.new_aspect_1.notnull()) & (data.new_aspect_2.isnull())].shape[0]))\n",
    "    print(\"Aspect with adjectives = {} \".format(data.loc[(data.new_aspect_1.notnull()) & (data.new_aspect_2.notnull())].shape[0]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>restaurant_code</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_body</th>\n",
       "      <th>account_name</th>\n",
       "      <th>account_id</th>\n",
       "      <th>aspect</th>\n",
       "      <th>phrase</th>\n",
       "      <th>phrase_no_aspect</th>\n",
       "      <th>phrase_no_noun</th>\n",
       "      <th>label</th>\n",
       "      <th>new_aspect_1</th>\n",
       "      <th>new_aspect_2</th>\n",
       "      <th>phrase_lemma</th>\n",
       "      <th>phrase_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>328-katong-laksa-united-square</td>\n",
       "      <td>328 Katong Laksa; Now you don't have to travel...</td>\n",
       "      <td>Back then during the 2013 hawker heroes challe...</td>\n",
       "      <td>Qing Xiang</td>\n",
       "      <td>@qingxiang</td>\n",
       "      <td>food</td>\n",
       "      <td>katong laksa travel way</td>\n",
       "      <td>['katong', 'travel', 'way']</td>\n",
       "      <td>['katong', 'laksa', 'travel']</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>katong laksa travel way</td>\n",
       "      <td>katong laksa travel way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>328-katong-laksa-united-square</td>\n",
       "      <td>328 Katong Laksa; Now you don't have to travel...</td>\n",
       "      <td>Back then during the 2013 hawker heroes challe...</td>\n",
       "      <td>Qing Xiang</td>\n",
       "      <td>@qingxiang</td>\n",
       "      <td>food</td>\n",
       "      <td>east centralised outlet united square shopping...</td>\n",
       "      <td>['east', 'centralised', 'outlet', 'united', 's...</td>\n",
       "      <td>['centralised', 'united', 'square', 'shopping']</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>east centralised outlet united square shopping...</td>\n",
       "      <td>east centralis outlet unit squar shop mall novena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>328-katong-laksa-united-square</td>\n",
       "      <td>328 Katong Laksa; Now you don't have to travel...</td>\n",
       "      <td>Back then during the 2013 hawker heroes challe...</td>\n",
       "      <td>Qing Xiang</td>\n",
       "      <td>@qingxiang</td>\n",
       "      <td>food</td>\n",
       "      <td>laksa defeated gordon ramsay version</td>\n",
       "      <td>['defeated', 'gordon', 'ramsay', 'version']</td>\n",
       "      <td>['laksa', 'defeated', 'ramsay']</td>\n",
       "      <td>1</td>\n",
       "      <td>food</td>\n",
       "      <td>food</td>\n",
       "      <td>laksa defeated gordon ramsay version</td>\n",
       "      <td>laksa defeat gordon ramsay version</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>328-katong-laksa-united-square</td>\n",
       "      <td>328 Katong Laksa; Now you don't have to travel...</td>\n",
       "      <td>Back then during the 2013 hawker heroes challe...</td>\n",
       "      <td>Qing Xiang</td>\n",
       "      <td>@qingxiang</td>\n",
       "      <td>food</td>\n",
       "      <td>laksa category</td>\n",
       "      <td>['category']</td>\n",
       "      <td>['laksa']</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>laksa category</td>\n",
       "      <td>laksa categori</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>328-katong-laksa-united-square</td>\n",
       "      <td>328 Katong Laksa; Now you don't have to travel...</td>\n",
       "      <td>Back then during the 2013 hawker heroes challe...</td>\n",
       "      <td>Qing Xiang</td>\n",
       "      <td>@qingxiang</td>\n",
       "      <td>food</td>\n",
       "      <td>definitely live</td>\n",
       "      <td>['definitely', 'live']</td>\n",
       "      <td>['definitely', 'live']</td>\n",
       "      <td>1</td>\n",
       "      <td>food</td>\n",
       "      <td>food</td>\n",
       "      <td>definitely live</td>\n",
       "      <td>definit live</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Unnamed: 0                 restaurant_code  \\\n",
       "0      0           0  328-katong-laksa-united-square   \n",
       "1      1           1  328-katong-laksa-united-square   \n",
       "2      2           2  328-katong-laksa-united-square   \n",
       "3      3           3  328-katong-laksa-united-square   \n",
       "4      4           4  328-katong-laksa-united-square   \n",
       "\n",
       "                                        review_title  \\\n",
       "0  328 Katong Laksa; Now you don't have to travel...   \n",
       "1  328 Katong Laksa; Now you don't have to travel...   \n",
       "2  328 Katong Laksa; Now you don't have to travel...   \n",
       "3  328 Katong Laksa; Now you don't have to travel...   \n",
       "4  328 Katong Laksa; Now you don't have to travel...   \n",
       "\n",
       "                                         review_body account_name  account_id  \\\n",
       "0  Back then during the 2013 hawker heroes challe...   Qing Xiang  @qingxiang   \n",
       "1  Back then during the 2013 hawker heroes challe...   Qing Xiang  @qingxiang   \n",
       "2  Back then during the 2013 hawker heroes challe...   Qing Xiang  @qingxiang   \n",
       "3  Back then during the 2013 hawker heroes challe...   Qing Xiang  @qingxiang   \n",
       "4  Back then during the 2013 hawker heroes challe...   Qing Xiang  @qingxiang   \n",
       "\n",
       "  aspect                                             phrase  \\\n",
       "0   food                            katong laksa travel way   \n",
       "1   food  east centralised outlet united square shopping...   \n",
       "2   food               laksa defeated gordon ramsay version   \n",
       "3   food                                     laksa category   \n",
       "4   food                                    definitely live   \n",
       "\n",
       "                                    phrase_no_aspect  \\\n",
       "0                        ['katong', 'travel', 'way']   \n",
       "1  ['east', 'centralised', 'outlet', 'united', 's...   \n",
       "2        ['defeated', 'gordon', 'ramsay', 'version']   \n",
       "3                                       ['category']   \n",
       "4                             ['definitely', 'live']   \n",
       "\n",
       "                                    phrase_no_noun label new_aspect_1  \\\n",
       "0                    ['katong', 'laksa', 'travel']     0          NaN   \n",
       "1  ['centralised', 'united', 'square', 'shopping']     0          NaN   \n",
       "2                  ['laksa', 'defeated', 'ramsay']     1         food   \n",
       "3                                        ['laksa']     0          NaN   \n",
       "4                           ['definitely', 'live']     1         food   \n",
       "\n",
       "  new_aspect_2                                       phrase_lemma  \\\n",
       "0          NaN                            katong laksa travel way   \n",
       "1          NaN  east centralised outlet united square shopping...   \n",
       "2         food               laksa defeated gordon ramsay version   \n",
       "3          NaN                                     laksa category   \n",
       "4         food                                    definitely live   \n",
       "\n",
       "                                         phrase_stem  \n",
       "0                            katong laksa travel way  \n",
       "1  east centralis outlet unit squar shop mall novena  \n",
       "2                 laksa defeat gordon ramsay version  \n",
       "3                                     laksa categori  \n",
       "4                                       definit live  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Data Preprocessing\n",
    "\n",
    "1. Clean Phrase (Utils) \n",
    "2. Filter the phrase column where there is an empty string \n",
    "3. Combine the aspects into 1 & aggregate the sentiment scores by averaging then sign(>0 = 1,=0 =0 <0 = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean phrases\n",
    "train[\"phrase_lemma\"] = train.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "val[\"phrase_lemma\"] = val.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "test[\"phrase_lemma\"] = test.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "\n",
    "train[\"phrase_stem\"] = train.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "val[\"phrase_stem\"] = val.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "test[\"phrase_stem\"] = test.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "\n",
    "train.phrase = train.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))\n",
    "val.phrase = val.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))\n",
    "test.phrase = test.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows where there are no characters\n",
    "train = train.loc[(train.phrase.str.len() > 0)]\n",
    "val = val.loc[(val.phrase.str.len() > 0)]\n",
    "test = test.loc[(test.phrase.str.len() > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if nan label, replace with 0\n",
    "train.label = train.label.apply(lambda x: 0 if np.isnan(x) else x)\n",
    "val.label = val.label.apply(lambda x: 0 if np.isnan(x) else x)\n",
    "test.label = test.label.apply(lambda x: 0 if np.isnan(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>restaurant_code</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_body</th>\n",
       "      <th>account_name</th>\n",
       "      <th>account_id</th>\n",
       "      <th>aspect</th>\n",
       "      <th>phrase</th>\n",
       "      <th>label</th>\n",
       "      <th>new_aspect_1</th>\n",
       "      <th>new_aspect_2</th>\n",
       "      <th>phrase_lemma</th>\n",
       "      <th>phrase_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>So glad the weekend is here and I can finally ...</td>\n",
       "      <td>Breakfast with the husband this morning at Gen...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>food</td>\n",
       "      <td>cheat day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cheat day</td>\n",
       "      <td>cheat day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>So glad the weekend is here and I can finally ...</td>\n",
       "      <td>Breakfast with the husband this morning at Gen...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>food</td>\n",
       "      <td>glutinous rice</td>\n",
       "      <td>0.0</td>\n",
       "      <td>food</td>\n",
       "      <td>NaN</td>\n",
       "      <td>glutinous rice</td>\n",
       "      <td>glutin rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>My Saturday morning started with breakfast at ...</td>\n",
       "      <td>It is so inspiring talking to Uncle Kun as he ...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>food</td>\n",
       "      <td>cantonese style congee seasoned generous</td>\n",
       "      <td>1.0</td>\n",
       "      <td>food</td>\n",
       "      <td>food</td>\n",
       "      <td>cantonese style congee seasoned generous</td>\n",
       "      <td>cantones style conge season gener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>My Saturday morning started with breakfast at ...</td>\n",
       "      <td>It is so inspiring talking to Uncle Kun as he ...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>portion</td>\n",
       "      <td>seasoned generous ingredients</td>\n",
       "      <td>1.0</td>\n",
       "      <td>portion</td>\n",
       "      <td>portion</td>\n",
       "      <td>seasoned generous ingredient</td>\n",
       "      <td>season gener ingredi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>03 Jan 2018 my comfort food..</td>\n",
       "      <td>Lunar New Year is approaching.. 根叔 Gen Shu is ...</td>\n",
       "      <td>Bobcatsysop YK Chan</td>\n",
       "      <td>@BobcatSysOp</td>\n",
       "      <td>time</td>\n",
       "      <td>sold lunch time</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sold lunch time</td>\n",
       "      <td>sold lunch time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index restaurant_code                                       review_title  \\\n",
       "0      0        2lhaZp7B  So glad the weekend is here and I can finally ...   \n",
       "1      1        2lhaZp7B  So glad the weekend is here and I can finally ...   \n",
       "2      2        2lhaZp7B  My Saturday morning started with breakfast at ...   \n",
       "3      3        2lhaZp7B  My Saturday morning started with breakfast at ...   \n",
       "4      4        2lhaZp7B                      03 Jan 2018 my comfort food..   \n",
       "\n",
       "                                         review_body         account_name  \\\n",
       "0  Breakfast with the husband this morning at Gen...           Maureen Ow   \n",
       "1  Breakfast with the husband this morning at Gen...           Maureen Ow   \n",
       "2  It is so inspiring talking to Uncle Kun as he ...           Maureen Ow   \n",
       "3  It is so inspiring talking to Uncle Kun as he ...           Maureen Ow   \n",
       "4  Lunar New Year is approaching.. 根叔 Gen Shu is ...  Bobcatsysop YK Chan   \n",
       "\n",
       "     account_id   aspect                                    phrase  label  \\\n",
       "0        @72128     food                                 cheat day    0.0   \n",
       "1        @72128     food                            glutinous rice    0.0   \n",
       "2        @72128     food  cantonese style congee seasoned generous    1.0   \n",
       "3        @72128  portion             seasoned generous ingredients    1.0   \n",
       "4  @BobcatSysOp     time                           sold lunch time    0.0   \n",
       "\n",
       "  new_aspect_1 new_aspect_2                              phrase_lemma  \\\n",
       "0          NaN          NaN                                 cheat day   \n",
       "1         food          NaN                            glutinous rice   \n",
       "2         food         food  cantonese style congee seasoned generous   \n",
       "3      portion      portion              seasoned generous ingredient   \n",
       "4          NaN          NaN                           sold lunch time   \n",
       "\n",
       "                         phrase_stem  \n",
       "0                          cheat day  \n",
       "1                        glutin rice  \n",
       "2  cantones style conge season gener  \n",
       "3               season gener ingredi  \n",
       "4                    sold lunch time  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgn(x):\n",
    "    if x < 0:\n",
    "        return -1\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out phrases with hashtags & those with character length <=5 then get the majority (might not make sense)\n",
    "# train\n",
    "trainagg = train.groupby([\"restaurant_code\",\"review_title\",\"account_name\",\"new_aspect_1\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                                                \"phrase_lemma\": \" \".join,\n",
    "                                                                                                \"phrase_stem\":\" \".join,\n",
    "                                                                                                \"label\": \"mean\"})\n",
    "trainagg = trainagg.reset_index()\n",
    "trainagg.label = trainagg.label.map(sgn)\n",
    "\n",
    "\n",
    "# test\n",
    "valagg = val.groupby([\"restaurant_code\",\"review_title\",\"account_name\",\"new_aspect_1\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                                                \"phrase_lemma\": \" \".join,\n",
    "                                                                                                \"phrase_stem\":\" \".join,\n",
    "                                                                                                \"label\": \"mean\"})\n",
    "valagg = valagg.reset_index()\n",
    "valagg.label = valagg.label.map(sgn)\n",
    "\n",
    "#val \n",
    "testagg = test.groupby([\"restaurant_code\",\"review_title\",\"account_name\",\"new_aspect_1\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                                                \"phrase_lemma\": \" \".join,\n",
    "                                                                                                \"phrase_stem\":\" \".join,\n",
    "                                                                                                \"label\": \"mean\"})\n",
    "testagg = testagg.reset_index()\n",
    "testagg.label = testagg.label.map(sgn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvalagg = pd.concat([trainagg, valagg],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    372\n",
       " 0    120\n",
       "-1     40\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testagg.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"../data/new_labels/\"\n",
    "trainagg.to_csv(path + \"train_oldpreproc.csv\")\n",
    "valagg.to_csv(path + \"val_oldpreproc.csv\")\n",
    "testagg.to_csv(path + \"test_oldpreproc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(383, 8)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testagg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Text Vectorisation --> used on data2\n",
    "1. BoW\n",
    "2. Tf-idf\n",
    "3. Fasttext - can handle oov words https://www.quora.com/How-does-fastText-output-a-vector-for-a-word-that-is-not-in-the-pre-trained-model (not used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 3))\n",
    "bow_train = bow.fit_transform(trainvalagg.phrase)\n",
    "bow_test = bow.transform(testagg.phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 3))\n",
    "tfidf_train = tfidf.fit_transform(trainvalagg.phrase)\n",
    "tfidf_test = tfidf.transform(testagg.phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Preprocess data for fasttext word embeddings training\n",
    "train_clean = [process_text(sentence) for sentence in tqdm(trainvalagg.phrase) if sentence.strip() !='']\n",
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "train_word_tokens = [word_tokenizer.tokenize(sent) for sent in tqdm(train_clean)]\n",
    "\n",
    "# Preprocess data for fasttext word embeddings testing data\n",
    "test_clean = [process_text(sentence) for sentence in tqdm(test2.phrase) if sentence.strip() !='']\n",
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "test_word_tokens = [word_tokenizer.tokenize(sent) for sent in tqdm(test_clean)]\n",
    "\n",
    "# Train Fasttext word embeddings on training data\n",
    "# Defining values for parameters\n",
    "embedding_size = 300\n",
    "window_size = 5\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n",
    " \n",
    "\n",
    "fast_Text_model = FastText(train_word_tokens,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      workers = 4,\n",
    "                      sg=1,\n",
    "                      iter=100)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Save fastText gensim model\n",
    "fast_Text_model.save(\"ftmodel/ft_model_train\")\n",
    "# Load saved gensim fastText model\n",
    "fast_Text_model = Word2Vec.load(\"ftmodel/ft_model_train\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# convert sentence --> average(each word embedding)\n",
    "fasttext_train = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in train_word_tokens]\n",
    "# convert sentence --> average(each word embedding)\n",
    "fasttext_test = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in test_word_tokens]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fasttext_train_processed = []\n",
    "for row in fasttext_train:\n",
    "    try:\n",
    "        fasttext_train_processed.append(list(row))\n",
    "    except:\n",
    "        fasttext_train_processed.append([0]*300)\n",
    "\n",
    "fasttext_test_processed = []\n",
    "for row in fasttext_test:\n",
    "    try:\n",
    "        fasttext_test_processed.append(list(row))\n",
    "    except:\n",
    "        fasttext_test_processed.append([0]*300)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fasttext_train_processed = np.array(fasttext_train_processed).reshape(-1,300)\n",
    "fasttext_test_processed = np.array(fasttext_test_processed).reshape(-1,300)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelling\n",
    "\n",
    "1. Naive Bayes/Support Vector Machines (NBSVM)/Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Baseline) 1. Naive Bayes -Support Vector Machines (NBSVM)- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.96      0.98       397\n",
      "           0       0.98      0.99      0.99      1015\n",
      "           1       0.99      0.99      0.99      1318\n",
      "\n",
      "    accuracy                           0.99      2730\n",
      "   macro avg       0.99      0.98      0.99      2730\n",
      "weighted avg       0.99      0.99      0.99      2730\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.52      0.30      0.38        40\n",
      "           0       0.43      0.82      0.56       120\n",
      "           1       0.89      0.67      0.77       372\n",
      "\n",
      "    accuracy                           0.68       532\n",
      "   macro avg       0.61      0.60      0.57       532\n",
      "weighted avg       0.76      0.68      0.69       532\n",
      "\n",
      "SVM + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.97      0.68      0.80       397\n",
      "           0       0.79      0.97      0.87      1015\n",
      "           1       0.97      0.89      0.93      1318\n",
      "\n",
      "    accuracy                           0.89      2730\n",
      "   macro avg       0.91      0.85      0.87      2730\n",
      "weighted avg       0.90      0.89      0.89      2730\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.46      0.15      0.23        40\n",
      "           0       0.35      0.90      0.50       120\n",
      "           1       0.90      0.51      0.65       372\n",
      "\n",
      "    accuracy                           0.57       532\n",
      "   macro avg       0.57      0.52      0.46       532\n",
      "weighted avg       0.74      0.57      0.59       532\n",
      "\n",
      "Naive Bayes + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.98      0.93      0.95       397\n",
      "           0       0.98      0.93      0.96      1015\n",
      "           1       0.94      0.99      0.97      1318\n",
      "\n",
      "    accuracy                           0.96      2730\n",
      "   macro avg       0.97      0.95      0.96      2730\n",
      "weighted avg       0.96      0.96      0.96      2730\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.23      0.35        40\n",
      "           0       0.47      0.41      0.44       120\n",
      "           1       0.77      0.86      0.82       372\n",
      "\n",
      "    accuracy                           0.71       532\n",
      "   macro avg       0.66      0.50      0.53       532\n",
      "weighted avg       0.70      0.71      0.69       532\n",
      "\n",
      "Logistic Regression + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.58      0.73       397\n",
      "           0       0.91      0.97      0.94      1015\n",
      "           1       0.92      1.00      0.96      1318\n",
      "\n",
      "    accuracy                           0.93      2730\n",
      "   macro avg       0.94      0.85      0.88      2730\n",
      "weighted avg       0.93      0.93      0.92      2730\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.56      0.25      0.34        40\n",
      "           0       0.46      0.64      0.54       120\n",
      "           1       0.84      0.78      0.81       372\n",
      "\n",
      "    accuracy                           0.71       532\n",
      "   macro avg       0.62      0.56      0.56       532\n",
      "weighted avg       0.73      0.71      0.71       532\n",
      "\n",
      "SVM + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.97      0.99       397\n",
      "           0       0.99      0.99      0.99      1015\n",
      "           1       0.99      1.00      0.99      1318\n",
      "\n",
      "    accuracy                           0.99      2730\n",
      "   macro avg       0.99      0.99      0.99      2730\n",
      "weighted avg       0.99      0.99      0.99      2730\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.54      0.17      0.26        40\n",
      "           0       0.49      0.61      0.54       120\n",
      "           1       0.82      0.82      0.82       372\n",
      "\n",
      "    accuracy                           0.72       532\n",
      "   macro avg       0.62      0.53      0.54       532\n",
      "weighted avg       0.73      0.72      0.72       532\n",
      "\n",
      "Naive Bayes + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.27      0.42       397\n",
      "           0       0.93      0.94      0.93      1015\n",
      "           1       0.82      1.00      0.90      1318\n",
      "\n",
      "    accuracy                           0.87      2730\n",
      "   macro avg       0.92      0.73      0.75      2730\n",
      "weighted avg       0.89      0.87      0.84      2730\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.05      0.09        40\n",
      "           0       0.51      0.32      0.39       120\n",
      "           1       0.76      0.92      0.83       372\n",
      "\n",
      "    accuracy                           0.72       532\n",
      "   macro avg       0.65      0.43      0.44       532\n",
      "weighted avg       0.69      0.72      0.68       532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "svm = SVC()\n",
    "\n",
    "all_train = [bow_train,tfidf_train]\n",
    "all_test = [bow_test,tfidf_test]\n",
    "all_vectorisers = [\"BoW\",\"TfIdf\"]\n",
    "\n",
    "for train,test,vectoriser in zip(all_train,all_test,all_vectorisers):\n",
    "    logreg.fit(train,trainvalagg.label)\n",
    "    \n",
    "    print(\"Logistic Regression + \" + vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, logreg.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, logreg.predict(test))) \n",
    "    \n",
    "    svm.fit(train,trainvalagg.label)\n",
    "    \n",
    "    print(\"SVM + \" + vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, svm.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, svm.predict(test))) \n",
    "\n",
    "    # Fasttest dont work w NaiveBayes\n",
    "    nb.fit(train,trainvalagg.label)\n",
    "    print(\"Naive Bayes + \"+ vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, nb.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, nb.predict(test)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
