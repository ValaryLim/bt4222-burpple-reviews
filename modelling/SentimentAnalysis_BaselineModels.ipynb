{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# old labels\n",
    "#path = \"\"\n",
    "path =\"old_labels/\"\n",
    "test = pd.read_csv(path + \"test_subset.csv\")\n",
    "train_1 = pd.read_csv(path + \"train_first_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_2 = pd.read_csv(path + \"train_second_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_3 = pd.read_csv(path + \"train_third_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_1[\"label\"] = pd.to_numeric(train_1[\"label\"], errors='coerce')\n",
    "train = pd.concat([train_1,train_2,train_3],axis=0)\n",
    "val = pd.read_csv(path + \"val_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "\n",
    "test = test.reset_index()\n",
    "train=train.reset_index()\n",
    "val = val.reset_index()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8348, 10), (3055, 10), (3040, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new labels\n",
    "#path = \"new_labels/\"\n",
    "path = \"\"\n",
    "test = pd.read_csv(path + \"test_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "test[\"label\"] = pd.to_numeric(test[\"label\"], errors='coerce')\n",
    "train_1 = pd.read_csv(path + \"train_first_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_2 = pd.read_csv(path + \"train_second_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "extra_neg = pd.read_csv(path + \"extra_neg.csv\")[[\"Unnamed: 0\",'restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect1','new_aspect2']]\n",
    "extra_neg = extra_neg.rename(columns= {'new_aspect1':'new_aspect_1', 'new_aspect2':'new_aspect_2'})\n",
    "val = pd.read_csv(path + \"val_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "\n",
    "index = extra_neg.loc[extra_neg[\"Unnamed: 0\"] == 0].index\n",
    "\n",
    "test = pd.concat([test,extra_neg.iloc[:index[1],1:]],axis=0)\n",
    "train = pd.concat([train_1,train_2,extra_neg.iloc[index[1] + 1:index[2],1:]],axis=0)\n",
    "val = pd.concat([val,extra_neg.iloc[index[2] + 1:,1:]],axis=0)\n",
    "train.shape,val.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.phrase[500:590]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data in [train_1,train_2,train_3,train,val,test]:\n",
    "    print(\"Total size = {}\".format(data.shape[0]))\n",
    "    print(\"Noise = {} \".format(data.loc[(data.label == 0.0) & (data.new_aspect_1.isnull()) & (data.new_aspect_2.isnull())].shape[0]))\n",
    "    print(\"Aspect with no adjectives = {} \".format(data.loc[(data.new_aspect_1.notnull()) & (data.new_aspect_2.isnull())].shape[0]))\n",
    "    print(\"Aspect with adjectives = {} \".format(data.loc[(data.new_aspect_1.notnull()) & (data.new_aspect_2.notnull())].shape[0]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Data Preprocessing\n",
    "\n",
    "1. Clean Phrase (Utils) \n",
    "2. Filter the phrase column where there is an empty string \n",
    "3. Combine the aspects into 1 & aggregate the sentiment scores by averaging then sign(>0 = 1,=0 =0 <0 = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean phrases\n",
    "train[\"phrase_lemma\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False))\n",
    "val[\"phrase_lemma\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False))\n",
    "test[\"phrase_lemma\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False))\n",
    "\n",
    "train[\"phrase_stem\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True))\n",
    "val[\"phrase_stem\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True))\n",
    "test[\"phrase_stem\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True))\n",
    "\n",
    "train[\"phrase_emoticon_generic\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False, convert_emoji_generic = True))\n",
    "val[\"phrase_emoticon_generic\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False, convert_emoji_generic = True))\n",
    "test[\"phrase_emoticon_generic\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False, convert_emoji_generic = True))\n",
    "\n",
    "train[\"phrase_lemma_emoticon_generic\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False, convert_emoji_generic = True))\n",
    "val[\"phrase_lemma_emoticon_generic\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False, convert_emoji_generic = True))\n",
    "test[\"phrase_lemma_emoticon_generic\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False, convert_emoji_generic = True))\n",
    "\n",
    "train[\"phrase_stem_emoticon_generic\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True, convert_emoji_generic = True))\n",
    "val[\"phrase_stem_emoticon_generic\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True, convert_emoji_generic = True))\n",
    "test[\"phrase_stem_emoticon_generic\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True, convert_emoji_generic = True))\n",
    "\n",
    "train[\"phrase_emoticon_unique\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False, convert_emoji_unique = True))\n",
    "val[\"phrase_emoticon_unique\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False, convert_emoji_unique = True))\n",
    "test[\"phrase_emoticon_unique\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False, convert_emoji_unique = True))\n",
    "\n",
    "train[\"phrase_lemma_emoticon_unique\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False, convert_emoji_unique = True))\n",
    "val[\"phrase_lemma_emoticon_unique\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False, convert_emoji_unique = True))\n",
    "test[\"phrase_lemma_emoticon_unique\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False, convert_emoji_unique = True))\n",
    "\n",
    "train[\"phrase_stem_emoticon_unique\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True, convert_emoji_unique = True))\n",
    "val[\"phrase_stem_emoticon_unique\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True, convert_emoji_unique = True))\n",
    "test[\"phrase_stem_emoticon_unique\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True, convert_emoji_unique = True))\n",
    "\n",
    "train.phrase = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False))\n",
    "val.phrase = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False))\n",
    "test.phrase = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows where there are no characters\n",
    "train = train.loc[(train.phrase.str.len() > 0)]\n",
    "val = val.loc[(val.phrase.str.len() > 0)]\n",
    "test = test.loc[(test.phrase.str.len() > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if nan label, replace with 0\n",
    "train.label = train.label.apply(lambda x: 0 if np.isnan(x) else x)\n",
    "val.label = val.label.apply(lambda x: 0 if np.isnan(x) else x)\n",
    "test.label = test.label.apply(lambda x: 0 if np.isnan(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainagg = train.groupby([\"restaurant_code\",\"review_title\",\"account_name\",\"new_aspect_1\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                                                \"phrase_lemma\":\" \".join,\n",
    "                                                                                                \"phrase_stem\":\" \".join,\n",
    "                                                                                                \"phrase_emoticon_generic\" : \" \".join,\n",
    "                                                                                                \"phrase_emoticon_unique\" : \" \".join,\n",
    "                                                                                                \"phrase_stem_emoticon_generic\" : \" \".join,\n",
    "                                                                                                \"phrase_lemma_emoticon_generic\": \" \".join,\n",
    "                                                                                                \"phrase_stem_emoticon_unique\": \" \".join,\n",
    "                                                                                                \"phrase_lemma_emoticon_unique\" : \" \".join,                                                                                \n",
    "                                                                                                \"label\": np.mean})                                                                                                \n",
    "trainagg = trainagg.reset_index()\n",
    "trainagg.label = trainagg.label.map(np.sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valagg = val.groupby([\"restaurant_code\",\"review_title\",\"account_name\",\"new_aspect_1\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                                               \"phrase_lemma\":\" \".join,\n",
    "                                                                                               \"phrase_stem\":\" \".join,\n",
    "                                                                                               \"phrase_emoticon_generic\" : \" \".join,\n",
    "                                                                                                \"phrase_emoticon_unique\" : \" \".join,\n",
    "                                                                                                \"phrase_stem_emoticon_generic\" : \" \".join,\n",
    "                                                                                                \"phrase_lemma_emoticon_generic\": \" \".join,\n",
    "                                                                                                \"phrase_stem_emoticon_unique\": \" \".join,\n",
    "                                                                                                \"phrase_lemma_emoticon_unique\" : \" \".join,                                                                                  \n",
    "                                                                                               \"label\": np.mean})   \n",
    "valagg = valagg.reset_index()\n",
    "valagg.label = valagg.label.map(np.sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testagg = test.groupby([\"restaurant_code\",\"review_title\",\"account_name\",\"new_aspect_1\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                                               \"phrase_lemma\":\" \".join,\n",
    "                                                                                               \"phrase_stem\":\" \".join,\n",
    "                                                                                               \"phrase_emoticon_generic\" : \" \".join,\n",
    "                                                                                                \"phrase_emoticon_unique\" : \" \".join,\n",
    "                                                                                                \"phrase_stem_emoticon_generic\" : \" \".join,\n",
    "                                                                                                \"phrase_lemma_emoticon_generic\": \" \".join,\n",
    "                                                                                                \"phrase_stem_emoticon_unique\": \" \".join,\n",
    "                                                                                                \"phrase_lemma_emoticon_unique\" : \" \".join,                                                                                 \n",
    "                                                                                               \"label\": np.mean})   \n",
    "testagg = testagg.reset_index()\n",
    "testagg.label = testagg.label.map(np.sign)\n",
    "testagg.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvalagg = pd.concat([trainagg, valagg],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "trainagg.to_csv(path + \"train_newpreproc_emoticon.csv\")\n",
    "valagg.to_csv(path + \"val_newpreproc_emoticon.csv\")\n",
    "testagg.to_csv(path + \"test_newpreproc_emoticon.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Text Vectorisation --> used on data2\n",
    "1. BoW\n",
    "2. Tf-idf\n",
    "3. Fasttext - can handle oov words https://www.quora.com/How-does-fastText-output-a-vector-for-a-word-that-is-not-in-the-pre-trained-model (not used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainagg = pd.read_csv(\"train_newpreproc_emoticon.csv\")\n",
    "valagg = pd.read_csv(\"val_newpreproc_emoticon.csv\")\n",
    "testagg = pd.read_csv(\"test_newpreproc_emoticon.csv\")\n",
    "trainvalagg = pd.concat([trainagg, valagg],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 3))\n",
    "bow_train = bow.fit_transform(trainvalagg.phrase)\n",
    "bow_test = bow.transform(testagg.phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 3))\n",
    "tfidf_train = tfidf.fit_transform(trainvalagg.phrase)\n",
    "tfidf_test = tfidf.transform(testagg.phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Preprocess data for fasttext word embeddings training\n",
    "train_clean = [process_text(sentence) for sentence in tqdm(trainvalagg.phrase) if sentence.strip() !='']\n",
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "train_word_tokens = [word_tokenizer.tokenize(sent) for sent in tqdm(train_clean)]\n",
    "\n",
    "# Preprocess data for fasttext word embeddings testing data\n",
    "test_clean = [process_text(sentence) for sentence in tqdm(test2.phrase) if sentence.strip() !='']\n",
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "test_word_tokens = [word_tokenizer.tokenize(sent) for sent in tqdm(test_clean)]\n",
    "\n",
    "# Train Fasttext word embeddings on training data\n",
    "# Defining values for parameters\n",
    "embedding_size = 300\n",
    "window_size = 5\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n",
    " \n",
    "\n",
    "fast_Text_model = FastText(train_word_tokens,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      workers = 4,\n",
    "                      sg=1,\n",
    "                      iter=100)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Save fastText gensim model\n",
    "fast_Text_model.save(\"ftmodel/ft_model_train\")\n",
    "# Load saved gensim fastText model\n",
    "fast_Text_model = Word2Vec.load(\"ftmodel/ft_model_train\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# convert sentence --> average(each word embedding)\n",
    "fasttext_train = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in train_word_tokens]\n",
    "# convert sentence --> average(each word embedding)\n",
    "fasttext_test = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in test_word_tokens]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fasttext_train_processed = []\n",
    "for row in fasttext_train:\n",
    "    try:\n",
    "        fasttext_train_processed.append(list(row))\n",
    "    except:\n",
    "        fasttext_train_processed.append([0]*300)\n",
    "\n",
    "fasttext_test_processed = []\n",
    "for row in fasttext_test:\n",
    "    try:\n",
    "        fasttext_test_processed.append(list(row))\n",
    "    except:\n",
    "        fasttext_test_processed.append([0]*300)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fasttext_train_processed = np.array(fasttext_train_processed).reshape(-1,300)\n",
    "fasttext_test_processed = np.array(fasttext_test_processed).reshape(-1,300)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelling\n",
    "\n",
    "1. Naive Bayes/Support Vector Machines (NBSVM)/Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Baseline) 1. Naive Bayes -Support Vector Machines (NBSVM)- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "svm = SVC()\n",
    "\n",
    "all_train = [bow_train,tfidf_train]\n",
    "all_test = [bow_test,tfidf_test]\n",
    "all_vectorisers = [\"BoW\",\"TfIdf\"]\n",
    "\n",
    "for train,test,vectoriser in zip(all_train,all_test,all_vectorisers):\n",
    "    logreg.fit(train,trainvalagg.label)\n",
    "    \n",
    "    print(\"Logistic Regression + \" + vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, logreg.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, logreg.predict(test),digits=4)) \n",
    "    \n",
    "    svm.fit(train,trainvalagg.label)\n",
    "    \n",
    "    print(\"SVM + \" + vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, svm.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, svm.predict(test),digits=4)) \n",
    "\n",
    "    # Fasttest dont work w NaiveBayes\n",
    "    nb.fit(train,trainvalagg.label)\n",
    "    print(\"Naive Bayes + \"+ vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, nb.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, nb.predict(test),digits=4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(34/383)*0.2769  + (104/383)*0.5693 + (245/383)*0.6531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(104/775)*0.0917 + (218/775)*0.2963 + (453/775)*0.7483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}