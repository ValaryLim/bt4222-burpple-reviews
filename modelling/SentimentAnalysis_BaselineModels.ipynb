{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataProcessingUtils import clean_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((8348, 10), (3055, 10), (3040, 10))"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# new labels\n",
    "path = \"data/to_label/\"\n",
    "test = pd.read_csv(path + \"test_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "test[\"label\"] = pd.to_numeric(test[\"label\"], errors='coerce')\n",
    "train_1 = pd.read_csv(path + \"train_first_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_2 = pd.read_csv(path + \"train_second_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "extra_neg = pd.read_csv(path + \"extra_neg.csv\")[[\"Unnamed: 0\",'restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect1','new_aspect2']]\n",
    "extra_neg = extra_neg.rename(columns= {'new_aspect1':'new_aspect_1', 'new_aspect2':'new_aspect_2'})\n",
    "val = pd.read_csv(path + \"val_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "\n",
    "index = extra_neg.loc[extra_neg[\"Unnamed: 0\"] == 0].index\n",
    "\n",
    "test = pd.concat([test,extra_neg.iloc[:index[1],1:]],axis=0)\n",
    "train = pd.concat([train_1,train_2,extra_neg.iloc[index[1] + 1:index[2],1:]],axis=0)\n",
    "val = pd.concat([val,extra_neg.iloc[index[2] + 1:,1:]],axis=0)\n",
    "train.shape,val.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total size = 3108\nNoise = 156 \nAspect with no adjectives = 329 \nAspect with adjectives = 265 \n\nTotal size = 3108\nNoise = 894 \nAspect with no adjectives = 1052 \nAspect with adjectives = 1157 \n\nTotal size = 8348\nNoise = 1670 \nAspect with no adjectives = 2110 \nAspect with adjectives = 2204 \n\nTotal size = 3055\nNoise = 650 \nAspect with no adjectives = 892 \nAspect with adjectives = 1512 \n\nTotal size = 3040\nNoise = 707 \nAspect with no adjectives = 1005 \nAspect with adjectives = 1304 \n\n"
     ]
    }
   ],
   "source": [
    "for data in [train_1,train_2,train,val,test]:\n",
    "    print(\"Total size = {}\".format(data.shape[0]))\n",
    "    print(\"Noise = {} \".format(data.loc[(data.label == 0.0) & (data.new_aspect_1.isnull()) & (data.new_aspect_2.isnull())].shape[0]))\n",
    "    print(\"Aspect with no adjectives = {} \".format(data.loc[(data.new_aspect_1.notnull()) & (data.new_aspect_2.isnull())].shape[0]))\n",
    "    print(\"Aspect with adjectives = {} \".format(data.loc[(data.new_aspect_1.notnull()) & (data.new_aspect_2.notnull())].shape[0]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  restaurant_code  \\\n",
       "0  328-katong-laksa-united-square   \n",
       "1  328-katong-laksa-united-square   \n",
       "2  328-katong-laksa-united-square   \n",
       "3  328-katong-laksa-united-square   \n",
       "4  328-katong-laksa-united-square   \n",
       "\n",
       "                                        review_title  \\\n",
       "0  328 Katong Laksa; Now you don't have to travel...   \n",
       "1  328 Katong Laksa; Now you don't have to travel...   \n",
       "2  328 Katong Laksa; Now you don't have to travel...   \n",
       "3  328 Katong Laksa; Now you don't have to travel...   \n",
       "4  328 Katong Laksa; Now you don't have to travel...   \n",
       "\n",
       "                                         review_body account_name  account_id  \\\n",
       "0  Back then during the 2013 hawker heroes challe...   Qing Xiang  @qingxiang   \n",
       "1  Back then during the 2013 hawker heroes challe...   Qing Xiang  @qingxiang   \n",
       "2  Back then during the 2013 hawker heroes challe...   Qing Xiang  @qingxiang   \n",
       "3  Back then during the 2013 hawker heroes challe...   Qing Xiang  @qingxiang   \n",
       "4  Back then during the 2013 hawker heroes challe...   Qing Xiang  @qingxiang   \n",
       "\n",
       "  aspect                                             phrase  label  \\\n",
       "0   food                         katong, laksa, travel, way    0.0   \n",
       "1   food  east, centralised, outlet, united, square, sho...    0.0   \n",
       "2   food           laksa, defeated, gordon, ramsay, version    1.0   \n",
       "3   food                                    laksa, category    0.0   \n",
       "4   food                                   definitely, live    1.0   \n",
       "\n",
       "  new_aspect_1 new_aspect_2  \n",
       "0          NaN          NaN  \n",
       "1          NaN          NaN  \n",
       "2         food         food  \n",
       "3          NaN          NaN  \n",
       "4         food         food  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>restaurant_code</th>\n      <th>review_title</th>\n      <th>review_body</th>\n      <th>account_name</th>\n      <th>account_id</th>\n      <th>aspect</th>\n      <th>phrase</th>\n      <th>label</th>\n      <th>new_aspect_1</th>\n      <th>new_aspect_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>328-katong-laksa-united-square</td>\n      <td>328 Katong Laksa; Now you don't have to travel...</td>\n      <td>Back then during the 2013 hawker heroes challe...</td>\n      <td>Qing Xiang</td>\n      <td>@qingxiang</td>\n      <td>food</td>\n      <td>katong, laksa, travel, way</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>328-katong-laksa-united-square</td>\n      <td>328 Katong Laksa; Now you don't have to travel...</td>\n      <td>Back then during the 2013 hawker heroes challe...</td>\n      <td>Qing Xiang</td>\n      <td>@qingxiang</td>\n      <td>food</td>\n      <td>east, centralised, outlet, united, square, sho...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>328-katong-laksa-united-square</td>\n      <td>328 Katong Laksa; Now you don't have to travel...</td>\n      <td>Back then during the 2013 hawker heroes challe...</td>\n      <td>Qing Xiang</td>\n      <td>@qingxiang</td>\n      <td>food</td>\n      <td>laksa, defeated, gordon, ramsay, version</td>\n      <td>1.0</td>\n      <td>food</td>\n      <td>food</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>328-katong-laksa-united-square</td>\n      <td>328 Katong Laksa; Now you don't have to travel...</td>\n      <td>Back then during the 2013 hawker heroes challe...</td>\n      <td>Qing Xiang</td>\n      <td>@qingxiang</td>\n      <td>food</td>\n      <td>laksa, category</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>328-katong-laksa-united-square</td>\n      <td>328 Katong Laksa; Now you don't have to travel...</td>\n      <td>Back then during the 2013 hawker heroes challe...</td>\n      <td>Qing Xiang</td>\n      <td>@qingxiang</td>\n      <td>food</td>\n      <td>definitely, live</td>\n      <td>1.0</td>\n      <td>food</td>\n      <td>food</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Data Preprocessing\n",
    "\n",
    "1. Clean Phrase (Utils) \n",
    "2. Filter the phrase column where there is an empty string \n",
    "3. Combine the aspects into 1 & aggregate the sentiment scores by averaging then sign(>0 = 1,=0 =0 <0 = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean phrases\n",
    "train[\"phrase_lemma\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False))\n",
    "val[\"phrase_lemma\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False))\n",
    "test[\"phrase_lemma\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False))\n",
    "\n",
    "train[\"phrase_stem\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True))\n",
    "val[\"phrase_stem\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True))\n",
    "test[\"phrase_stem\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True))\n",
    "\n",
    "train[\"phrase_emoticon_generic\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False, convert_emoji_generic = True))\n",
    "val[\"phrase_emoticon_generic\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False, convert_emoji_generic = True))\n",
    "test[\"phrase_emoticon_generic\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False, convert_emoji_generic = True))\n",
    "\n",
    "train[\"phrase_lemma_emoticon_generic\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False, convert_emoji_generic = True))\n",
    "val[\"phrase_lemma_emoticon_generic\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False, convert_emoji_generic = True))\n",
    "test[\"phrase_lemma_emoticon_generic\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False, convert_emoji_generic = True))\n",
    "\n",
    "train[\"phrase_stem_emoticon_generic\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True, convert_emoji_generic = True))\n",
    "val[\"phrase_stem_emoticon_generic\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True, convert_emoji_generic = True))\n",
    "test[\"phrase_stem_emoticon_generic\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True, convert_emoji_generic = True))\n",
    "\n",
    "train[\"phrase_emoticon_unique\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False, convert_emoji_unique = True))\n",
    "val[\"phrase_emoticon_unique\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False, convert_emoji_unique = True))\n",
    "test[\"phrase_emoticon_unique\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False, convert_emoji_unique = True))\n",
    "\n",
    "train[\"phrase_lemma_emoticon_unique\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False, convert_emoji_unique = True))\n",
    "val[\"phrase_lemma_emoticon_unique\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False, convert_emoji_unique = True))\n",
    "test[\"phrase_lemma_emoticon_unique\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=True, stem=False, convert_emoji_unique = True))\n",
    "\n",
    "train[\"phrase_stem_emoticon_unique\"] = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True, convert_emoji_unique = True))\n",
    "val[\"phrase_stem_emoticon_unique\"] = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True, convert_emoji_unique = True))\n",
    "test[\"phrase_stem_emoticon_unique\"] = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=True, convert_emoji_unique = True))\n",
    "\n",
    "train.phrase = train.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False))\n",
    "val.phrase = val.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False))\n",
    "test.phrase = test.phrase.apply(lambda x: clean_phrase(x, lemmatize=False, stem=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows where there are no characters\n",
    "train = train.loc[(train.phrase.str.len() > 0)]\n",
    "val = val.loc[(val.phrase.str.len() > 0)]\n",
    "test = test.loc[(test.phrase.str.len() > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py:5168: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# if nan label, replace with 0\n",
    "train.label = train.label.apply(lambda x: 0 if np.isnan(x) else x)\n",
    "val.label = val.label.apply(lambda x: 0 if np.isnan(x) else x)\n",
    "test.label = test.label.apply(lambda x: 0 if np.isnan(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainagg = train.groupby([\"restaurant_code\",\"review_title\",\"account_name\",\"new_aspect_1\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                                                \"phrase_lemma\":\" \".join,\n",
    "                                                                                                \"phrase_stem\":\" \".join,\n",
    "                                                                                                \"phrase_emoticon_generic\" : \" \".join,\n",
    "                                                                                                \"phrase_emoticon_unique\" : \" \".join,\n",
    "                                                                                                \"phrase_stem_emoticon_generic\" : \" \".join,\n",
    "                                                                                                \"phrase_lemma_emoticon_generic\": \" \".join,\n",
    "                                                                                                \"phrase_stem_emoticon_unique\": \" \".join,\n",
    "                                                                                                \"phrase_lemma_emoticon_unique\" : \" \".join,                                                                                \n",
    "                                                                                                \"label\": np.mean})                                                                                                \n",
    "trainagg = trainagg.reset_index()\n",
    "trainagg.label = trainagg.label.map(np.sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "valagg = val.groupby([\"restaurant_code\",\"review_title\",\"account_name\",\"new_aspect_1\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                                               \"phrase_lemma\":\" \".join,\n",
    "                                                                                               \"phrase_stem\":\" \".join,\n",
    "                                                                                               \"phrase_emoticon_generic\" : \" \".join,\n",
    "                                                                                                \"phrase_emoticon_unique\" : \" \".join,\n",
    "                                                                                                \"phrase_stem_emoticon_generic\" : \" \".join,\n",
    "                                                                                                \"phrase_lemma_emoticon_generic\": \" \".join,\n",
    "                                                                                                \"phrase_stem_emoticon_unique\": \" \".join,\n",
    "                                                                                                \"phrase_lemma_emoticon_unique\" : \" \".join,                                                                                  \n",
    "                                                                                               \"label\": np.mean})   \n",
    "valagg = valagg.reset_index()\n",
    "valagg.label = valagg.label.map(np.sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       " 1.0    453\n",
       " 0.0    218\n",
       "-1.0    104\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "testagg = test.groupby([\"restaurant_code\",\"review_title\",\"account_name\",\"new_aspect_1\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                                               \"phrase_lemma\":\" \".join,\n",
    "                                                                                               \"phrase_stem\":\" \".join,\n",
    "                                                                                               \"phrase_emoticon_generic\" : \" \".join,\n",
    "                                                                                                \"phrase_emoticon_unique\" : \" \".join,\n",
    "                                                                                                \"phrase_stem_emoticon_generic\" : \" \".join,\n",
    "                                                                                                \"phrase_lemma_emoticon_generic\": \" \".join,\n",
    "                                                                                                \"phrase_stem_emoticon_unique\": \" \".join,\n",
    "                                                                                                \"phrase_lemma_emoticon_unique\" : \" \".join,                                                                                 \n",
    "                                                                                               \"label\": np.mean})   \n",
    "testagg = testagg.reset_index()\n",
    "testagg.label = testagg.label.map(np.sign)\n",
    "testagg.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvalagg = pd.concat([trainagg, valagg],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/labelled_data/\"\n",
    "trainagg.to_csv(path + \"train_newpreproc_emoticon.csv\")\n",
    "valagg.to_csv(path + \"val_newpreproc_emoticon.csv\")\n",
    "testagg.to_csv(path + \"test_newpreproc_emoticon.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Vectorisation \n",
    "1. BoW\n",
    "2. Tf-idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/labelled_data/\"\n",
    "trainagg = pd.read_csv(path + \"train_newpreproc_emoticon.csv\")\n",
    "valagg = pd.read_csv(path + \"val_newpreproc_emoticon.csv\")\n",
    "testagg = pd.read_csv(path + \"test_newpreproc_emoticon.csv\")\n",
    "trainvalagg = pd.concat([trainagg, valagg],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 3))\n",
    "bow_train = bow.fit_transform(trainvalagg.phrase)\n",
    "bow_test = bow.transform(testagg.phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 3))\n",
    "tfidf_train = tfidf.fit_transform(trainvalagg.phrase)\n",
    "tfidf_test = tfidf.transform(testagg.phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelling (baseline)\n",
    "\n",
    "Naive Bayes/Support Vector Machines (NBSVM)/Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logistic Regression + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.99      0.96      0.98       333\n",
      "         0.0       0.98      0.99      0.99       917\n",
      "         1.0       0.99      0.99      0.99      1236\n",
      "\n",
      "    accuracy                           0.99      2486\n",
      "   macro avg       0.99      0.98      0.99      2486\n",
      "weighted avg       0.99      0.99      0.99      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0     0.7021    0.3173    0.4371       104\n",
      "         0.0     0.5000    0.7982    0.6148       218\n",
      "         1.0     0.8368    0.7020    0.7635       453\n",
      "\n",
      "    accuracy                         0.6774       775\n",
      "   macro avg     0.6797    0.6058    0.6051       775\n",
      "weighted avg     0.7240    0.6774    0.6779       775\n",
      "\n",
      "SVM + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.97      0.66      0.78       333\n",
      "         0.0       0.79      0.97      0.87       917\n",
      "         1.0       0.97      0.89      0.93      1236\n",
      "\n",
      "    accuracy                           0.89      2486\n",
      "   macro avg       0.91      0.84      0.86      2486\n",
      "weighted avg       0.91      0.89      0.89      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0     0.6923    0.1731    0.2769       104\n",
      "         0.0     0.4176    0.8945    0.5693       218\n",
      "         1.0     0.8511    0.5298    0.6531       453\n",
      "\n",
      "    accuracy                         0.5845       775\n",
      "   macro avg     0.6536    0.5325    0.4998       775\n",
      "weighted avg     0.7078    0.5845    0.5790       775\n",
      "\n",
      "Naive Bayes + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.97      0.93      0.95       333\n",
      "         0.0       0.98      0.93      0.96       917\n",
      "         1.0       0.94      0.99      0.97      1236\n",
      "\n",
      "    accuracy                           0.96      2486\n",
      "   macro avg       0.97      0.95      0.96      2486\n",
      "weighted avg       0.96      0.96      0.96      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0     0.9167    0.2115    0.3437       104\n",
      "         0.0     0.5103    0.3394    0.4077       218\n",
      "         1.0     0.6617    0.8852    0.7573       453\n",
      "\n",
      "    accuracy                         0.6413       775\n",
      "   macro avg     0.6962    0.4787    0.5029       775\n",
      "weighted avg     0.6533    0.6413    0.6035       775\n",
      "\n",
      "Logistic Regression + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.99      0.52      0.68       333\n",
      "         0.0       0.92      0.97      0.95       917\n",
      "         1.0       0.91      1.00      0.95      1236\n",
      "\n",
      "    accuracy                           0.92      2486\n",
      "   macro avg       0.94      0.83      0.86      2486\n",
      "weighted avg       0.93      0.92      0.91      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0     0.7812    0.2404    0.3676       104\n",
      "         0.0     0.5314    0.5826    0.5558       218\n",
      "         1.0     0.7361    0.8190    0.7753       453\n",
      "\n",
      "    accuracy                         0.6748       775\n",
      "   macro avg     0.6829    0.5473    0.5663       775\n",
      "weighted avg     0.6846    0.6748    0.6589       775\n",
      "\n",
      "SVM + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       1.00      0.96      0.98       333\n",
      "         0.0       0.99      0.99      0.99       917\n",
      "         1.0       0.99      1.00      0.99      1236\n",
      "\n",
      "    accuracy                           0.99      2486\n",
      "   macro avg       0.99      0.98      0.99      2486\n",
      "weighted avg       0.99      0.99      0.99      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0     0.8462    0.2115    0.3385       104\n",
      "         0.0     0.5440    0.4541    0.4950       218\n",
      "         1.0     0.6966    0.8720    0.7745       453\n",
      "\n",
      "    accuracy                         0.6658       775\n",
      "   macro avg     0.6956    0.5125    0.5360       775\n",
      "weighted avg     0.6738    0.6658    0.6374       775\n",
      "\n",
      "Naive Bayes + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       1.00      0.20      0.34       333\n",
      "         0.0       0.94      0.93      0.94       917\n",
      "         1.0       0.82      1.00      0.90      1236\n",
      "\n",
      "    accuracy                           0.87      2486\n",
      "   macro avg       0.92      0.71      0.72      2486\n",
      "weighted avg       0.89      0.87      0.84      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0     1.0000    0.0481    0.0917       104\n",
      "         0.0     0.5570    0.2018    0.2963       218\n",
      "         1.0     0.6194    0.9448    0.7483       453\n",
      "\n",
      "    accuracy                         0.6155       775\n",
      "   macro avg     0.7255    0.3982    0.3788       775\n",
      "weighted avg     0.6529    0.6155    0.5330       775\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "svm = SVC()\n",
    "\n",
    "all_train = [bow_train,tfidf_train]\n",
    "all_test = [bow_test,tfidf_test]\n",
    "all_vectorisers = [\"BoW\",\"TfIdf\"]\n",
    "\n",
    "for train,test,vectoriser in zip(all_train,all_test,all_vectorisers):\n",
    "    logreg.fit(train,trainvalagg.label)\n",
    "    \n",
    "    print(\"Logistic Regression + \" + vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, logreg.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, logreg.predict(test),digits=4)) \n",
    "    \n",
    "    svm.fit(train,trainvalagg.label)\n",
    "    \n",
    "    print(\"SVM + \" + vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, svm.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, svm.predict(test),digits=4)) \n",
    "\n",
    "    # Fasttest dont work w NaiveBayes\n",
    "    nb.fit(train,trainvalagg.label)\n",
    "    print(\"Naive Bayes + \"+ vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, nb.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, nb.predict(test),digits=4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}