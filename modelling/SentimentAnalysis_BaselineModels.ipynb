{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old labels\n",
    "#path = \"\"\n",
    "path =\"old_labels/\"\n",
    "test = pd.read_csv(path + \"test_subset.csv\")\n",
    "train_1 = pd.read_csv(path + \"train_first_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_2 = pd.read_csv(path + \"train_second_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_3 = pd.read_csv(path + \"train_third_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_1[\"label\"] = pd.to_numeric(train_1[\"label\"], errors='coerce')\n",
    "train = pd.concat([train_1,train_2,train_3],axis=0)\n",
    "val = pd.read_csv(path + \"val_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "\n",
    "test = test.reset_index()\n",
    "train=train.reset_index()\n",
    "val = val.reset_index()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8348, 10), (3055, 10), (3040, 10))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new labels\n",
    "#path = \"new_labels/\"\n",
    "path = \"\"\n",
    "test = pd.read_csv(path + \"test_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "test[\"label\"] = pd.to_numeric(test[\"label\"], errors='coerce')\n",
    "train_1 = pd.read_csv(path + \"train_first_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_2 = pd.read_csv(path + \"train_second_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "extra_neg = pd.read_csv(path + \"extra_neg.csv\")[[\"Unnamed: 0\",'restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect1','new_aspect2']]\n",
    "extra_neg = extra_neg.rename(columns= {'new_aspect1':'new_aspect_1', 'new_aspect2':'new_aspect_2'})\n",
    "val = pd.read_csv(path + \"val_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "\n",
    "index = extra_neg.loc[extra_neg[\"Unnamed: 0\"] == 0].index\n",
    "\n",
    "test = pd.concat([test,extra_neg.iloc[:index[1],1:]],axis=0)\n",
    "train = pd.concat([train_1,train_2,extra_neg.iloc[index[1] + 1:index[2],1:]],axis=0)\n",
    "val = pd.concat([val,extra_neg.iloc[index[2] + 1:,1:]],axis=0)\n",
    "train.shape,val.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data in [train_1,train_2,train_3,train,val,test]:\n",
    "    print(\"Total size = {}\".format(data.shape[0]))\n",
    "    print(\"Noise = {} \".format(data.loc[(data.label == 0.0) & (data.new_aspect_1.isnull()) & (data.new_aspect_2.isnull())].shape[0]))\n",
    "    print(\"Aspect with no adjectives = {} \".format(data.loc[(data.new_aspect_1.notnull()) & (data.new_aspect_2.isnull())].shape[0]))\n",
    "    print(\"Aspect with adjectives = {} \".format(data.loc[(data.new_aspect_1.notnull()) & (data.new_aspect_2.notnull())].shape[0]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Data Preprocessing\n",
    "\n",
    "1. Clean Phrase (Utils) \n",
    "2. Filter the phrase column where there is an empty string \n",
    "3. Combine the aspects into 1 & aggregate the sentiment scores by averaging then sign(>0 = 1,=0 =0 <0 = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train.copy()\n",
    "val1 = val.copy()\n",
    "test1 = test.copy()\n",
    "\n",
    "train[\"phrase_emoticon\"] = utils.process_emojis(train1,\"phrase\",\"positive\",\"negative\")[\"phrase\"]\n",
    "val[\"phrase_emoticon\"] = utils.process_emojis(val1,\"phrase\",\"positive\",\"negative\")[\"phrase\"]\n",
    "test[\"phrase_emoticon\"] = utils.process_emojis(test1,\"phrase\",\"positive\",\"negative\")[\"phrase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean phrases\n",
    "train[\"phrase_lemma\"] = train.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "val[\"phrase_lemma\"] = val.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "test[\"phrase_lemma\"] = test.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "\n",
    "train[\"phrase_stem\"] = train.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "val[\"phrase_stem\"] = val.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "test[\"phrase_stem\"] = test.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "\n",
    "train[\"phrase_lemma_emoticons\"] = train.phrase_emoticon.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "val[\"phrase_lemma_emoticons\"] = val.phrase_emoticon.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "test[\"phrase_lemma_emoticons\"] = test.phrase_emoticon.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "\n",
    "train[\"phrase_stem_emoticons\"] = train.phrase_emoticon.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "val[\"phrase_stem_emoticons\"] = val.phrase_emoticon.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "test[\"phrase_stem_emoticons\"] = test.phrase_emoticon.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "\n",
    "train.phrase = train.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))\n",
    "val.phrase = val.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))\n",
    "test.phrase = test.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))\n",
    "\n",
    "train.phrase_emoticon = train.phrase_emoticon.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))\n",
    "val.phrase_emoticon = val.phrase_emoticon.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))\n",
    "test.phrase_emoticon = test.phrase_emoticon.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows where there are no characters\n",
    "train = train.loc[(train.phrase.str.len() > 0)]\n",
    "val = val.loc[(val.phrase.str.len() > 0)]\n",
    "test = test.loc[(test.phrase.str.len() > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if nan label, replace with 0\n",
    "train.label = train.label.apply(lambda x: 0 if np.isnan(x) else x)\n",
    "val.label = val.label.apply(lambda x: 0 if np.isnan(x) else x)\n",
    "test.label = test.label.apply(lambda x: 0 if np.isnan(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_code</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_body</th>\n",
       "      <th>account_name</th>\n",
       "      <th>account_id</th>\n",
       "      <th>aspect</th>\n",
       "      <th>phrase</th>\n",
       "      <th>label</th>\n",
       "      <th>new_aspect_1</th>\n",
       "      <th>new_aspect_2</th>\n",
       "      <th>phrase_emoticon</th>\n",
       "      <th>phrase_lemma</th>\n",
       "      <th>phrase_stem</th>\n",
       "      <th>phrase_lemma_emoticons</th>\n",
       "      <th>phrase_stem_emoticons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>So glad the weekend is here and I can finally ...</td>\n",
       "      <td>Breakfast with the husband this morning at Gen...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>food</td>\n",
       "      <td>cheat day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cheat day</td>\n",
       "      <td>cheat day</td>\n",
       "      <td>cheat day</td>\n",
       "      <td>cheat day</td>\n",
       "      <td>cheat day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>So glad the weekend is here and I can finally ...</td>\n",
       "      <td>Breakfast with the husband this morning at Gen...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>food</td>\n",
       "      <td>glutinous rice</td>\n",
       "      <td>0.0</td>\n",
       "      <td>food</td>\n",
       "      <td>NaN</td>\n",
       "      <td>glutinous rice</td>\n",
       "      <td>glutinous rice</td>\n",
       "      <td>glutin rice</td>\n",
       "      <td>glutinous rice</td>\n",
       "      <td>glutin rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>My Saturday morning started with breakfast at ...</td>\n",
       "      <td>It is so inspiring talking to Uncle Kun as he ...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>food</td>\n",
       "      <td>cantonese style congee seasoned generous</td>\n",
       "      <td>1.0</td>\n",
       "      <td>food</td>\n",
       "      <td>food</td>\n",
       "      <td>cantonese style congee seasoned generous</td>\n",
       "      <td>cantonese style congee seasoned generous</td>\n",
       "      <td>cantones style conge season gener</td>\n",
       "      <td>cantonese style congee seasoned generous</td>\n",
       "      <td>cantones style conge season gener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>My Saturday morning started with breakfast at ...</td>\n",
       "      <td>It is so inspiring talking to Uncle Kun as he ...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>portion</td>\n",
       "      <td>seasoned generous ingredients</td>\n",
       "      <td>1.0</td>\n",
       "      <td>portion</td>\n",
       "      <td>portion</td>\n",
       "      <td>seasoned generous ingredients</td>\n",
       "      <td>seasoned generous ingredient</td>\n",
       "      <td>season gener ingredi</td>\n",
       "      <td>seasoned generous ingredient</td>\n",
       "      <td>season gener ingredi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>03 Jan 2018 my comfort food..</td>\n",
       "      <td>Lunar New Year is approaching.. 根叔 Gen Shu is ...</td>\n",
       "      <td>Bobcatsysop YK Chan</td>\n",
       "      <td>@BobcatSysOp</td>\n",
       "      <td>time</td>\n",
       "      <td>sold lunch time</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sold lunch time</td>\n",
       "      <td>sold lunch time</td>\n",
       "      <td>sold lunch time</td>\n",
       "      <td>sold lunch time</td>\n",
       "      <td>sold lunch time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  restaurant_code                                       review_title  \\\n",
       "0        2lhaZp7B  So glad the weekend is here and I can finally ...   \n",
       "1        2lhaZp7B  So glad the weekend is here and I can finally ...   \n",
       "2        2lhaZp7B  My Saturday morning started with breakfast at ...   \n",
       "3        2lhaZp7B  My Saturday morning started with breakfast at ...   \n",
       "4        2lhaZp7B                      03 Jan 2018 my comfort food..   \n",
       "\n",
       "                                         review_body         account_name  \\\n",
       "0  Breakfast with the husband this morning at Gen...           Maureen Ow   \n",
       "1  Breakfast with the husband this morning at Gen...           Maureen Ow   \n",
       "2  It is so inspiring talking to Uncle Kun as he ...           Maureen Ow   \n",
       "3  It is so inspiring talking to Uncle Kun as he ...           Maureen Ow   \n",
       "4  Lunar New Year is approaching.. 根叔 Gen Shu is ...  Bobcatsysop YK Chan   \n",
       "\n",
       "     account_id   aspect                                    phrase  label  \\\n",
       "0        @72128     food                                 cheat day    0.0   \n",
       "1        @72128     food                            glutinous rice    0.0   \n",
       "2        @72128     food  cantonese style congee seasoned generous    1.0   \n",
       "3        @72128  portion             seasoned generous ingredients    1.0   \n",
       "4  @BobcatSysOp     time                           sold lunch time    0.0   \n",
       "\n",
       "  new_aspect_1 new_aspect_2                           phrase_emoticon  \\\n",
       "0          NaN          NaN                                 cheat day   \n",
       "1         food          NaN                            glutinous rice   \n",
       "2         food         food  cantonese style congee seasoned generous   \n",
       "3      portion      portion             seasoned generous ingredients   \n",
       "4          NaN          NaN                           sold lunch time   \n",
       "\n",
       "                               phrase_lemma  \\\n",
       "0                                 cheat day   \n",
       "1                            glutinous rice   \n",
       "2  cantonese style congee seasoned generous   \n",
       "3              seasoned generous ingredient   \n",
       "4                           sold lunch time   \n",
       "\n",
       "                         phrase_stem  \\\n",
       "0                          cheat day   \n",
       "1                        glutin rice   \n",
       "2  cantones style conge season gener   \n",
       "3               season gener ingredi   \n",
       "4                    sold lunch time   \n",
       "\n",
       "                     phrase_lemma_emoticons              phrase_stem_emoticons  \n",
       "0                                 cheat day                          cheat day  \n",
       "1                            glutinous rice                        glutin rice  \n",
       "2  cantonese style congee seasoned generous  cantones style conge season gener  \n",
       "3              seasoned generous ingredient               season gener ingredi  \n",
       "4                           sold lunch time                    sold lunch time  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_code</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_body</th>\n",
       "      <th>account_name</th>\n",
       "      <th>account_id</th>\n",
       "      <th>aspect</th>\n",
       "      <th>phrase</th>\n",
       "      <th>label</th>\n",
       "      <th>new_aspect_1</th>\n",
       "      <th>new_aspect_2</th>\n",
       "      <th>phrase_emoticon</th>\n",
       "      <th>phrase_lemma</th>\n",
       "      <th>phrase_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>big-small-restaurant</td>\n",
       "      <td>私房菜</td>\n",
       "      <td>A relatively new Chinese restaurant, which is ...</td>\n",
       "      <td>Doreen Tan</td>\n",
       "      <td>@alamakgirl</td>\n",
       "      <td>food</td>\n",
       "      <td>gold medal pork rib</td>\n",
       "      <td>1.0</td>\n",
       "      <td>food</td>\n",
       "      <td>food</td>\n",
       "      <td>gold, medal, positive , pork, rib</td>\n",
       "      <td>gold medal pork rib</td>\n",
       "      <td>gold medal pork rib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>nam-kee-pau-fraser-centrepoint</td>\n",
       "      <td>Big Pau</td>\n",
       "      <td>First time trying this traditional handmade pa...</td>\n",
       "      <td>Daphne Wee</td>\n",
       "      <td>@carbiedaphie</td>\n",
       "      <td>food</td>\n",
       "      <td>lean meat pau skin dry</td>\n",
       "      <td>0.0</td>\n",
       "      <td>food</td>\n",
       "      <td>food</td>\n",
       "      <td>lean, meat, pau, ’, skin, n, ’, dry, either</td>\n",
       "      <td>lean meat pau skin dry</td>\n",
       "      <td>lean meat pau skin dri</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    restaurant_code review_title  \\\n",
       "532            big-small-restaurant          私房菜   \n",
       "532  nam-kee-pau-fraser-centrepoint      Big Pau   \n",
       "\n",
       "                                           review_body account_name  \\\n",
       "532  A relatively new Chinese restaurant, which is ...   Doreen Tan   \n",
       "532  First time trying this traditional handmade pa...   Daphne Wee   \n",
       "\n",
       "        account_id aspect                  phrase  label new_aspect_1  \\\n",
       "532    @alamakgirl   food     gold medal pork rib    1.0         food   \n",
       "532  @carbiedaphie   food  lean meat pau skin dry    0.0         food   \n",
       "\n",
       "    new_aspect_2                              phrase_emoticon  \\\n",
       "532         food            gold, medal, positive , pork, rib   \n",
       "532         food  lean, meat, pau, ’, skin, n, ’, dry, either   \n",
       "\n",
       "               phrase_lemma             phrase_stem  \n",
       "532     gold medal pork rib     gold medal pork rib  \n",
       "532  lean meat pau skin dry  lean meat pau skin dri  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[532]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgn(x):\n",
    "    if x < 0:\n",
    "        return -1\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1.0    453\n",
       " 0.0    218\n",
       "-1.0    104\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testagg.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvalagg = pd.concat([trainagg, valagg],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "trainagg.to_csv(path + \"train_newpreproc_emoticon.csv\")\n",
    "valagg.to_csv(path + \"val_newpreproc_emoticon.csv\")\n",
    "testagg.to_csv(path + \"test_newpreproc_emoticon.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Text Vectorisation --> used on data2\n",
    "1. BoW\n",
    "2. Tf-idf\n",
    "3. Fasttext - can handle oov words https://www.quora.com/How-does-fastText-output-a-vector-for-a-word-that-is-not-in-the-pre-trained-model (not used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainagg = pd.read_csv(\"train_newpreproc.csv\")\n",
    "valagg = pd.read_csv(\"val_newpreproc.csv\")\n",
    "testagg = pd.read_csv(\"test_newpreproc.csv\")\n",
    "trainvalagg = pd.concat([trainagg, valagg],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 3))\n",
    "bow_train = bow.fit_transform(trainvalagg.phrase)\n",
    "bow_test = bow.transform(testagg.phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 3))\n",
    "tfidf_train = tfidf.fit_transform(trainvalagg.phrase)\n",
    "tfidf_test = tfidf.transform(testagg.phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Preprocess data for fasttext word embeddings training\n",
    "train_clean = [process_text(sentence) for sentence in tqdm(trainvalagg.phrase) if sentence.strip() !='']\n",
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "train_word_tokens = [word_tokenizer.tokenize(sent) for sent in tqdm(train_clean)]\n",
    "\n",
    "# Preprocess data for fasttext word embeddings testing data\n",
    "test_clean = [process_text(sentence) for sentence in tqdm(test2.phrase) if sentence.strip() !='']\n",
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "test_word_tokens = [word_tokenizer.tokenize(sent) for sent in tqdm(test_clean)]\n",
    "\n",
    "# Train Fasttext word embeddings on training data\n",
    "# Defining values for parameters\n",
    "embedding_size = 300\n",
    "window_size = 5\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n",
    " \n",
    "\n",
    "fast_Text_model = FastText(train_word_tokens,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      workers = 4,\n",
    "                      sg=1,\n",
    "                      iter=100)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Save fastText gensim model\n",
    "fast_Text_model.save(\"ftmodel/ft_model_train\")\n",
    "# Load saved gensim fastText model\n",
    "fast_Text_model = Word2Vec.load(\"ftmodel/ft_model_train\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# convert sentence --> average(each word embedding)\n",
    "fasttext_train = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in train_word_tokens]\n",
    "# convert sentence --> average(each word embedding)\n",
    "fasttext_test = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in test_word_tokens]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fasttext_train_processed = []\n",
    "for row in fasttext_train:\n",
    "    try:\n",
    "        fasttext_train_processed.append(list(row))\n",
    "    except:\n",
    "        fasttext_train_processed.append([0]*300)\n",
    "\n",
    "fasttext_test_processed = []\n",
    "for row in fasttext_test:\n",
    "    try:\n",
    "        fasttext_test_processed.append(list(row))\n",
    "    except:\n",
    "        fasttext_test_processed.append([0]*300)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fasttext_train_processed = np.array(fasttext_train_processed).reshape(-1,300)\n",
    "fasttext_test_processed = np.array(fasttext_test_processed).reshape(-1,300)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelling\n",
    "\n",
    "1. Naive Bayes/Support Vector Machines (NBSVM)/Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Baseline) 1. Naive Bayes -Support Vector Machines (NBSVM)- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.96      0.98       333\n",
      "           0       0.98      0.99      0.99       917\n",
      "           1       0.99      0.99      0.99      1236\n",
      "\n",
      "    accuracy                           0.99      2486\n",
      "   macro avg       0.99      0.98      0.99      2486\n",
      "weighted avg       0.99      0.99      0.99      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.7021    0.3173    0.4371       104\n",
      "           0     0.5000    0.7982    0.6148       218\n",
      "           1     0.8368    0.7020    0.7635       453\n",
      "\n",
      "    accuracy                         0.6774       775\n",
      "   macro avg     0.6797    0.6058    0.6051       775\n",
      "weighted avg     0.7240    0.6774    0.6779       775\n",
      "\n",
      "SVM + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.97      0.66      0.78       333\n",
      "           0       0.79      0.97      0.87       917\n",
      "           1       0.97      0.89      0.93      1236\n",
      "\n",
      "    accuracy                           0.89      2486\n",
      "   macro avg       0.91      0.84      0.86      2486\n",
      "weighted avg       0.91      0.89      0.89      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.6923    0.1731    0.2769       104\n",
      "           0     0.4176    0.8945    0.5693       218\n",
      "           1     0.8511    0.5298    0.6531       453\n",
      "\n",
      "    accuracy                         0.5845       775\n",
      "   macro avg     0.6536    0.5325    0.4998       775\n",
      "weighted avg     0.7078    0.5845    0.5790       775\n",
      "\n",
      "Naive Bayes + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.97      0.93      0.95       333\n",
      "           0       0.98      0.93      0.96       917\n",
      "           1       0.94      0.99      0.97      1236\n",
      "\n",
      "    accuracy                           0.96      2486\n",
      "   macro avg       0.97      0.95      0.96      2486\n",
      "weighted avg       0.96      0.96      0.96      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9167    0.2115    0.3437       104\n",
      "           0     0.5103    0.3394    0.4077       218\n",
      "           1     0.6617    0.8852    0.7573       453\n",
      "\n",
      "    accuracy                         0.6413       775\n",
      "   macro avg     0.6962    0.4787    0.5029       775\n",
      "weighted avg     0.6533    0.6413    0.6035       775\n",
      "\n",
      "Logistic Regression + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.52      0.68       333\n",
      "           0       0.92      0.97      0.95       917\n",
      "           1       0.91      1.00      0.95      1236\n",
      "\n",
      "    accuracy                           0.92      2486\n",
      "   macro avg       0.94      0.83      0.86      2486\n",
      "weighted avg       0.93      0.92      0.91      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.7812    0.2404    0.3676       104\n",
      "           0     0.5314    0.5826    0.5558       218\n",
      "           1     0.7361    0.8190    0.7753       453\n",
      "\n",
      "    accuracy                         0.6748       775\n",
      "   macro avg     0.6829    0.5473    0.5663       775\n",
      "weighted avg     0.6846    0.6748    0.6589       775\n",
      "\n",
      "SVM + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.96      0.98       333\n",
      "           0       0.99      0.99      0.99       917\n",
      "           1       0.99      1.00      0.99      1236\n",
      "\n",
      "    accuracy                           0.99      2486\n",
      "   macro avg       0.99      0.98      0.99      2486\n",
      "weighted avg       0.99      0.99      0.99      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.8462    0.2115    0.3385       104\n",
      "           0     0.5440    0.4541    0.4950       218\n",
      "           1     0.6966    0.8720    0.7745       453\n",
      "\n",
      "    accuracy                         0.6658       775\n",
      "   macro avg     0.6956    0.5125    0.5360       775\n",
      "weighted avg     0.6738    0.6658    0.6374       775\n",
      "\n",
      "Naive Bayes + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.20      0.34       333\n",
      "           0       0.94      0.93      0.94       917\n",
      "           1       0.82      1.00      0.90      1236\n",
      "\n",
      "    accuracy                           0.87      2486\n",
      "   macro avg       0.92      0.71      0.72      2486\n",
      "weighted avg       0.89      0.87      0.84      2486\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     1.0000    0.0481    0.0917       104\n",
      "           0     0.5570    0.2018    0.2963       218\n",
      "           1     0.6194    0.9448    0.7483       453\n",
      "\n",
      "    accuracy                         0.6155       775\n",
      "   macro avg     0.7255    0.3982    0.3788       775\n",
      "weighted avg     0.6529    0.6155    0.5330       775\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "svm = SVC()\n",
    "\n",
    "all_train = [bow_train,tfidf_train]\n",
    "all_test = [bow_test,tfidf_test]\n",
    "all_vectorisers = [\"BoW\",\"TfIdf\"]\n",
    "\n",
    "for train,test,vectoriser in zip(all_train,all_test,all_vectorisers):\n",
    "    logreg.fit(train,trainvalagg.label)\n",
    "    \n",
    "    print(\"Logistic Regression + \" + vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, logreg.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, logreg.predict(test),digits=4)) \n",
    "    \n",
    "    svm.fit(train,trainvalagg.label)\n",
    "    \n",
    "    print(\"SVM + \" + vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, svm.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, svm.predict(test),digits=4)) \n",
    "\n",
    "    # Fasttest dont work w NaiveBayes\n",
    "    nb.fit(train,trainvalagg.label)\n",
    "    print(\"Naive Bayes + \"+ vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(trainvalagg.label, nb.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(testagg.label, nb.predict(test),digits=4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5813514360313315"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(34/383)*0.2769  + (104/383)*0.5693 + (245/383)*0.6531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5330452903225806"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(104/775)*0.0917 + (218/775)*0.2963 + (453/775)*0.7483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
