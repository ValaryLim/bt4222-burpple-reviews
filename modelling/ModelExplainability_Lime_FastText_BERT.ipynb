{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# models\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from scipy.special import softmax\n",
    "import fasttext\n",
    "\n",
    "#lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visualisation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample data\n",
    "all_sample = pd.read_csv(\"data/stacking_folds/ALL_LABELLED_DATA.csv\")[['phrase', 'phrase_stem']]\n",
    "print(all_sample.shape)\n",
    "all_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-carrier",
   "metadata": {},
   "source": [
    "# LIME Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_dict_of_scores(d, lime_exp, label):\n",
    "    l = lime_exp.as_list(label=label)\n",
    "    for item in l:\n",
    "        key = item[0]\n",
    "        val = item[1]\n",
    "        if key in d:\n",
    "            d[key].append(val)\n",
    "        else:\n",
    "            d[key] = [val]\n",
    "            \n",
    "def dict_to_df(d, newcols):\n",
    "    token_df =  pd.DataFrame([d]).T\n",
    "    token_df = token_df.reset_index()\n",
    "    token_df.columns = newcols\n",
    "    return token_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-riding",
   "metadata": {},
   "source": [
    "# FastText + LIME Analysis\n",
    "\n",
    "Reference: https://medium.com/@ageitgey/natural-language-processing-is-fun-part-3-explaining-model-predictions-486d8616813c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read model\n",
    "FASTTEXT_MODEL = fasttext.load_model(\"saved_models/model_fasttext.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_prediction_in_sklearn_format(classifier, texts):\n",
    "    '''\n",
    "    This helper function generates fasttext predictions in sklearn format\n",
    "    for inputting into LIME.\n",
    "    \n",
    "    Inputs:\n",
    "        classifier (FastText object): (Trained) FastText model.\n",
    "        texts (str): Text to analyse.\n",
    "    '''\n",
    "    # initialise list to store results\n",
    "    res = []\n",
    "    # predict classes and probabilities\n",
    "    # raw output: (('__label__pos', '__label__zer', '__label__neg'), array([0.74627936, 0.19218659, 0.06156404]))\n",
    "    labels, probabilities = classifier.predict(texts, k=-1)\n",
    "\n",
    "    # for each prediction, sort the probabaility scores into the same order\n",
    "    for label, probs, text in zip(labels, probabilities, texts):\n",
    "        order = np.argsort(np.array(label)) # sorted in: neg, pos, zer (alphabetical)\n",
    "        res.append(probs[order])\n",
    "\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define phrase to use \n",
    "phrase_ver = \"phrase_stem\"\n",
    "\n",
    "# initialise dictionaries\n",
    "d_neg = defaultdict()\n",
    "d_neu = defaultdict()\n",
    "d_pos = defaultdict()\n",
    "class_names = [-1, 1, 0] #Â neg, pos, zer\n",
    "\n",
    "# initialise explainer\n",
    "explainer = LimeTextExplainer(class_names = class_names, random_state=42)\n",
    "\n",
    "for i in tqdm(range(len(all_sample))):\n",
    "    current_text = all_sample[phrase_ver].iloc[i]\n",
    "    exp = explainer.explain_instance(current_text, classifier_fn=lambda x: fasttext_prediction_in_sklearn_format(FASTTEXT_MODEL, x), labels=class_names)\n",
    "    append_dict_of_scores(d_neg, exp, -1)\n",
    "    append_dict_of_scores(d_neu, exp, 0)\n",
    "    append_dict_of_scores(d_pos, exp, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgDict_pos = {}\n",
    "avgDict_neu = {}\n",
    "avgDict_neg = {}\n",
    "\n",
    "for k,v in d_pos.items():\n",
    "    # v is the list of impact on probability of predicting a class for a particular token\n",
    "    avgDict_pos[k] = sum(v)/ float(len(v))\n",
    "for k,v in d_neg.items():\n",
    "    # v is the list of impact on probability of predicting a class for a particular token\n",
    "    avgDict_neg[k] = sum(v)/ float(len(v))\n",
    "for k,v in d_neu.items():\n",
    "    # v is the list of impact on probability of predicting a class for a particular token\n",
    "    avgDict_neu[k] = sum(v)/ float(len(v))\n",
    "\n",
    "pos=dict_to_df(avgDict_pos, ['token', 'average_pos_impact'])\n",
    "neg=dict_to_df(avgDict_neg, ['token', 'average_neg_impact'])\n",
    "neu=dict_to_df(avgDict_neu, ['token', 'average_neu_impact'])\n",
    "fasttext_lime = pos.merge(neg, on='token', how = 'inner').merge(neu, on='token', how = 'inner')\n",
    "fasttext_lime.sort_values(['average_pos_impact'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_lime.to_csv('explain_results/fasttext_lime.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-restaurant",
   "metadata": {},
   "source": [
    "## Plot Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_eval = pd.read_csv(\"data/explain_results/fasttext_lime.csv\")\n",
    "\n",
    "rf_eval[\"mag_neg\"] = np.abs(rf_eval.average_neg_impact)\n",
    "rf_eval[\"mag_neu\"] = np.abs(rf_eval.average_neu_impact)\n",
    "rf_eval[\"mag_pos\"] = np.abs(rf_eval.average_pos_impact)\n",
    "\n",
    "rf_eval[\"average_pos_impact\"] = -rf_eval[\"average_pos_impact\"]\n",
    "rf_eval[\"average_neg_impact\"] = -rf_eval[\"average_neg_impact\"]\n",
    "rf_eval[\"average_neu_impact\"] = -rf_eval[\"average_neu_impact\"]\n",
    "\n",
    "rf_eval_neg = rf_eval.nlargest(20, \"mag_neg\")\n",
    "rf_eval_neu = rf_eval.nlargest(20, \"mag_neu\")\n",
    "rf_eval_pos = rf_eval.nlargest(20, \"mag_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(rf_eval_neg.token, rf_eval_neg.average_neg_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in rf_eval_neg.average_neg_impact])\n",
    "plt.title('y=-1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(rf_eval_neu.token, rf_eval_neu.average_neu_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in rf_eval_neu.average_neu_impact])\n",
    "plt.title('y=0.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(rf_eval_pos.token, rf_eval_pos.average_pos_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in rf_eval_pos.average_pos_impact])\n",
    "plt.title('y=1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-hamilton",
   "metadata": {},
   "source": [
    "# BERT + LIME Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read model\n",
    "bert_model_args = ClassificationArgs(num_train_epochs=2, learning_rate=5e-5)\n",
    "BERT_MODEL = ClassificationModel(model_type = 'bert', \\\n",
    "                                 model_name = 'saved_models/model_bert', \\\n",
    "                                 args = bert_model_args, use_cuda = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_prediction_in_sklearn_format(classifier, texts):\n",
    "    '''\n",
    "    This helper function generates BERT predictions in sklearn format\n",
    "    for inputting into LIME.\n",
    "    '''\n",
    "    # initialise list to store results\n",
    "    res = []\n",
    "    # predict classes and probabilities\n",
    "    \n",
    "    bert_pred, bert_raw_outputs = classifier.predict(texts)\n",
    "    # convert raw output to probabilities\n",
    "    bert_probabilities = softmax(bert_raw_outputs, axis=1)\n",
    "    prob_neu = bert_probabilities[:, 0]\n",
    "    prob_pos = bert_probabilities[:, 1]\n",
    "    prob_neg = bert_probabilities[:, 2]\n",
    "    \n",
    "    for i in range(len(prob_neu)):\n",
    "        res.append([prob_neg[i], prob_neu[i], prob_pos[i]])\n",
    "    \n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-ranch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define phrase to use \n",
    "phrase_ver = \"phrase\"\n",
    "\n",
    "# initialise dictionaries\n",
    "d_neg = defaultdict()\n",
    "d_neu = defaultdict()\n",
    "d_pos = defaultdict()\n",
    "class_names = [-1, 0, 1]\n",
    "\n",
    "# initialise explainer\n",
    "explainer = LimeTextExplainer(class_names = class_names, random_state=42)\n",
    "\n",
    "for i in tqdm(range(len(all_sample))):\n",
    "    current_text = all_sample[phrase_ver].iloc[i]\n",
    "    exp = explainer.explain_instance(current_text, \\\n",
    "                                     classifier_fn=lambda x: BERT_prediction_in_sklearn_format(BERT_MODEL, x), \\\n",
    "                                     labels=class_names, \\\n",
    "                                     num_features=100, num_samples=10) # reduced as BERT predictions take very long to run\n",
    "    append_dict_of_scores(d_neg, exp, -1)\n",
    "    append_dict_of_scores(d_neu, exp, 0)\n",
    "    append_dict_of_scores(d_pos, exp, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgDict_pos = {}\n",
    "avgDict_neu = {}\n",
    "avgDict_neg = {}\n",
    "\n",
    "for k,v in d_pos.items():\n",
    "    # v is the list of impact on probability of predicting a class for a particular token\n",
    "    avgDict_pos[k] = sum(v)/ float(len(v))\n",
    "for k,v in d_neg.items():\n",
    "    # v is the list of impact on probability of predicting a class for a particular token\n",
    "    avgDict_neg[k] = sum(v)/ float(len(v))\n",
    "for k,v in d_neu.items():\n",
    "    # v is the list of impact on probability of predicting a class for a particular token\n",
    "    avgDict_neu[k] = sum(v)/ float(len(v))\n",
    "\n",
    "pos=dict_to_df(avgDict_pos, ['token', 'average_pos_impact'])\n",
    "neg=dict_to_df(avgDict_neg, ['token', 'average_neg_impact'])\n",
    "neu=dict_to_df(avgDict_neu, ['token', 'average_neu_impact'])\n",
    "bert_lime = pos.merge(neg, on='token', how = 'inner').merge(neu, on='token', how = 'inner')\n",
    "bert_lime.sort_values(['average_pos_impact'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_lime.to_csv('explain_results/bert_lime.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-society",
   "metadata": {},
   "source": [
    "## Plot Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_eval = pd.read_csv(\"explain_results/bert_lime.csv\")\n",
    "\n",
    "rf_eval[\"mag_neg\"] = np.abs(rf_eval.average_neg_impact)\n",
    "rf_eval[\"mag_neu\"] = np.abs(rf_eval.average_neu_impact)\n",
    "rf_eval[\"mag_pos\"] = np.abs(rf_eval.average_pos_impact)\n",
    "\n",
    "rf_eval[\"average_pos_impact\"] = -rf_eval[\"average_pos_impact\"]\n",
    "rf_eval[\"average_neg_impact\"] = -rf_eval[\"average_neg_impact\"]\n",
    "rf_eval[\"average_neu_impact\"] = -rf_eval[\"average_neu_impact\"]\n",
    "\n",
    "rf_eval_neg = rf_eval.nlargest(20, \"mag_neg\")\n",
    "rf_eval_neu = rf_eval.nlargest(20, \"mag_neu\")\n",
    "rf_eval_pos = rf_eval.nlargest(20, \"mag_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(rf_eval_neg.token, rf_eval_neg.average_neg_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in rf_eval_neg.average_neg_impact])\n",
    "plt.title('y=-1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(rf_eval_neu.token, rf_eval_neu.average_neu_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in rf_eval_neu.average_neu_impact])\n",
    "plt.title('y=0.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(rf_eval_pos.token, rf_eval_pos.average_pos_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in rf_eval_pos.average_pos_impact])\n",
    "plt.title('y=1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-elite",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-confidence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
