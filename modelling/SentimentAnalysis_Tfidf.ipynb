{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# text explainer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/labelled_data/train_newpreproc_emoticon.csv\", index_col=0)\n",
    "val = pd.read_csv(\"data/labelled_data/val_newpreproc_emoticon.csv\", index_col=0)\n",
    "test = pd.read_csv(\"data/labelled_data/test_newpreproc_emoticon.csv\", index_col=0)\n",
    "\n",
    "trainval = pd.concat([train, val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_params = {\n",
    "    \"analyzer\": [\"word\"],\n",
    "    \"lowercase\": [True],\n",
    "    \"ngram_range\": [(1,1), (1,2), (1,3)],\n",
    "    \"max_df\": [0.25, 0.5, 1.0],\n",
    "    \"min_df\": [1, 10, 20]\n",
    "}\n",
    "tfidf_paramgrid = list(ParameterGrid(tfidf_params))\n",
    "\n",
    "type_proc_params = {\n",
    "    \"type\": [\"normal\", \"stem\", \"lemma\"], # normal, lemma, stem\n",
    "    \"processing\": [\"original\", \"generic\", \"unique\"] # new phrase splitting\n",
    "}\n",
    "type_proc_paramgrid = list(ParameterGrid(type_proc_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "logreg_params = {\n",
    "    \"C\": [0.1, 0.5, 1.0, 1.5, 5],\n",
    "    \"solver\": [\"lbfgs\", \"newton-cg\"],\n",
    "    \"penalty\": [\"l2\", \"none\"],\n",
    "    \"class_weight\": [\"balanced\", None] \n",
    "}\n",
    "logreg_paramgrid = list(ParameterGrid(logreg_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "nb_params = {\n",
    "    \"alpha\": [0, 0.001, 0.01, 0.1, 0.25, 0.5, 1]\n",
    "}\n",
    "nb_paramgrid = list(ParameterGrid(nb_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "svm_params = {\n",
    "    \"C\": [0.1, 0.5, 1.0, 1.5, 5],\n",
    "    \"kernel\": [\"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"],\n",
    "    \"class_weight\": [\"balanced\", None] \n",
    "}\n",
    "\n",
    "svm_paramgrid = list(ParameterGrid(svm_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = [\n",
    "    {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"class_weight\": [\"balanced\", \"balanced_subsample\", None],\n",
    "        \"max_features\": [\"auto\", \"sqrt\"],\n",
    "        \"min_samples_leaf\": [1, 2, 4]\n",
    "    }\n",
    "]\n",
    "\n",
    "rf_paramgrid = list(ParameterGrid(rf_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_params = { \"strategy\": [\"prior\"] }\n",
    "dummy_paramgrid = list(ParameterGrid(dummy_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(tfidf_param):\n",
    "    # original\n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train_emoticon_generic = tfidf.fit_transform(train.phrase_emoticon_generic)\n",
    "    tfidf_val_emoticon_generic = tfidf.transform(val.phrase_emoticon_generic)\n",
    "    tfidf_test_emoticon_generic = tfidf.transform(test.phrase_emoticon_generic)\n",
    "    tfidf_trainval_emoticon_generic = tfidf.transform(trainval.phrase_emoticon_generic)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train_emoticon_unique = tfidf.fit_transform(train.phrase_emoticon_unique)\n",
    "    tfidf_val_emoticon_unique = tfidf.transform(val.phrase_emoticon_unique)\n",
    "    tfidf_test_emoticon_unique = tfidf.transform(test.phrase_emoticon_unique)\n",
    "    tfidf_trainval_emoticon_unique = tfidf.transform(trainval.phrase_emoticon_unique)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train = tfidf.fit_transform(train.phrase)\n",
    "    tfidf_val = tfidf.transform(val.phrase)\n",
    "    tfidf_test = tfidf.transform(test.phrase)\n",
    "    tfidf_trainval = tfidf.transform(trainval.phrase)\n",
    "    \n",
    "    # lemmatize\n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train_emoticon_generic_lemma = tfidf.fit_transform(train.phrase_lemma_emoticon_generic)\n",
    "    tfidf_val_emoticon_generic_lemma = tfidf.transform(val.phrase_lemma_emoticon_generic)\n",
    "    tfidf_test_emoticon_generic_lemma = tfidf.transform(test.phrase_lemma_emoticon_generic)\n",
    "    tfidf_trainval_emoticon_generic_lemma = tfidf.transform(trainval.phrase_lemma_emoticon_generic)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train_emoticon_unique_lemma = tfidf.fit_transform(train.phrase_lemma_emoticon_unique)\n",
    "    tfidf_val_emoticon_unique_lemma = tfidf.transform(val.phrase_lemma_emoticon_unique)\n",
    "    tfidf_test_emoticon_unique_lemma = tfidf.transform(test.phrase_lemma_emoticon_unique)\n",
    "    tfidf_trainval_emoticon_unique_lemma = tfidf.transform(trainval.phrase_lemma_emoticon_unique)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train_lemma = tfidf.fit_transform(train.phrase_lemma)\n",
    "    tfidf_val_lemma = tfidf.transform(val.phrase_lemma)\n",
    "    tfidf_test_lemma = tfidf.transform(test.phrase_lemma)\n",
    "    tfidf_trainval_lemma = tfidf.transform(trainval.phrase_lemma)\n",
    "    \n",
    "    # stem\n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train_emoticon_generic_stem = tfidf.fit_transform(train.phrase_stem_emoticon_generic)\n",
    "    tfidf_val_emoticon_generic_stem = tfidf.transform(val.phrase_stem_emoticon_generic)\n",
    "    tfidf_test_emoticon_generic_stem = tfidf.transform(test.phrase_stem_emoticon_generic)\n",
    "    tfidf_trainval_emoticon_generic_stem = tfidf.transform(trainval.phrase_stem_emoticon_generic)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train_emoticon_unique_stem = tfidf.fit_transform(train.phrase_stem_emoticon_unique)\n",
    "    tfidf_val_emoticon_unique_stem = tfidf.transform(val.phrase_stem_emoticon_unique)\n",
    "    tfidf_test_emoticon_unique_stem = tfidf.transform(test.phrase_stem_emoticon_unique)\n",
    "    tfidf_trainval_emoticon_unique_stem = tfidf.transform(trainval.phrase_stem_emoticon_unique)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train_stem = tfidf.fit_transform(train.phrase_stem)\n",
    "    tfidf_val_stem = tfidf.transform(val.phrase_stem)\n",
    "    tfidf_test_stem = tfidf.transform(test.phrase_stem)\n",
    "    tfidf_trainval_stem = tfidf.transform(trainval.phrase_stem)\n",
    "    \n",
    "    return {\n",
    "        \"normal\": {\n",
    "            \"original\": [tfidf_train, tfidf_val, tfidf_test, tfidf_trainval],\n",
    "            \"generic\": [tfidf_train_emoticon_generic, tfidf_val_emoticon_generic, tfidf_test_emoticon_generic, tfidf_trainval_emoticon_generic],\n",
    "            \"unique\": [tfidf_train_emoticon_unique, tfidf_val_emoticon_unique, tfidf_test_emoticon_unique, tfidf_trainval_emoticon_unique]\n",
    "        },\n",
    "        \"lemma\": {\n",
    "            \"original\": [tfidf_train_lemma, tfidf_val_lemma, tfidf_test_lemma, tfidf_trainval_lemma],\n",
    "            \"generic\": [tfidf_train_emoticon_generic_lemma, tfidf_val_emoticon_generic_lemma, tfidf_test_emoticon_generic_lemma, tfidf_trainval_emoticon_generic_lemma],\n",
    "            \"unique\": [tfidf_train_emoticon_unique_lemma, tfidf_val_emoticon_unique_lemma, tfidf_test_emoticon_unique_lemma, tfidf_trainval_emoticon_unique_lemma]\n",
    "        },\n",
    "        \"stem\": {\n",
    "            \"original\": [tfidf_train_stem, tfidf_val_stem, tfidf_test_stem, tfidf_trainval_stem],\n",
    "            \"generic\": [tfidf_train_emoticon_generic_stem, tfidf_val_emoticon_generic_stem, tfidf_test_emoticon_generic_stem, tfidf_trainval_emoticon_generic_stem],\n",
    "            \"unique\": [tfidf_train_emoticon_unique_stem, tfidf_val_emoticon_unique_stem, tfidf_test_emoticon_unique_stem, tfidf_trainval_emoticon_unique_stem]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"original\": [train.label, val.label, test.label, trainval.label],\n",
    "    \"generic\": [train.label, val.label, test.label, trainval.label],\n",
    "    \"unique\": [train.label, val.label, test.label, trainval.label]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"logreg\"\n",
    "model_fn = LogisticRegression\n",
    "model_paramgrid = logreg_paramgrid\n",
    "\n",
    "gridsearch_results = []\n",
    "for tfidf_param in tqdm(tfidf_paramgrid):\n",
    "    datasets = prepare_datasets(tfidf_param)\n",
    "    \n",
    "    for type_proc_param in type_proc_paramgrid:\n",
    "        data_type = type_proc_param[\"type\"]\n",
    "        data_proc = type_proc_param[\"processing\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_proc][0]\n",
    "        val_set = datasets[data_type][data_proc][1]\n",
    "        test_set = datasets[data_type][data_proc][2]\n",
    "        trainval_set = datasets[data_type][data_proc][3]\n",
    "        \n",
    "        train_label = labels[data_proc][0]\n",
    "        val_label = labels[data_proc][1]\n",
    "        test_label = labels[data_proc][2]\n",
    "        trainval_label = labels[data_proc][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # scoring\n",
    "            val_metrics = classification_report(val_label, val_pred, output_dict=True)\n",
    "            val_accuracy = val_metrics[\"accuracy\"]\n",
    "            val_f1_weighted = val_metrics[\"weighted avg\"][\"f1-score\"]\n",
    "            val_f1_neg = val_metrics[\"-1.0\"][\"f1-score\"]\n",
    "            val_f1_zero = val_metrics[\"0.0\"][\"f1-score\"]\n",
    "            val_f1_pos = val_metrics[\"1.0\"][\"f1-score\"]\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # scoring\n",
    "            test_metrics = classification_report(test_label, test_pred, output_dict=True)\n",
    "            test_accuracy = test_metrics[\"accuracy\"]\n",
    "            test_f1_weighted = test_metrics[\"weighted avg\"][\"f1-score\"]\n",
    "            test_f1_neg = test_metrics[\"-1.0\"][\"f1-score\"]\n",
    "            test_f1_zero = test_metrics[\"0.0\"][\"f1-score\"]\n",
    "            test_f1_pos = test_metrics[\"1.0\"][\"f1-score\"]\n",
    "\n",
    "            results = { \"model\": model_name }\n",
    "            results.update(tfidf_param)\n",
    "            results.update(type_proc_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1_weighted\": val_f1_weighted, \"val_f1_neg\": val_f1_neg, \n",
    "                            \"val_f1_zero\": val_f1_zero, \"val_f1_pos\": val_f1_pos,\n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1_weighted\": test_f1_weighted, \"test_f1_neg\": test_f1_neg, \n",
    "                            \"test_f1_zero\": test_f1_zero, \"test_f1_pos\": test_f1_pos,\n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            \n",
    "final_logreg_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_logreg_results = final_logreg_results.sort_values(by=[\"val_f1_weighted\", \"test_f1_weighted\"], ascending=False)\n",
    "# final_logreg_results.to_csv(\"model_results/tfidf/logreg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nb\"\n",
    "model_fn = MultinomialNB\n",
    "model_paramgrid = nb_paramgrid\n",
    "\n",
    "gridsearch_results = []\n",
    "for tfidf_param in tqdm(tfidf_paramgrid):\n",
    "    datasets = prepare_datasets(tfidf_param)\n",
    "    \n",
    "    for type_proc_param in type_proc_paramgrid:\n",
    "        data_type = type_proc_param[\"type\"]\n",
    "        data_proc = type_proc_param[\"processing\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_proc][0]\n",
    "        val_set = datasets[data_type][data_proc][1]\n",
    "        test_set = datasets[data_type][data_proc][2]\n",
    "        trainval_set = datasets[data_type][data_proc][3]\n",
    "        \n",
    "        train_label = labels[data_proc][0]\n",
    "        val_label = labels[data_proc][1]\n",
    "        test_label = labels[data_proc][2]\n",
    "        trainval_label = labels[data_proc][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # scoring\n",
    "            val_metrics = classification_report(val_label, val_pred, output_dict=True)\n",
    "            val_accuracy = val_metrics[\"accuracy\"]\n",
    "            val_f1_weighted = val_metrics[\"weighted avg\"][\"f1-score\"]\n",
    "            val_f1_neg = val_metrics[\"-1.0\"][\"f1-score\"]\n",
    "            val_f1_zero = val_metrics[\"0.0\"][\"f1-score\"]\n",
    "            val_f1_pos = val_metrics[\"1.0\"][\"f1-score\"]\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # scoring\n",
    "            test_metrics = classification_report(test_label, test_pred, output_dict=True)\n",
    "            test_accuracy = test_metrics[\"accuracy\"]\n",
    "            test_f1_weighted = test_metrics[\"weighted avg\"][\"f1-score\"]\n",
    "            test_f1_neg = test_metrics[\"-1.0\"][\"f1-score\"]\n",
    "            test_f1_zero = test_metrics[\"0.0\"][\"f1-score\"]\n",
    "            test_f1_pos = test_metrics[\"1.0\"][\"f1-score\"]\n",
    "\n",
    "            results = { \"model\": model_name }\n",
    "            results.update(tfidf_param)\n",
    "            results.update(type_proc_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1_weighted\": val_f1_weighted, \"val_f1_neg\": val_f1_neg, \n",
    "                            \"val_f1_zero\": val_f1_zero, \"val_f1_pos\": val_f1_pos,\n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1_weighted\": test_f1_weighted, \"test_f1_neg\": test_f1_neg, \n",
    "                            \"test_f1_zero\": test_f1_zero, \"test_f1_pos\": test_f1_pos,\n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            \n",
    "final_nb_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_nb_results = final_nb_results.sort_values(by=[\"val_f1_weighted\", \"test_f1_weighted\"], ascending=False)\n",
    "final_nb_results.to_csv(\"model_results/tfidf/nb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"rf\"\n",
    "model_fn = RandomForestClassifier\n",
    "model_paramgrid = rf_paramgrid\n",
    "\n",
    "gridsearch_results = []\n",
    "for tfidf_param in tqdm(tfidf_paramgrid):\n",
    "    datasets = prepare_datasets(tfidf_param)\n",
    "    \n",
    "    for type_proc_param in type_proc_paramgrid:\n",
    "        data_type = type_proc_param[\"type\"]\n",
    "        data_proc = type_proc_param[\"processing\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_proc][0]\n",
    "        val_set = datasets[data_type][data_proc][1]\n",
    "        test_set = datasets[data_type][data_proc][2]\n",
    "        trainval_set = datasets[data_type][data_proc][3]\n",
    "        \n",
    "        train_label = labels[data_proc][0]\n",
    "        val_label = labels[data_proc][1]\n",
    "        test_label = labels[data_proc][2]\n",
    "        trainval_label = labels[data_proc][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # scoring\n",
    "            val_metrics = classification_report(val_label, val_pred, output_dict=True)\n",
    "            val_accuracy = val_metrics[\"accuracy\"]\n",
    "            val_f1_weighted = val_metrics[\"weighted avg\"][\"f1-score\"]\n",
    "            val_f1_neg = val_metrics[\"-1.0\"][\"f1-score\"]\n",
    "            val_f1_zero = val_metrics[\"0.0\"][\"f1-score\"]\n",
    "            val_f1_pos = val_metrics[\"1.0\"][\"f1-score\"]\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # scoring\n",
    "            test_metrics = classification_report(test_label, test_pred, output_dict=True)\n",
    "            test_accuracy = test_metrics[\"accuracy\"]\n",
    "            test_f1_weighted = test_metrics[\"weighted avg\"][\"f1-score\"]\n",
    "            test_f1_neg = test_metrics[\"-1.0\"][\"f1-score\"]\n",
    "            test_f1_zero = test_metrics[\"0.0\"][\"f1-score\"]\n",
    "            test_f1_pos = test_metrics[\"1.0\"][\"f1-score\"]\n",
    "\n",
    "            results = { \"model\": model_name }\n",
    "            results.update(tfidf_param)\n",
    "            results.update(type_proc_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1_weighted\": val_f1_weighted, \"val_f1_neg\": val_f1_neg, \n",
    "                            \"val_f1_zero\": val_f1_zero, \"val_f1_pos\": val_f1_pos,\n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1_weighted\": test_f1_weighted, \"test_f1_neg\": test_f1_neg, \n",
    "                            \"test_f1_zero\": test_f1_zero, \"test_f1_pos\": test_f1_pos,\n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            \n",
    "final_rf_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_rf_results = final_rf_results.sort_values(by=[\"val_f1_weighted\", \"test_f1_weighted\"], ascending=False)\n",
    "final_rf_results.to_csv(\"model_results/tfidf/rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"svm\"\n",
    "model_fn = SVC\n",
    "model_paramgrid = svm_paramgrid\n",
    "\n",
    "gridsearch_results = []\n",
    "for tfidf_param in tqdm(tfidf_paramgrid):\n",
    "    datasets = prepare_datasets(tfidf_param)\n",
    "    \n",
    "    for type_proc_param in type_proc_paramgrid:\n",
    "        data_type = type_proc_param[\"type\"]\n",
    "        data_proc = type_proc_param[\"processing\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_proc][0]\n",
    "        val_set = datasets[data_type][data_proc][1]\n",
    "        test_set = datasets[data_type][data_proc][2]\n",
    "        trainval_set = datasets[data_type][data_proc][3]\n",
    "        \n",
    "        train_label = labels[data_proc][0]\n",
    "        val_label = labels[data_proc][1]\n",
    "        test_label = labels[data_proc][2]\n",
    "        trainval_label = labels[data_proc][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # scoring\n",
    "            val_metrics = classification_report(val_label, val_pred, output_dict=True)\n",
    "            val_accuracy = val_metrics[\"accuracy\"]\n",
    "            val_f1_weighted = val_metrics[\"weighted avg\"][\"f1-score\"]\n",
    "            val_f1_neg = val_metrics[\"-1.0\"][\"f1-score\"]\n",
    "            val_f1_zero = val_metrics[\"0.0\"][\"f1-score\"]\n",
    "            val_f1_pos = val_metrics[\"1.0\"][\"f1-score\"]\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # scoring\n",
    "            test_metrics = classification_report(test_label, test_pred, output_dict=True)\n",
    "            test_accuracy = test_metrics[\"accuracy\"]\n",
    "            test_f1_weighted = test_metrics[\"weighted avg\"][\"f1-score\"]\n",
    "            test_f1_neg = test_metrics[\"-1.0\"][\"f1-score\"]\n",
    "            test_f1_zero = test_metrics[\"0.0\"][\"f1-score\"]\n",
    "            test_f1_pos = test_metrics[\"1.0\"][\"f1-score\"]\n",
    "\n",
    "            results = { \"model\": model_name }\n",
    "            results.update(tfidf_param)\n",
    "            results.update(type_proc_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1_weighted\": val_f1_weighted, \"val_f1_neg\": val_f1_neg, \n",
    "                            \"val_f1_zero\": val_f1_zero, \"val_f1_pos\": val_f1_pos,\n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1_weighted\": test_f1_weighted, \"test_f1_neg\": test_f1_neg, \n",
    "                            \"test_f1_zero\": test_f1_zero, \"test_f1_pos\": test_f1_pos,\n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            \n",
    "final_svm_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_svm_results = final_svm_results.sort_values(by=[\"val_f1_weighted\", \"test_f1_weighted\"], ascending=False)\n",
    "final_svm_results.to_csv(\"model_results/tfidf/svm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dummy\"\n",
    "model_fn = DummyClassifier\n",
    "model_paramgrid = dummy_paramgrid\n",
    "\n",
    "gridsearch_results = []\n",
    "for tfidf_param in tqdm(tfidf_paramgrid):\n",
    "    datasets = prepare_datasets(tfidf_param)\n",
    "    \n",
    "    for type_proc_param in type_proc_paramgrid:\n",
    "        data_type = type_proc_param[\"type\"]\n",
    "        data_proc = type_proc_param[\"processing\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_proc][0]\n",
    "        val_set = datasets[data_type][data_proc][1]\n",
    "        test_set = datasets[data_type][data_proc][2]\n",
    "        trainval_set = datasets[data_type][data_proc][3]\n",
    "        \n",
    "        train_label = labels[data_proc][0]\n",
    "        val_label = labels[data_proc][1]\n",
    "        test_label = labels[data_proc][2]\n",
    "        trainval_label = labels[data_proc][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # scoring\n",
    "            val_metrics = classification_report(val_label, val_pred, output_dict=True)\n",
    "            val_accuracy = val_metrics[\"accuracy\"]\n",
    "            val_f1_weighted = val_metrics[\"weighted avg\"][\"f1-score\"]\n",
    "            val_f1_neg = val_metrics[\"-1\"][\"f1-score\"]\n",
    "            val_f1_zero = val_metrics[\"0\"][\"f1-score\"]\n",
    "            val_f1_pos = val_metrics[\"1\"][\"f1-score\"]\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # scoring\n",
    "            test_metrics = classification_report(test_label, test_pred, output_dict=True)\n",
    "            test_accuracy = test_metrics[\"accuracy\"]\n",
    "            test_f1_weighted = test_metrics[\"weighted avg\"][\"f1-score\"]\n",
    "            test_f1_neg = test_metrics[\"-1\"][\"f1-score\"]\n",
    "            test_f1_zero = test_metrics[\"0\"][\"f1-score\"]\n",
    "            test_f1_pos = test_metrics[\"1\"][\"f1-score\"]\n",
    "\n",
    "            results = { \"model\": model_name }\n",
    "            results.update(tfidf_param)\n",
    "            results.update(type_proc_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1_weighted\": val_f1_weighted, \"val_f1_neg\": val_f1_neg, \n",
    "                            \"val_f1_zero\": val_f1_zero, \"val_f1_pos\": val_f1_pos,\n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1_weighted\": test_f1_weighted, \"test_f1_neg\": test_f1_neg, \n",
    "                            \"test_f1_zero\": test_f1_zero, \"test_f1_pos\": test_f1_pos,\n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            \n",
    "final_dummy_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_dummy_results = final_dummy_results.sort_values(by=[\"val_f1_weighted\", \"test_f1_weighted\"], ascending=False)\n",
    "final_dummy_results.to_csv(\"model_results/tfidf/dummy.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([final_logreg_results, final_nb_results, final_svm_results, final_rf_results])\n",
    "combined_df = combined_df.sort_values(by=[\"val_f1_weighted\", \"test_f1_weighted\"], ascending=False)\n",
    "combined_df.to_csv(\"model_results/tfidf/combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([final_logreg_results, final_nb_results, final_svm_results, final_rf_results, \\\n",
    "                        final_dummy_results])\n",
    "combined_df = combined_df.sort_values(by=[\"val_f1_weighted\", \"test_f1_weighted\"], ascending=False)\n",
    "combined_df.to_csv(\"model_results/tfidf/combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions for Meta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_k_fold(model_grid, vectorizer_grid, column, data, model_name):\n",
    "    \n",
    "    # Generate fold predictions\n",
    "    fold_num = 1\n",
    "    for tf_combi in data:\n",
    "        train = tf_combi[0]\n",
    "        predict_on = tf_combi[1]\n",
    "\n",
    "        # Vectorise Data\n",
    "        for v in vectorizer_grid:\n",
    "            vectorizer = TfidfVectorizer(**v)\n",
    "        bow_train = vectorizer.fit_transform(train[column])\n",
    "        bow_predict_on = vectorizer.transform(predict_on[column])\n",
    "        \n",
    "        # Get Labels\n",
    "        train_label = train.label\n",
    "        \n",
    "        # Fit Model\n",
    "        for m in model_grid:\n",
    "            model = model_fn(**m)\n",
    "        model.fit(bow_train, train_label)\n",
    "        predictions = model.predict_proba(bow_predict_on)\n",
    "        \n",
    "        # Create Dataframe and output\n",
    "        df = pd.DataFrame(data=predictions, columns = [model_name+'_prob_neg', model_name+'_prob_neu', model_name+'_prob_pos'])\n",
    "        df.drop(columns= [model_name+'_prob_neu'])\n",
    "        ordered_cols = [model_name+'_prob_pos',model_name+'_prob_neg']\n",
    "        df=df[ordered_cols]\n",
    "        \n",
    "        if fold_num <=5:\n",
    "            path = \"data/fold_predictions/\" + model_name + \"/\" + model_name + '_fold' + str(fold_num) +'.csv'\n",
    "        else:\n",
    "            path = \"data/fold_predictions/\" + model_name + \"/\" + model_name + '_test.csv'\n",
    "        \n",
    "        df.to_csv(path, index=False)\n",
    "        \n",
    "        fold_num +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "fold1 = pd.read_csv('data/stacking_folds/fold1.csv')\n",
    "fold2 = pd.read_csv('data/stacking_folds/fold2.csv')\n",
    "fold3 = pd.read_csv('data/stacking_folds/fold3.csv')\n",
    "fold4 = pd.read_csv('data/stacking_folds/fold4.csv')\n",
    "fold5 = pd.read_csv('data/stacking_folds/fold5.csv')\n",
    "\n",
    "train1 = pd.read_csv('data/stacking_folds/train1.csv')\n",
    "train2 = pd.read_csv('data/stacking_folds/train2.csv')\n",
    "train3 = pd.read_csv('data/stacking_folds/train3.csv')\n",
    "train4 = pd.read_csv('data/stacking_folds/train4.csv')\n",
    "train5 = pd.read_csv('data/stacking_folds/train5.csv')\n",
    "\n",
    "train_all = pd.read_csv('data/stacking_folds/train_all.csv')\n",
    "test = pd.read_csv('data/stacking_folds/test.csv')\n",
    "\n",
    "# store in suitable data structure\n",
    "data = [(train1, fold1), (train2, fold2),(train3, fold3), (train4, fold4), (train5, fold5), (train_all, test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model grid that gives highest validtion weighted F1\n",
    "logreg_params = {\n",
    "    \"C\": [5],\n",
    "    \"solver\": [\"lbfgs\"],\n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"class_weight\": [\"balanced\"] \n",
    "}\n",
    "\n",
    "logreg_paramgrid = list(ParameterGrid(logreg_params))\n",
    "\n",
    "# Instantiate CountVectorizer grid with Params giving highest validation weighted F1\n",
    "logreg_tfidf_params = {\n",
    "    \"analyzer\": [\"word\"],\n",
    "    \"lowercase\": [True],\n",
    "    \"ngram_range\": [(1,2)],\n",
    "    \"max_df\": [0.25],\n",
    "    \"min_df\": [1]\n",
    "}\n",
    "logreg_tfidf_paramgrid = list(ParameterGrid(logreg_tfidf_params))\n",
    "\n",
    "# Best text processing\n",
    "column = 'phrase_stem_emoticon_unique'\n",
    "\n",
    "# Model Function\n",
    "model_fn = LogisticRegression\n",
    "\n",
    "custom_k_fold(logreg_paramgrid, logreg_tfidf_paramgrid, column, data, \"logreg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full model\n",
    "full_df = pd.read_csv(\"data/stacking_folds/all_labelled_data.csv\")\n",
    "\n",
    "tfidf = TfidfVectorizer(**logreg_tfidf_paramgrid[0])\n",
    "tfidf_train_emoticon_unique_stem = tfidf.fit_transform(full_df.phrase_stem_emoticon_unique)\n",
    "\n",
    "final_model = LogisticRegression(**logreg_paramgrid[0])\n",
    "final_model.fit(tfidf_train_emoticon_unique_stem, full_df.label)\n",
    "\n",
    "vect_pkl_filename = \"saved_models/model_logreg_vectorizer.pkl\"\n",
    "model_pkl_filename = \"saved_models/model_logreg.pkl\"\n",
    "with open(model_pkl_filename, 'wb') as file:\n",
    "    pickle.dump(final_model, file)\n",
    "with open(vect_pkl_filename, 'wb') as file:\n",
    "    pickle.dump(tfidf, file)\n",
    "    \n",
    "# final predictions\n",
    "transformed_text = tfidf.transform(full_df.phrase_stem_emoticon_unique)\n",
    "model_pred = final_model.predict_proba(transformed_text)\n",
    "\n",
    "model_name = \"logreg\"\n",
    "df = pd.DataFrame(data=model_pred, columns = [model_name+'_prob_neg', model_name+'_prob_neu', model_name+'_prob_pos'])\n",
    "df.drop(columns= [model_name+'_prob_neu'])\n",
    "ordered_cols = [model_name+'_prob_pos',model_name+'_prob_neg']\n",
    "df=df[ordered_cols]\n",
    "\n",
    "df.to_csv(\"data/fold_predictions/logreg/logreg_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_csv(\"data/explain_results/meta_model_feature_importance.csv\")\n",
    "\n",
    "meta_df_neg = meta_df[meta_df.Class == -1.0]\n",
    "meta_df_neu = meta_df[meta_df.Class == 0.0]\n",
    "meta_df_pos = meta_df[meta_df.Class == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(meta_df_neg.Feature, meta_df_neg.Score, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in meta_df_neg.Score])\n",
    "plt.title('y=-1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(meta_df_neu.Feature, meta_df_neu.Score, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in meta_df_neu.Score])\n",
    "plt.title('y=0.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(meta_df_pos.Feature, meta_df_pos.Score, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in meta_df_pos.Score])\n",
    "plt.title('y=1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "all_train = pd.read_csv('data/stacking_folds/train_all.csv', header = 0)\n",
    "all_test = pd.read_csv('data/stacking_folds/test.csv', header = 0)\n",
    "full_df = pd.concat([all_train, all_test], axis=0).reset_index().drop('index', axis=1)\n",
    "full_df_subset = full_df[[\"new_aspect_1\", \"phrase_stem_emoticon_unique\", \"label\"]]\n",
    "\n",
    "# load saved models\n",
    "vect_pkl_filename = \"saved_models/model_logreg_vectorizer.pkl\"\n",
    "model_pkl_filename = \"saved_models/model_logreg.pkl\"\n",
    "lr_vectorizer = pickle.load(open(vect_pkl_filename, \"rb\"))\n",
    "lr_model = pickle.load(open(model_pkl_filename, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr_eval = eli5.explain_weights_df(lr_model, vec=lr_vectorizer, top=20)\n",
    "lr_eval_neg = lr_eval[lr_eval.target == -1.0]\n",
    "lr_eval_neu = lr_eval[lr_eval.target == 0.0]\n",
    "lr_eval_pos = lr_eval[lr_eval.target == 1.0]\n",
    "\n",
    "# save results\n",
    "lr_eval.to_csv(\"data/explain_results/logreg_lime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(lr_eval_neg.feature, lr_eval_neg.weight, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in lr_eval_neg.weight])\n",
    "plt.title('y=-1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(lr_eval_neu.feature, lr_eval_neu.weight, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in lr_eval_neu.weight])\n",
    "plt.title('y=0.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(lr_eval_pos.feature, lr_eval_pos.weight, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in lr_eval_pos.weight])\n",
    "plt.title('y=1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_eval = pd.read_csv(\"data/explain_results/rf_lime.csv\")\n",
    "\n",
    "rf_eval[\"mag_neg\"] = np.abs(rf_eval.average_neg_impact)\n",
    "rf_eval[\"mag_neu\"] = np.abs(rf_eval.average_neu_impact)\n",
    "rf_eval[\"mag_pos\"] = np.abs(rf_eval.average_pos_impact)\n",
    "\n",
    "rf_eval_neg = rf_eval.nlargest(20, \"mag_neg\")\n",
    "rf_eval_neu = rf_eval.nlargest(20, \"mag_neu\")\n",
    "rf_eval_pos = rf_eval.nlargest(20, \"mag_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(rf_eval_neg.token, rf_eval_neg.average_neg_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in rf_eval_neg.average_neg_impact])\n",
    "plt.title('y=-1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(rf_eval_neu.token, rf_eval_neu.average_neu_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in rf_eval_neu.average_neu_impact])\n",
    "plt.title('y=0.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(rf_eval_pos.token, rf_eval_pos.average_pos_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in rf_eval_pos.average_pos_impact])\n",
    "plt.title('y=1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_eval = pd.read_csv(\"data/explain_results/svm_lime.csv\")\n",
    "\n",
    "svm_eval[\"mag_neg\"] = np.abs(svm_eval.average_neg_impact)\n",
    "svm_eval[\"mag_neu\"] = np.abs(svm_eval.average_neu_impact)\n",
    "svm_eval[\"mag_pos\"] = np.abs(svm_eval.average_pos_impact)\n",
    "\n",
    "svm_eval_neg = svm_eval.nlargest(20, \"mag_neg\")\n",
    "svm_eval_neu = svm_eval.nlargest(20, \"mag_neu\")\n",
    "svm_eval_pos = svm_eval.nlargest(20, \"mag_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(svm_eval_neg.token, svm_eval_neg.average_neg_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in svm_eval_neg.average_neg_impact])\n",
    "plt.title('y=-1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(svm_eval_neu.token, svm_eval_neu.average_neu_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in svm_eval_neu.average_neu_impact])\n",
    "plt.title('y=0.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(svm_eval_pos.token, svm_eval_pos.average_pos_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in svm_eval_pos.average_pos_impact])\n",
    "plt.title('y=1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_eval = pd.read_csv(\"data/explain_results/nb_lime.csv\")\n",
    "\n",
    "nb_eval[\"mag_neg\"] = np.abs(nb_eval.average_neg_impact)\n",
    "nb_eval[\"mag_neu\"] = np.abs(nb_eval.average_neu_impact)\n",
    "nb_eval[\"mag_pos\"] = np.abs(nb_eval.average_pos_impact)\n",
    "\n",
    "nb_eval_neg = nb_eval.nlargest(20, \"mag_neg\")\n",
    "nb_eval_neu = nb_eval.nlargest(20, \"mag_neu\")\n",
    "nb_eval_pos = nb_eval.nlargest(20, \"mag_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(nb_eval_neg.token, nb_eval_neg.average_neg_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in nb_eval_neg.average_neg_impact])\n",
    "plt.title('y=-1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(nb_eval_neu.token, nb_eval_neu.average_neu_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in nb_eval_neu.average_neu_impact])\n",
    "plt.title('y=0.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(nb_eval_pos.token, nb_eval_pos.average_pos_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in nb_eval_pos.average_pos_impact])\n",
    "plt.title('y=1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
