{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "neutral-debut",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-president",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# model training\n",
    "import fasttext\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# for display\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-falls",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path and filenames\n",
    "path = 'data/old_labels/'\n",
    "train = 'train'\n",
    "val = 'val'\n",
    "test = 'test'\n",
    "suffix = '_oldpreproc.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data dict\n",
    "data = {}\n",
    "data_names = [train, val, test]\n",
    "# change this accordingly: 'phrase', 'phrase_lemma', 'phrase_stem'\n",
    "text_column = 'phrase_stem'\n",
    "old_new = 'old_preproc'\n",
    "\n",
    "for name in data_names:\n",
    "    # read data\n",
    "    df = pd.read_csv(path+name+suffix)\n",
    "    # add to data dict\n",
    "    data[f'{name}_{text_column}'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-dimension",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for df_name, df in data.items():\n",
    "    print(df_name)\n",
    "    print(df.shape)\n",
    "    print(df.label.value_counts())\n",
    "    display(df.head(3))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-definition",
   "metadata": {},
   "source": [
    "## Baseline: Yelp Polarity Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained yelp model\n",
    "model_yelp = fasttext.load_model(\"utils/fasttext/yelp_review_polarity.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-warren",
   "metadata": {},
   "source": [
    "### Predict on Validation\n",
    "* To find the best threshold to classify phrases into 3 categories: -1, 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on val set\n",
    "val_pred_df = data['val_'+text_column].copy()\n",
    "val_pred_df['raw_output'] = val_pred_df.apply(lambda x: model_yelp.predict(x[text_column].replace(\"\\n\", \"\")), axis=1)\n",
    "val_pred_df['raw_pred'] = val_pred_df.apply(lambda x: int(x.raw_output[0][0][-1]), axis=1)\n",
    "val_pred_df['raw_prob'] = val_pred_df.apply(lambda x: x.raw_output[1][0], axis=1)\n",
    "val_pred_df['prob_pos'] = val_pred_df.apply(lambda x: x.raw_prob if x.raw_pred == 2 else (1-x.raw_prob), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that assigns class labels based on prob (positive sentiment)\n",
    "def label_from_prob(x, lower, upper):    \n",
    "    if x < lower: # negative sentiment\n",
    "        return -1\n",
    "    elif x < upper: # neutral sentiment\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for best threshold\n",
    "lower_lst = [round(i*0.05, 2) for i in range(1, 9)]\n",
    "upper_lst = [round(1 - i, 2) for i in lower_lst]\n",
    "\n",
    "# dataframe to store results\n",
    "val_threshold_results = pd.DataFrame(columns = ['lower_thresh', 'upper_thresh', 'accuracy', \\\n",
    "                                                'weighted_ave_f1', 'f1_neg', 'f1_zero', 'f1_pos'])\n",
    "\n",
    "for i in range(len(lower_lst)):\n",
    "    df = val_pred_df.copy()\n",
    "    # assign labels based on threshold definition\n",
    "    lower = lower_lst[i]\n",
    "    upper = upper_lst[i]\n",
    "    df['pred'] = df.apply(lambda x: label_from_prob(x.prob_pos, lower, upper), axis=1)\n",
    "    \n",
    "    # classification report\n",
    "    report = classification_report(df.label, df.pred, output_dict=True)\n",
    "    # retrieve metrics\n",
    "    accuracy = report['accuracy']\n",
    "    weighted_ave_f1 = report['weighted avg']['f1-score']\n",
    "    f1_neg = report['-1']['f1-score']\n",
    "    f1_zero = report['0']['f1-score']\n",
    "    f1_pos = report['1']['f1-score']\n",
    "    \n",
    "    row = {'lower_thresh': lower, 'upper_thresh': upper, 'accuracy': accuracy, \\\n",
    "           'weighted_ave_f1': weighted_ave_f1, 'f1_neg': f1_neg, 'f1_zero': f1_zero, 'f1_pos': f1_pos}\n",
    "    \n",
    "    val_threshold_results = val_threshold_results.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_threshold_results.to_csv(f'model_results/fasttext/baseline/{old_new}/val_threshold_{text_column}.csv')\n",
    "val_threshold_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-driving",
   "metadata": {},
   "source": [
    "### Apply Best Threshold on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_row = val_threshold_results.loc[val_threshold_results['weighted_ave_f1'] == max(val_threshold_results['weighted_ave_f1'])]\n",
    "best_lower = best_row['lower_thresh'][0]\n",
    "best_upper = best_row['upper_thresh'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test\n",
    "test_pred_df = data['test_'+text_column].copy()\n",
    "test_pred_df['raw_output'] = test_pred_df.apply(lambda x: model_yelp.predict(x[text_column].replace(\"\\n\", \"\")), axis=1)\n",
    "test_pred_df['raw_pred'] = test_pred_df.apply(lambda x: int(x.raw_output[0][0][-1]), axis=1)\n",
    "test_pred_df['raw_prob'] = test_pred_df.apply(lambda x: x.raw_output[1][0], axis=1)\n",
    "test_pred_df['prob_pos'] = test_pred_df.apply(lambda x: x.raw_prob if x.raw_pred == 2 else (1-x.raw_prob), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_df['pred'] = test_pred_df.apply(lambda x: label_from_prob(x['prob_pos'], best_lower, best_upper), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = classification_report(test_pred_df.label, test_pred_df.pred, output_dict=True)\n",
    "# save to txt\n",
    "f = open(f'model_results/fasttext/baseline/{old_new}/test_{text_column}.txt', \"w\")\n",
    "f.write( str(test_results) )\n",
    "f.close()\n",
    "\n",
    "print(classification_report(test_pred_df.label, test_pred_df.pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_df.pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-paradise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-receipt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-darkness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
