{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/labelled_data/train_old.csv\")\n",
    "val = pd.read_csv(\"../data/labelled_data/val_old.csv\")\n",
    "test = pd.read_csv(\"../data/labelled_data/test_old.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decode emojis from text\n",
    "# train = utils.one_hot_encode_emojis(train, \"phrase\")\n",
    "# val = utils.one_hot_encode_emojis(val, \"phrase\")\n",
    "# test = utils.one_hot_encode_emojis(test, \"phrase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean phrases\n",
    "train[\"phrase_lemma\"] = train.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "val[\"phrase_lemma\"] = val.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "test[\"phrase_lemma\"] = test.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=True, stem=False))\n",
    "\n",
    "train[\"phrase_stem\"] = train.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "val[\"phrase_stem\"] = val.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "test[\"phrase_stem\"] = test.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=True))\n",
    "\n",
    "train.phrase = train.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))\n",
    "val.phrase = val.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))\n",
    "test.phrase = test.phrase.apply(lambda x: utils.clean_phrase(x, lemmatize=False, stem=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows where there are no characters\n",
    "train = train.loc[(train.phrase.str.len() > 0)]\n",
    "val = val.loc[(val.phrase.str.len() > 0)]\n",
    "test = test.loc[(test.phrase.str.len() > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if nan label, replace with 0\n",
    "train.label = train.label.apply(lambda x: 0 if np.isnan(x) else x)\n",
    "val.label = val.label.apply(lambda x: 0 if np.isnan(x) else x)\n",
    "test.label = test.label.apply(lambda x: 0 if np.isnan(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form trainval set\n",
    "trainval = pd.concat([train, val])\n",
    "\n",
    "# filter for only relevant aspects\n",
    "train1 = train.loc[~(train.new_aspect_1.isnull())]\n",
    "train2 = train.loc[~(train.new_aspect_2.isnull())]\n",
    "\n",
    "val1 = val.loc[~(val.new_aspect_1.isnull())]\n",
    "val2 = val.loc[~(val.new_aspect_2.isnull())]\n",
    "\n",
    "trainval1 = trainval.loc[~(trainval.new_aspect_1.isnull())]\n",
    "trainval2 = trainval.loc[~(trainval.new_aspect_2.isnull())]\n",
    "\n",
    "test1 = test.loc[~(test.new_aspect_1.isnull())]\n",
    "test2 = test.loc[~(test.new_aspect_2.isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_params = {\n",
    "    \"analyzer\": [\"word\"],\n",
    "    \"lowercase\": [True],\n",
    "    \"ngram_range\": [(1,1), (1,2), (1,3)]\n",
    "}\n",
    "tfidf_paramgrid = list(ParameterGrid(tfidf_params))\n",
    "\n",
    "phrase_params = {\n",
    "    \"type\": [\"original\", \"lemma\", \"stem\"], # normal, lemma, stem\n",
    "    \"aspect\": [\"original\", \"new_aspect_1\", \"new_aspect_2\"]\n",
    "}\n",
    "phrase_paramgrid = list(ParameterGrid(phrase_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "logreg_params = {\n",
    "    \"C\": [0.5, 1.0, 1.5],\n",
    "    \"solver\": [\"lbfgs\", \"newton-cg\"],\n",
    "    \"penalty\": [\"l2\", \"none\"],\n",
    "    \"class_weight\": [\"balanced\", None] \n",
    "}\n",
    "logreg_paramgrid = list(ParameterGrid(logreg_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "nb_params = {\n",
    "    \"alpha\": [0, 1]\n",
    "}\n",
    "nb_paramgrid = list(ParameterGrid(nb_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "svm_params = {\n",
    "    \"C\": [0.5, 1.0, 1.5],\n",
    "    \"kernel\": [\"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"],\n",
    "    \"class_weight\": [\"balanced\", None] \n",
    "}\n",
    "\n",
    "svm_paramgrid = list(ParameterGrid(svm_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = [\n",
    "    {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"class_weight\": [\"balanced\", \"balanced_subsample\", None]\n",
    "    }\n",
    "]\n",
    "rf_paramgrid = list(ParameterGrid(rf_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_params = { \"strategy\": [\"prior\"] }\n",
    "dummy_paramgrid = list(ParameterGrid(dummy_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(tfidf_param):\n",
    "    # original\n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train = tfidf.fit_transform(train.phrase)\n",
    "    tfidf_val = tfidf.transform(val.phrase)\n",
    "    tfidf_test = tfidf.transform(test.phrase)\n",
    "    tfidf_trainval = tfidf.transform(trainval.phrase)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train1 = tfidf.fit_transform(train1.phrase)\n",
    "    tfidf_val1 = tfidf.transform(val1.phrase)\n",
    "    tfidf_test1 = tfidf.transform(test1.phrase)\n",
    "    tfidf_trainval1 = tfidf.transform(trainval1.phrase)\n",
    "    \n",
    "    tfidf_train2 = tfidf.fit_transform(train2.phrase)\n",
    "    tfidf_val2 = tfidf.transform(val2.phrase)\n",
    "    tfidf_test2 = tfidf.transform(test2.phrase)\n",
    "    tfidf_trainval2 = tfidf.transform(trainval2.phrase)\n",
    "    \n",
    "    # lemmatize\n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train_lemma = tfidf.fit_transform(train.phrase_lemma)\n",
    "    tfidf_val_lemma = tfidf.transform(val.phrase_lemma)\n",
    "    tfidf_test_lemma = tfidf.transform(test.phrase_lemma)\n",
    "    tfidf_trainval_lemma = tfidf.transform(trainval.phrase_lemma)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train1_lemma = tfidf.fit_transform(train1.phrase_lemma)\n",
    "    tfidf_val1_lemma = tfidf.transform(val1.phrase_lemma)\n",
    "    tfidf_test1_lemma = tfidf.transform(test1.phrase_lemma)\n",
    "    tfidf_trainval1_lemma = tfidf.transform(trainval1.phrase_lemma)\n",
    "    \n",
    "    tfidf_train2_lemma = tfidf.fit_transform(train2.phrase_lemma)\n",
    "    tfidf_val2_lemma = tfidf.transform(val2.phrase_lemma)\n",
    "    tfidf_test2_lemma = tfidf.transform(test2.phrase_lemma)\n",
    "    tfidf_trainval2_lemma = tfidf.transform(trainval2.phrase_lemma)\n",
    "    \n",
    "    # stem\n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train_stem = tfidf.fit_transform(train.phrase_stem)\n",
    "    tfidf_val_stem = tfidf.transform(val.phrase_stem)\n",
    "    tfidf_test_stem = tfidf.transform(test.phrase_stem)\n",
    "    tfidf_trainval_stem = tfidf.transform(trainval.phrase_stem)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(**tfidf_param)\n",
    "    tfidf_train1_stem = tfidf.fit_transform(train1.phrase_stem)\n",
    "    tfidf_val1_stem = tfidf.transform(val1.phrase_stem)\n",
    "    tfidf_test1_stem = tfidf.transform(test1.phrase_stem)\n",
    "    tfidf_trainval1_stem = tfidf.transform(trainval1.phrase_stem)\n",
    "    \n",
    "    tfidf_train2_stem = tfidf.fit_transform(train2.phrase_stem)\n",
    "    tfidf_val2_stem = tfidf.transform(val2.phrase_stem)\n",
    "    tfidf_test2_stem = tfidf.transform(test2.phrase_stem)\n",
    "    tfidf_trainval2_stem = tfidf.transform(trainval2.phrase_stem)\n",
    "    \n",
    "    return {\n",
    "        \"original\": {\n",
    "            \"original\": [tfidf_train, tfidf_val, tfidf_test, tfidf_trainval],\n",
    "            \"new_aspect_1\": [tfidf_train1, tfidf_val1, tfidf_test1, tfidf_trainval1],\n",
    "            \"new_aspect_2\": [tfidf_train2, tfidf_val2, tfidf_test2, tfidf_trainval2]\n",
    "        },\n",
    "        \"lemma\": {\n",
    "            \"original\": [tfidf_train_lemma, tfidf_val_lemma, tfidf_test_lemma, tfidf_trainval_lemma],\n",
    "            \"new_aspect_1\": [tfidf_train1_lemma, tfidf_val1_lemma, tfidf_test1_lemma, tfidf_trainval1_lemma],\n",
    "            \"new_aspect_2\": [tfidf_train2_lemma, tfidf_val2_lemma, tfidf_test2_lemma, tfidf_trainval2_lemma]\n",
    "        },\n",
    "        \"stem\": {\n",
    "            \"original\": [tfidf_train_stem, tfidf_val_stem, tfidf_test_stem, tfidf_trainval_stem],\n",
    "            \"new_aspect_1\": [tfidf_train1_stem, tfidf_val1_stem, tfidf_test1_stem, tfidf_trainval1_stem],\n",
    "            \"new_aspect_2\": [tfidf_train2_stem, tfidf_val2_stem, tfidf_test2_stem, tfidf_trainval2_stem]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"original\": [train.label, val.label, test.label, trainval.label],\n",
    "    \"new_aspect_1\": [train1.label, val1.label, test1.label, trainval1.label],\n",
    "    \"new_aspect_2\": [train2.label, val2.label, test2.label, trainval2.label],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"logreg\"\n",
    "model_fn = LogisticRegression\n",
    "model_paramgrid = logreg_paramgrid\n",
    "\n",
    "ind = 0 \n",
    "gridsearch_results = []\n",
    "for tfidf_param in tfidf_paramgrid:\n",
    "    datasets = prepare_datasets(tfidf_param)\n",
    "    \n",
    "    for phrase_param in phrase_paramgrid:\n",
    "        data_type = phrase_param[\"type\"]\n",
    "        data_aspect = phrase_param[\"aspect\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_aspect][0]\n",
    "        val_set = datasets[data_type][data_aspect][1]\n",
    "        test_set = datasets[data_type][data_aspect][2]\n",
    "        trainval_set = datasets[data_type][data_aspect][3]\n",
    "        \n",
    "        train_label = labels[data_aspect][0]\n",
    "        val_label = labels[data_aspect][1]\n",
    "        test_label = labels[data_aspect][2]\n",
    "        trainval_label = labels[data_aspect][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # predict\n",
    "            val_f1 = f1_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_recall = recall_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_precision = precision_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_accuracy = accuracy_score(val_label, val_pred)\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # predict\n",
    "            test_f1 = f1_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_recall = recall_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_precision = precision_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_accuracy = accuracy_score(test_label, test_pred)\n",
    "            \n",
    "            results = { \"model\": model_name }\n",
    "            results.update(tfidf_param)\n",
    "            results.update(phrase_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1\": val_f1, \"val_recall\": val_recall, \"val_precision\": val_precision, \n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1\": test_f1, \"test_recall\": test_recall, \"test_precision\": test_precision, \n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            ind += 1\n",
    "            \n",
    "final_logreg_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_logreg_results = final_logreg_results.sort_values(by=[\"val_f1\", \"test_f1\"], ascending=False)\n",
    "final_logreg_results.to_csv(\"model_results/tfidf/logreg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nb\"\n",
    "model_fn = MultinomialNB\n",
    "model_paramgrid = nb_paramgrid\n",
    "\n",
    "gridsearch_results = []\n",
    "for tfidf_param in tfidf_paramgrid:\n",
    "    datasets = prepare_datasets(tfidf_param)\n",
    "    \n",
    "    for phrase_param in phrase_paramgrid:\n",
    "        data_type = phrase_param[\"type\"]\n",
    "        data_aspect = phrase_param[\"aspect\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_aspect][0]\n",
    "        val_set = datasets[data_type][data_aspect][1]\n",
    "        test_set = datasets[data_type][data_aspect][2]\n",
    "        trainval_set = datasets[data_type][data_aspect][3]\n",
    "        \n",
    "        train_label = labels[data_aspect][0]\n",
    "        val_label = labels[data_aspect][1]\n",
    "        test_label = labels[data_aspect][2]\n",
    "        trainval_label = labels[data_aspect][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # predict\n",
    "            val_f1 = f1_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_recall = recall_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_precision = precision_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_accuracy = accuracy_score(val_label, val_pred)\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # predict\n",
    "            test_f1 = f1_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_recall = recall_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_precision = precision_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_accuracy = accuracy_score(test_label, test_pred)\n",
    "            \n",
    "            results = { \"model\": model_name }\n",
    "            results.update(tfidf_param)\n",
    "            results.update(phrase_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1\": val_f1, \"val_recall\": val_recall, \"val_precision\": val_precision, \n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1\": test_f1, \"test_recall\": test_recall, \"test_precision\": test_precision, \n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            \n",
    "final_nb_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_nb_results = final_nb_results.sort_values(by=[\"val_f1\", \"test_f1\"], ascending=False)\n",
    "final_nb_results.to_csv(\"model_results/tfidf/nb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"rf\"\n",
    "model_fn = RandomForestClassifier\n",
    "model_paramgrid = rf_paramgrid\n",
    "\n",
    "gridsearch_results = []\n",
    "for tfidf_param in tfidf_paramgrid:\n",
    "    datasets = prepare_datasets(tfidf_param)\n",
    "    \n",
    "    for phrase_param in phrase_paramgrid:\n",
    "        data_type = phrase_param[\"type\"]\n",
    "        data_aspect = phrase_param[\"aspect\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_aspect][0]\n",
    "        val_set = datasets[data_type][data_aspect][1]\n",
    "        test_set = datasets[data_type][data_aspect][2]\n",
    "        trainval_set = datasets[data_type][data_aspect][3]\n",
    "        \n",
    "        train_label = labels[data_aspect][0]\n",
    "        val_label = labels[data_aspect][1]\n",
    "        test_label = labels[data_aspect][2]\n",
    "        trainval_label = labels[data_aspect][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # predict\n",
    "            val_f1 = f1_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_recall = recall_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_precision = precision_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_accuracy = accuracy_score(val_label, val_pred)\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # predict\n",
    "            test_f1 = f1_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_recall = recall_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_precision = precision_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_accuracy = accuracy_score(test_label, test_pred)\n",
    "            \n",
    "            results = { \"model\": model_name }\n",
    "            results.update(tfidf_param)\n",
    "            results.update(phrase_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1\": val_f1, \"val_recall\": val_recall, \"val_precision\": val_precision, \n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1\": test_f1, \"test_recall\": test_recall, \"test_precision\": test_precision, \n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            \n",
    "final_rf_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_rf_results = final_rf_results.sort_values(by=[\"val_f1\", \"test_f1\"], ascending=False)\n",
    "final_rf_results.to_csv(\"model_results/tfidf/rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"svm\"\n",
    "model_fn = SVC\n",
    "model_paramgrid = svm_paramgrid\n",
    "\n",
    "gridsearch_results = []\n",
    "for tfidf_param in tfidf_paramgrid:\n",
    "    datasets = prepare_datasets(tfidf_param)\n",
    "    \n",
    "    for phrase_param in phrase_paramgrid:\n",
    "        data_type = phrase_param[\"type\"]\n",
    "        data_aspect = phrase_param[\"aspect\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_aspect][0]\n",
    "        val_set = datasets[data_type][data_aspect][1]\n",
    "        test_set = datasets[data_type][data_aspect][2]\n",
    "        trainval_set = datasets[data_type][data_aspect][3]\n",
    "        \n",
    "        train_label = labels[data_aspect][0]\n",
    "        val_label = labels[data_aspect][1]\n",
    "        test_label = labels[data_aspect][2]\n",
    "        trainval_label = labels[data_aspect][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # predict\n",
    "            val_f1 = f1_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_recall = recall_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_precision = precision_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_accuracy = accuracy_score(val_label, val_pred)\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # predict\n",
    "            test_f1 = f1_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_recall = recall_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_precision = precision_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_accuracy = accuracy_score(test_label, test_pred)\n",
    "            \n",
    "            results = { \"model\": model_name }\n",
    "            results.update(tfidf_param)\n",
    "            results.update(phrase_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1\": val_f1, \"val_recall\": val_recall, \"val_precision\": val_precision, \n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1\": test_f1, \"test_recall\": test_recall, \"test_precision\": test_precision, \n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            \n",
    "final_svm_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_svm_results = final_svm_results.sort_values(by=[\"val_f1\", \"test_f1\"], ascending=False)\n",
    "final_svm_results.to_csv(\"model_results/tfidf/svm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dummy\"\n",
    "model_fn = DummyClassifier\n",
    "model_paramgrid = dummy_paramgrid\n",
    "\n",
    "gridsearch_results = []\n",
    "for tfidf_param in tfidf_paramgrid:\n",
    "    datasets = prepare_datasets(tfidf_param)\n",
    "    \n",
    "    for phrase_param in phrase_paramgrid:\n",
    "        data_type = phrase_param[\"type\"]\n",
    "        data_aspect = phrase_param[\"aspect\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_aspect][0]\n",
    "        val_set = datasets[data_type][data_aspect][1]\n",
    "        test_set = datasets[data_type][data_aspect][2]\n",
    "        trainval_set = datasets[data_type][data_aspect][3]\n",
    "        \n",
    "        train_label = labels[data_aspect][0]\n",
    "        val_label = labels[data_aspect][1]\n",
    "        test_label = labels[data_aspect][2]\n",
    "        trainval_label = labels[data_aspect][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # predict\n",
    "            val_f1 = f1_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_recall = recall_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_precision = precision_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_accuracy = accuracy_score(val_label, val_pred)\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # predict\n",
    "            test_f1 = f1_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_recall = recall_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_precision = precision_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_accuracy = accuracy_score(test_label, test_pred)\n",
    "            \n",
    "            results = { \"model\": model_name }\n",
    "            results.update(tfidf_param)\n",
    "            results.update(phrase_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1\": val_f1, \"val_recall\": val_recall, \"val_precision\": val_precision, \n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1\": test_f1, \"test_recall\": test_recall, \"test_precision\": test_precision, \n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            \n",
    "final_dummy_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_dummy_results = final_dummy_results.sort_values(by=[\"val_f1\", \"test_f1\"], ascending=False)\n",
    "final_dummy_results.to_csv(\"model_results/tfidf/dummy.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([final_logreg_results, final_nb_results, final_svm_results, final_rf_results, \\\n",
    "                        final_dummy_results])\n",
    "combined_df = combined_df.sort_values(by=[\"val_f1\", \"test_f1\"], ascending=False)\n",
    "combined_df.to_csv(\"model_results/tfidf/combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
