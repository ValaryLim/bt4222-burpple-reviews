{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/labelled_data/\"\n",
    "test = pd.read_csv(path + \"test_subset.csv\")\n",
    "train_1 = pd.read_csv(path + \"train_first_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_2 = pd.read_csv(path + \"train_second_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_3 = pd.read_csv(path + \"train_third_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_1[\"label\"] = pd.to_numeric(train_1[\"label\"], errors='coerce')\n",
    "train = pd.concat([train_1,train_2,train_3],axis=0)\n",
    "val = pd.read_csv(path + \"val_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "\n",
    "test = test.reset_index()\n",
    "train=train.reset_index()\n",
    "val = val.reset_index()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data in [train_1,train_2,train_3,train,val,test]:\n",
    "    print(\"Total size = {}\".format(data.shape[0]))\n",
    "    print(\"Noise = {} \".format(data.loc[(data.label == 0.0) & (data.new_aspect_1.isnull()) & (data.new_aspect_2.isnull())].shape[0]))\n",
    "    print(\"Aspect with no adjectives = {} \".format(data.loc[(data.new_aspect_1.notnull()) & (data.new_aspect_2.isnull())].shape[0]))\n",
    "    print(\"Aspect with adjectives = {} \".format(data.loc[(data.new_aspect_1.notnull()) & (data.new_aspect_2.notnull())].shape[0]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing\n",
    "\n",
    "2 data sets for modelling\n",
    "\n",
    "1. Use this original data without noise\n",
    "2. Create new dataset --> Filter the phrase column (punctuations/hashtags) then combine the aspects into 1 & aggregate the sentiment scores (label)\n",
    "\n",
    "Question : when aggregating use majority? but what if i had alot of noise eg. 0 0 0 0 0 and only 1 -1 then using majority voting will only get 0 not -1? And also mean doesnt work either"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "noise = train.loc[(train.label == 0.0) & (train.new_aspect_1.isnull()) & (train.new_aspect_2.isnull())].index\n",
    "train1 = train.loc[~train.index.isin(noise)]\n",
    "print(train1.shape)\n",
    "\n",
    "# test\n",
    "noise = val.loc[(val.label == 0.0) & (val.new_aspect_1.isnull()) & (val.new_aspect_2.isnull())].index\n",
    "val1 = val.loc[~val.index.isin(noise)]\n",
    "print(val1.shape)\n",
    "\n",
    "#val \n",
    "noise = test.loc[(test.label == 0.0) & (test.new_aspect_1.isnull()) & (test.new_aspect_2.isnull())].index\n",
    "test1 = test.loc[~test.index.isin(noise)]\n",
    "print(test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train1.label.value_counts())\n",
    "print(test1.label.value_counts())\n",
    "print(val1.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 2 **USE THIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgn(x):\n",
    "    if x < 0:\n",
    "        return -1\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out phrases with hashtags & those with character length <=5 then get the majority (might not make sense)\n",
    "# train\n",
    "train2_filtered = train.loc[~train.index.isin(train.loc[(train.phrase.str.contains(\"#\")) | (train.phrase.str.len() <= 5)].index)]\n",
    "print(\"Number of rows after filtering (Train) = {}\".format(train2_filtered.shape[0]))\n",
    "train2 = train2_filtered.groupby([\"review_title\",\"account_name\",\"aspect\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                                \"label\": \"mean\"})\n",
    "train2 = train2.reset_index()\n",
    "train2.label = train2.label.map(sgn)\n",
    "\n",
    "\n",
    "# test\n",
    "test2_filtered = test.loc[~test.index.isin(test.loc[(test.phrase.str.contains(\"#\")) | (test.phrase.str.len() <= 5)].index)]\n",
    "print(\"Number of rows after filtering (Test) = {}\".format(test2_filtered.shape[0]))\n",
    "test2 = test2_filtered.groupby([\"review_title\",\"account_name\",\"aspect\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                              \"label\": \"mean\"})\n",
    "test2 = test2.reset_index()\n",
    "test2.label = test2.label.map(sgn)\n",
    "\n",
    "#val \n",
    "val2_filtered = val.loc[~val.index.isin(val.loc[(val.phrase.str.contains(\"#\")) | (val.phrase.str.len() <= 5)].index)]\n",
    "print(\"Number of rows after filtering (Val) = {}\".format(val2_filtered.shape[0]))\n",
    "val2 = val2_filtered.groupby([\"review_title\",\"account_name\",\"aspect\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                              \"label\": \"mean\"})\n",
    "val2 = val2.reset_index()\n",
    "val2.label = val2.label.map(sgn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use train & val as training\n",
    "train_val2 = pd.concat([train2,val2],axis=0)\n",
    "train_val2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_val2.label.value_counts())\n",
    "print(test2.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Text Vectorisation --> used on data2\n",
    "1. BoW\n",
    "2. Tf-idf\n",
    "3. Fasttext - can handle oov words https://www.quora.com/How-does-fastText-output-a-vector-for-a-word-that-is-not-in-the-pre-trained-model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function for gensim fastText word embeddings in python\n",
    "# remove numbers\n",
    "def process_text(document):     \n",
    "        # Remove extra white space from text\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)     \n",
    "        # Remove all the special characters from text\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "        # Remove all single characters from text\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "        # Remove numbers\n",
    "        document = re.sub(\"(\\s\\d+)\",\"\",document) \n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "        # Word tokenization       \n",
    "        tokens = document.split()\n",
    "        # Lemmatization using NLTK\n",
    "        lemma_txt = [stemmer.lemmatize(word) for word in tokens]\n",
    "        # Remove stop words\n",
    "        lemma_no_stop_txt = [word for word in lemma_txt if word not in stopwords]\n",
    "        # Drop words \n",
    "        tokens = [word for word in tokens if len(word) > 3]            \n",
    "        clean_txt = ' '.join(lemma_no_stop_txt)\n",
    "        return clean_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 3),preprocessor=process_text)\n",
    "bow_train = bow.fit_transform(train_val2.phrase)\n",
    "bow_test = bow.transform(test2.phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 3),preprocessor=process_text)\n",
    "tfidf_train = tfidf.fit_transform(train_val2.phrase)\n",
    "tfidf_test = tfidf.transform(test2.phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Preprocess data for fasttext word embeddings training\n",
    "train_clean = [process_text(sentence) for sentence in tqdm(train_val2.phrase) if sentence.strip() !='']\n",
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "train_word_tokens = [word_tokenizer.tokenize(sent) for sent in tqdm(train_clean)]\n",
    "\n",
    "# Preprocess data for fasttext word embeddings testing data\n",
    "test_clean = [process_text(sentence) for sentence in tqdm(test2.phrase) if sentence.strip() !='']\n",
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "test_word_tokens = [word_tokenizer.tokenize(sent) for sent in tqdm(test_clean)]\n",
    "\n",
    "# Train Fasttext word embeddings on training data\n",
    "# Defining values for parameters\n",
    "embedding_size = 300\n",
    "window_size = 5\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n",
    " \n",
    "\n",
    "fast_Text_model = FastText(train_word_tokens,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      workers = 4,\n",
    "                      sg=1,\n",
    "                      iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fastText gensim model\n",
    "fast_Text_model.save(\"ftmodel/ft_model_train\")\n",
    "# Load saved gensim fastText model\n",
    "fast_Text_model = Word2Vec.load(\"ftmodel/ft_model_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentence --> average(each word embedding)\n",
    "fasttext_train = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in train_word_tokens]\n",
    "# convert sentence --> average(each word embedding)\n",
    "fasttext_test = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in test_word_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_train_processed = []\n",
    "for row in fasttext_train:\n",
    "    try:\n",
    "        fasttext_train_processed.append(list(row))\n",
    "    except:\n",
    "        fasttext_train_processed.append([0]*300)\n",
    "\n",
    "fasttext_test_processed = []\n",
    "for row in fasttext_test:\n",
    "    try:\n",
    "        fasttext_test_processed.append(list(row))\n",
    "    except:\n",
    "        fasttext_test_processed.append([0]*300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_train_processed = np.array(fasttext_train_processed).reshape(-1,300)\n",
    "fasttext_test_processed = np.array(fasttext_test_processed).reshape(-1,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelling\n",
    "\n",
    "1. Naive Bayes/Support Vector Machines (NBSVM)/Logistic Regression\n",
    "2. Fasttext\n",
    "3. BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Baseline) 1. Naive Bayes -Support Vector Machines (NBSVM)- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "svm = SVC()\n",
    "\n",
    "all_train = [bow_train,tfidf_train,fasttext_train_processed]\n",
    "all_test = [bow_test,tfidf_test,fasttext_test_processed]\n",
    "all_vectorisers = [\"BoW\",\"TfIdf\",\"FastText\"]\n",
    "\n",
    "for train,test,vectoriser in zip(all_train,all_test,all_vectorisers):\n",
    "    logreg.fit(train,train_val2.label)\n",
    "    \n",
    "    print(\"Logistic Regression + \" + vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(train_val2.label, logreg.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(test2.label, logreg.predict(test))) \n",
    "    \n",
    "    svm.fit(train,train_val2.label)\n",
    "    \n",
    "    print(\"SVM + \" + vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(train_val2.label, svm.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(test2.label, svm.predict(test))) \n",
    "\n",
    "    # Fasttest dont work w NaiveBayes\n",
    "    nb.fit(train,train_val2.label)\n",
    "    print(\"Naive Bayes + \"+ vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(train_val2.label, nb.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(test2.label, nb.predict(test)))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
