{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample data\n",
    "all_train = pd.read_csv('data/stacking_folds/train_all.csv', header = 0)[[\"phrase_stem_emoticon_generic\", \"phrase_stem_emoticon_unique\",\"phrase\", \"label\"]]\n",
    "all_test = pd.read_csv('data/stacking_folds/test.csv', header = 0)[[\"phrase_stem_emoticon_generic\",\"phrase_stem_emoticon_unique\", \"phrase\", \"label\"]]\n",
    "all_sample = pd.concat([all_train, all_test], axis=0).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_dict_of_scores(d, lime_exp, label):\n",
    "    l = lime_exp.as_list(label=label)\n",
    "    for item in l:\n",
    "        key = item[0]\n",
    "        val = item[1]\n",
    "        if key in d:\n",
    "            d[key].append(val)\n",
    "        else:\n",
    "            d[key] = [val]\n",
    "\n",
    "\n",
    "def generate_lime_scores(pipe, phrase_ver):\n",
    "    d_neg = defaultdict()\n",
    "    d_neu = defaultdict()\n",
    "    d_pos = defaultdict()\n",
    "    class_names = [-1, 0, 1]\n",
    "    explainer = LimeTextExplainer(class_names = class_names, random_state=42)\n",
    "    for i in tqdm(range(len(all_sample))):\n",
    "        current_text = all_sample[phrase_ver].iloc[i]\n",
    "        exp = explainer.explain_instance(current_text, pipe.predict_proba, labels=[-1,0,1]) #1,-1\n",
    "        append_dict_of_scores(d_neg, exp, -1)\n",
    "        append_dict_of_scores(d_neu, exp, 0)\n",
    "        append_dict_of_scores(d_pos, exp, 1)\n",
    "    return d_neg, d_neu, d_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_df(d, newcols):\n",
    "    token_df =  pd.DataFrame([d]).T\n",
    "    token_df = token_df.reset_index()\n",
    "    token_df.columns = newcols\n",
    "    return token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_avg_score(dpos, dneu, dneg):\n",
    "    avgDict_pos = {}\n",
    "    avgDict_neu = {}\n",
    "    avgDict_neg = {}\n",
    "    for k,v in dpos.items():\n",
    "        # v is the list of impact on probability of predicting a class for a particular token\n",
    "        avgDict_pos[k] = sum(v)/ float(len(v))\n",
    "    for k,v in dneg.items():\n",
    "        # v is the list of impact on probability of predicting a class for a particular token\n",
    "        avgDict_neg[k] = sum(v)/ float(len(v))\n",
    "    for k,v in dneu.items():\n",
    "        # v is the list of impact on probability of predicting a class for a particular token\n",
    "        avgDict_neu[k] = sum(v)/ float(len(v))\n",
    "    pos=dict_to_df(avgDict_pos, ['token', 'average_pos_impact'])\n",
    "    neg=dict_to_df(avgDict_neg, ['token', 'average_neg_impact'])\n",
    "    neu=dict_to_df(avgDict_neu, ['token', 'average_neu_impact'])\n",
    "    fin = pos.merge(neg, on='token', how = 'inner').merge(neu, on='token', how = 'inner')\n",
    "    fin.sort_values(['average_pos_impact'], ascending=False)\n",
    "    return fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "vec = TfidfVectorizer(analyzer=\"word\",\n",
    "    lowercase= True,\n",
    "    ngram_range =(1,2),\n",
    "    max_df = 0.25)\n",
    "lr = LogisticRegression(C=5, class_weight='balanced')\n",
    "pipe_lr = make_pipeline(vec, lr)\n",
    "pipe_lr.fit(all_train.phrase_stem_emoticon_unique, all_train.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dneg, dneu, dpos = generate_lime_scores(pipe_lr, \"phrase_stem_emoticon_unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lime = get_token_avg_score(dpos, dneu, dneg)\n",
    "lr_lime.to_csv('data/explain_results/lr_lime.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVES BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "vec = CountVectorizer(analyzer=\"word\",\n",
    "    lowercase= True,\n",
    "    ngram_range =(1,1),\n",
    "    max_df = 0.25,\n",
    "    min_df = 10)\n",
    "nb = MultinomialNB(alpha = 0.5)\n",
    "pipe_nb = make_pipeline(vec, nb)\n",
    "pipe_nb.fit(all_train.phrase_stem_emoticon_generic, all_train.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dneg, dneu, dpos = generate_lime_scores(pipe_nb, \"phrase_stem_emoticon_generic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_lime = get_token_avg_score(dpos, dneu, dneg)\n",
    "nb_lime.to_csv('data/explain_results/nb_lime.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "vec_rf = CountVectorizer(analyzer=\"word\",\n",
    "    lowercase= True,\n",
    "    ngram_range =(1,1),\n",
    "    max_df = 1.0,\n",
    "    min_df = 1)\n",
    "rf = RandomForestClassifier(criterion = \"gini\", min_samples_split = 5, class_weight=None, max_features=\"auto\", min_samples_leaf=1)\n",
    "pipe_rf = make_pipeline(vec_rf, rf)\n",
    "pipe_rf.fit(all_train.phrase_stem_emoticon_generic, all_train.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dneg, dneu, dpos = generate_lime_scores(pipe_rf, \"phrase_stem_emoticon_generic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_lime = get_token_avg_score(dpos, dneu, dneg)\n",
    "rf_lime.to_csv('data/explain_results/rf_lime.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "vec_svm = CountVectorizer(analyzer=\"word\",\n",
    "    lowercase= True,\n",
    "    ngram_range =(1,1),\n",
    "    max_df = 0.25,\n",
    "    min_df = 1)\n",
    "svm = SVC(C=5, kernel='rbf', probability=True,class_weight=None,gamma='scale')\n",
    "pipe_svm = make_pipeline(vec_svm, svm)\n",
    "pipe_svm.fit(all_train.phrase_stem_emoticon_generic, all_train.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dneg, dneu, dpos = generate_lime_scores(pipe_svm, \"phrase_stem_emoticon_generic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_lime = get_token_avg_score(dpos, dneu, dneg)\n",
    "svm_lime.to_csv('data/explain_results/svm_lime.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
