{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample data\n",
    "all_train = pd.read_csv('data/stacking_folds/train_all.csv', header = 0)[[\"phrase_stem_emoticon_generic\", \"phrase_stem_emoticon_unique\",\"phrase_emoticon_generic\", \"label\"]]\n",
    "all_train.label = all_train.label.astype('int32') # convert target to int\n",
    "all_test = pd.read_csv('data/stacking_folds/test.csv', header = 0)[[\"phrase_stem_emoticon_generic\",\"phrase_stem_emoticon_unique\", \"phrase_emoticon_generic\", \"label\"]]\n",
    "all_test.label = all_test.label.astype('int32') # convert target to int\n",
    "all_sample = pd.concat([all_train, all_test], axis=0).reset_index().drop('index', axis=1)\n",
    "print(all_sample.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_dict_of_scores(d, lime_exp, label):\n",
    "    l = lime_exp.as_list(label=label)\n",
    "    for item in l:\n",
    "        key = item[0]\n",
    "        val = item[1]\n",
    "        if key in d:\n",
    "            d[key].append(val)\n",
    "        else:\n",
    "            d[key] = [val]\n",
    "\n",
    "\n",
    "def generate_lime_scores(pipe, phrase_ver):\n",
    "    d_neg = defaultdict()\n",
    "    d_neu = defaultdict()\n",
    "    d_pos = defaultdict()\n",
    "    class_names = [-1, 0, 1] # ordered according to the classifier\n",
    "    explainer = LimeTextExplainer(class_names = class_names, random_state=42)\n",
    "    for i in tqdm(range(len(all_sample))):\n",
    "        current_text = all_sample[phrase_ver].iloc[i]\n",
    "        # labels – iterable with labels to be explained\n",
    "        # num_samples – size of the neighborhood to learn the linear model\n",
    "        exp = explainer.explain_instance(current_text, pipe.predict_proba, labels=[0, 1, 2], num_samples=100)\n",
    "        append_dict_of_scores(d_neg, exp, 0) # class_names[0] = -1\n",
    "        append_dict_of_scores(d_neu, exp, 1)\n",
    "        append_dict_of_scores(d_pos, exp, 2)\n",
    "    return d_neg, d_neu, d_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_df(d, newcols):\n",
    "    token_df =  pd.DataFrame([d]).T\n",
    "    token_df = token_df.reset_index()\n",
    "    token_df.columns = newcols\n",
    "    return token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_avg_score(dpos, dneu, dneg):\n",
    "    avgDict_pos = {}\n",
    "    avgDict_neu = {}\n",
    "    avgDict_neg = {}\n",
    "    for k,v in dpos.items():\n",
    "        # v is the list of impact on probability of predicting a class for a particular token\n",
    "        avgDict_pos[k] = sum(v)/ float(len(v))\n",
    "    for k,v in dneg.items():\n",
    "        # v is the list of impact on probability of predicting a class for a particular token\n",
    "        avgDict_neg[k] = sum(v)/ float(len(v))\n",
    "    for k,v in dneu.items():\n",
    "        # v is the list of impact on probability of predicting a class for a particular token\n",
    "        avgDict_neu[k] = sum(v)/ float(len(v))\n",
    "    pos=dict_to_df(avgDict_pos, ['token', 'average_pos_impact'])\n",
    "    neg=dict_to_df(avgDict_neg, ['token', 'average_neg_impact'])\n",
    "    neu=dict_to_df(avgDict_neu, ['token', 'average_neu_impact'])\n",
    "    fin = pos.merge(neg, on='token', how = 'inner').merge(neu, on='token', how = 'inner')\n",
    "    fin.sort_values(['average_pos_impact'], ascending=False)\n",
    "    return fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "vec = TfidfVectorizer(analyzer=\"word\",\n",
    "    lowercase= True,\n",
    "    ngram_range =(1,2),\n",
    "    max_df = 0.25)\n",
    "lr = LogisticRegression(C=5, class_weight='balanced')\n",
    "pipe_lr = make_pipeline(vec, lr)\n",
    "pipe_lr.fit(all_train.phrase_stem_emoticon_unique, all_train.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELI5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "all_train = pd.read_csv('data/stacking_folds/train_all.csv', header = 0)\n",
    "all_test = pd.read_csv('data/stacking_folds/test.csv', header = 0)\n",
    "full_df = pd.concat([all_train, all_test], axis=0).reset_index().drop('index', axis=1)\n",
    "full_df_subset = full_df[[\"new_aspect_1\", \"phrase_stem_emoticon_unique\", \"label\"]]\n",
    "\n",
    "# load saved models\n",
    "vect_pkl_filename = \"saved_models/model_logreg_vectorizer.pkl\"\n",
    "model_pkl_filename = \"saved_models/model_logreg.pkl\"\n",
    "lr_vectorizer = pickle.load(open(vect_pkl_filename, \"rb\"))\n",
    "lr_model = pickle.load(open(model_pkl_filename, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_eval = eli5.explain_weights_df(lr_model, vec=lr_vectorizer, top=20)\n",
    "lr_eval_neg = lr_eval[lr_eval.target == -1.0]\n",
    "lr_eval_neu = lr_eval[lr_eval.target == 0.0]\n",
    "lr_eval_pos = lr_eval[lr_eval.target == 1.0]\n",
    "\n",
    "# save results\n",
    "lr_eval.to_csv(\"data/explain_results/logreg_lime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(lr_eval_neg.feature, lr_eval_neg.weight, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in lr_eval_neg.weight])\n",
    "plt.title('y=-1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(lr_eval_neu.feature, lr_eval_neu.weight, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in lr_eval_neu.weight])\n",
    "plt.title('y=0.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(lr_eval_pos.feature, lr_eval_pos.weight, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in lr_eval_pos.weight])\n",
    "plt.title('y=1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# LIME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dneg, dneu, dpos = generate_lime_scores(pipe_lr, \"phrase_stem_emoticon_unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr_eval = get_token_avg_score(dpos, dneu, dneg)\n",
    "lr_eval.to_csv('data/explain_results/lr_lime.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_eval[\"mag_neg\"] = np.abs(lr_eval.average_neg_impact)\n",
    "lr_eval[\"mag_neu\"] = np.abs(lr_eval.average_neu_impact)\n",
    "lr_eval[\"mag_pos\"] = np.abs(lr_eval.average_pos_impact)\n",
    "\n",
    "lr_eval_neg = lr_eval.nlargest(20, \"mag_neg\")\n",
    "lr_eval_neu = lr_eval.nlargest(20, \"mag_neu\")\n",
    "lr_eval_pos = lr_eval.nlargest(20, \"mag_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(lr_eval_neg.token, lr_eval_neg.average_neg_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in lr_eval_neg.average_neg_impact])\n",
    "plt.title('y=-1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(lr_eval_neu.token, lr_eval_neu.average_neu_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in lr_eval_neu.average_neu_impact])\n",
    "plt.title('y=0.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(lr_eval_pos.token, lr_eval_pos.average_pos_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in lr_eval_pos.average_pos_impact])\n",
    "plt.title('y=1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# NAIVES BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "vec = CountVectorizer(analyzer=\"word\",\n",
    "    lowercase= True,\n",
    "    ngram_range =(1,1),\n",
    "    max_df = 0.25,\n",
    "    min_df = 10)\n",
    "nb = MultinomialNB(alpha = 0.5)\n",
    "pipe_nb = make_pipeline(vec, nb)\n",
    "pipe_nb.fit(all_train.phrase_stem_emoticon_generic, all_train.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# LIME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dneg, dneu, dpos = generate_lime_scores(pipe_nb, \"phrase_stem_emoticon_generic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nb_eval = get_token_avg_score(dpos, dneu, dneg)\n",
    "nb_eval.to_csv('data/explain_results/nb_lime.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_eval[\"mag_neg\"] = np.abs(nb_eval.average_neg_impact)\n",
    "nb_eval[\"mag_neu\"] = np.abs(nb_eval.average_neu_impact)\n",
    "nb_eval[\"mag_pos\"] = np.abs(nb_eval.average_pos_impact)\n",
    "\n",
    "nb_eval_neg = nb_eval.nlargest(20, \"mag_neg\")\n",
    "nb_eval_neu = nb_eval.nlargest(20, \"mag_neu\")\n",
    "nb_eval_pos = nb_eval.nlargest(20, \"mag_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(nb_eval_neg.token, nb_eval_neg.average_neg_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in nb_eval_neg.average_neg_impact])\n",
    "plt.title('y=-1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(nb_eval_neu.token, nb_eval_neu.average_neu_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in nb_eval_neu.average_neu_impact])\n",
    "plt.title('y=0.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(nb_eval_pos.token, nb_eval_pos.average_pos_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in nb_eval_pos.average_pos_impact])\n",
    "plt.title('y=1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "vec_rf = CountVectorizer(analyzer=\"word\",\n",
    "    lowercase= True,\n",
    "    ngram_range =(1,1),\n",
    "    max_df = 1.0,\n",
    "    min_df = 1)\n",
    "rf = RandomForestClassifier(criterion = \"gini\", min_samples_split = 5, class_weight=None, max_features=\"auto\", min_samples_leaf=1)\n",
    "pipe_rf = make_pipeline(vec_rf, rf)\n",
    "pipe_rf.fit(all_train.phrase_stem_emoticon_generic, all_train.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dneg, dneu, dpos = generate_lime_scores(pipe_rf, \"phrase_stem_emoticon_generic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_eval = get_token_avg_score(dpos, dneu, dneg)\n",
    "rf_eval.to_csv('data/explain_results/rf_lime.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_eval[\"mag_neg\"] = np.abs(rf_eval.average_neg_impact)\n",
    "rf_eval[\"mag_neu\"] = np.abs(rf_eval.average_neu_impact)\n",
    "rf_eval[\"mag_pos\"] = np.abs(rf_eval.average_pos_impact)\n",
    "\n",
    "rf_eval_neg = rf_eval.nlargest(20, \"mag_neg\")\n",
    "rf_eval_neu = rf_eval.nlargest(20, \"mag_neu\")\n",
    "rf_eval_pos = rf_eval.nlargest(20, \"mag_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(rf_eval_neg.token, rf_eval_neg.average_neg_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in rf_eval_neg.average_neg_impact])\n",
    "plt.title('y=-1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(rf_eval_neu.token, rf_eval_neu.average_neu_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in rf_eval_neu.average_neu_impact])\n",
    "plt.title('y=0.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(rf_eval_pos.token, rf_eval_pos.average_pos_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in rf_eval_pos.average_pos_impact])\n",
    "plt.title('y=1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "vec_svm = CountVectorizer(analyzer=\"word\",\n",
    "    lowercase= True,\n",
    "    ngram_range =(1,1),\n",
    "    max_df = 0.25,\n",
    "    min_df = 1)\n",
    "svm = SVC(C=5, kernel='rbf', probability=True,class_weight=None,gamma='scale')\n",
    "pipe_svm = make_pipeline(vec_svm, svm)\n",
    "pipe_svm.fit(all_train.phrase_stem_emoticon_generic, all_train.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dneg, dneu, dpos = generate_lime_scores(pipe_svm, \"phrase_emoticon_generic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_eval = get_token_avg_score(dpos, dneu, dneg)\n",
    "svm_eval.to_csv('data/explain_results/svm_lime.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_eval[\"mag_neg\"] = np.abs(svm_eval.average_neg_impact)\n",
    "svm_eval[\"mag_neu\"] = np.abs(svm_eval.average_neu_impact)\n",
    "svm_eval[\"mag_pos\"] = np.abs(svm_eval.average_pos_impact)\n",
    "\n",
    "svm_eval_neg = svm_eval.nlargest(20, \"mag_neg\")\n",
    "svm_eval_neu = svm_eval.nlargest(20, \"mag_neu\")\n",
    "svm_eval_pos = svm_eval.nlargest(20, \"mag_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(svm_eval_neg.token, svm_eval_neg.average_neg_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in svm_eval_neg.average_neg_impact])\n",
    "plt.title('y=-1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(svm_eval_neu.token, svm_eval_neu.average_neu_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in svm_eval_neu.average_neu_impact])\n",
    "plt.title('y=0.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(svm_eval_pos.token, svm_eval_pos.average_pos_impact, height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in svm_eval_pos.average_pos_impact])\n",
    "plt.title('y=1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
