{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "2. hashtags/punctuations\n",
    "3. \"not worth\"/\"did not taste bad\" —> extracted \"worth\"/\"bad\" so meaning changed (negspacy)\n",
    "4. emojis —> label if it is a good one?\n",
    "5. quite alot of duplicated rows\n",
    "\n",
    "\n",
    "TODO : \n",
    "1. Extract frequently occuring words with aspects1/2 \n",
    "2. Send preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_code</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_body</th>\n",
       "      <th>account_name</th>\n",
       "      <th>account_id</th>\n",
       "      <th>aspect</th>\n",
       "      <th>phrase</th>\n",
       "      <th>label</th>\n",
       "      <th>new_aspect_1</th>\n",
       "      <th>new_aspect_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>So glad the weekend is here and I can finally ...</td>\n",
       "      <td>Breakfast with the husband this morning at Gen...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>food</td>\n",
       "      <td>gen shu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>food</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>So glad the weekend is here and I can finally ...</td>\n",
       "      <td>Breakfast with the husband this morning at Gen...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>food</td>\n",
       "      <td>and glutinous rice</td>\n",
       "      <td>0.0</td>\n",
       "      <td>food</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>My Saturday morning started with breakfast at ...</td>\n",
       "      <td>It is so inspiring talking to Uncle Kun as he ...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>food</td>\n",
       "      <td>it is</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>My Saturday morning started with breakfast at ...</td>\n",
       "      <td>It is so inspiring talking to Uncle Kun as he ...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>@72128</td>\n",
       "      <td>food</td>\n",
       "      <td>cantonese style congee ,</td>\n",
       "      <td>0.0</td>\n",
       "      <td>food</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>Wonderful breakfast to start my day!</td>\n",
       "      <td>Wonderful consistency in texture too！#根叔美食世家 #...</td>\n",
       "      <td>Benjamin Yeo</td>\n",
       "      <td>@4898</td>\n",
       "      <td>food</td>\n",
       "      <td># 皮蛋粥 # breakfast # porridge</td>\n",
       "      <td>0.0</td>\n",
       "      <td>food</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  restaurant_code                                       review_title  \\\n",
       "0        2lhaZp7B  So glad the weekend is here and I can finally ...   \n",
       "1        2lhaZp7B  So glad the weekend is here and I can finally ...   \n",
       "2        2lhaZp7B  My Saturday morning started with breakfast at ...   \n",
       "3        2lhaZp7B  My Saturday morning started with breakfast at ...   \n",
       "4        2lhaZp7B               Wonderful breakfast to start my day!   \n",
       "\n",
       "                                         review_body  account_name account_id  \\\n",
       "0  Breakfast with the husband this morning at Gen...    Maureen Ow     @72128   \n",
       "1  Breakfast with the husband this morning at Gen...    Maureen Ow     @72128   \n",
       "2  It is so inspiring talking to Uncle Kun as he ...    Maureen Ow     @72128   \n",
       "3  It is so inspiring talking to Uncle Kun as he ...    Maureen Ow     @72128   \n",
       "4  Wonderful consistency in texture too！#根叔美食世家 #...  Benjamin Yeo      @4898   \n",
       "\n",
       "  aspect                         phrase  label new_aspect_1 new_aspect_2  \n",
       "0   food                       gen shu     0.0         food          NaN  \n",
       "1   food            and glutinous rice     0.0         food          NaN  \n",
       "2   food                         it is     0.0          NaN          NaN  \n",
       "3   food      cantonese style congee ,     0.0         food          NaN  \n",
       "4   food  # 皮蛋粥 # breakfast # porridge     0.0         food          NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path = \"/Users/xinminaw/Documents/GitHub/bt4222-burpple-reviews/data/labelled_data/\"\n",
    "path = \"\"\n",
    "test = pd.read_csv(path + \"test_subset.csv\")\n",
    "train_1 = pd.read_csv(path + \"train_first_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_2 = pd.read_csv(path + \"train_second_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_3 = pd.read_csv(path + \"train_third_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "train_1[\"label\"] = pd.to_numeric(train_1[\"label\"], errors='coerce')\n",
    "train = pd.concat([train_1,train_2,train_3],axis=0)\n",
    "val = pd.read_csv(path + \"val_subset.csv\")[['restaurant_code','review_title','review_body','account_name','account_id','aspect','phrase','label','new_aspect_1','new_aspect_2']]\n",
    "\n",
    "test = test.reset_index().drop(\"index\",axis=1)\n",
    "train=train.reset_index().drop(\"index\",axis=1)\n",
    "val = val.reset_index().drop(\"index\",axis=1)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"test_old.csv\")\n",
    "train.to_csv(\"train_old.csv\")\n",
    "val.to_csv(\"val_old.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size = 1362\n",
      "Noise = 257 \n",
      "Aspect with no adjectives = 812 \n",
      "Aspect with adjectives = 285 \n",
      "\n",
      "Total size = 1362\n",
      "Noise = 317 \n",
      "Aspect with no adjectives = 474 \n",
      "Aspect with adjectives = 496 \n",
      "\n",
      "Total size = 1363\n",
      "Noise = 419 \n",
      "Aspect with no adjectives = 474 \n",
      "Aspect with adjectives = 469 \n",
      "\n",
      "Total size = 4087\n",
      "Noise = 993 \n",
      "Aspect with no adjectives = 1760 \n",
      "Aspect with adjectives = 1250 \n",
      "\n",
      "Total size = 1497\n",
      "Noise = 347 \n",
      "Aspect with no adjectives = 579 \n",
      "Aspect with adjectives = 569 \n",
      "\n",
      "Total size = 1472\n",
      "Noise = 462 \n",
      "Aspect with no adjectives = 493 \n",
      "Aspect with adjectives = 509 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data in [train_1,train_2,train_3,train,val,test]:\n",
    "    print(\"Total size = {}\".format(data.shape[0]))\n",
    "    print(\"Noise = {} \".format(data.loc[(data.label == 0.0) & (data.new_aspect_1.isnull()) & (data.new_aspect_2.isnull())].shape[0]))\n",
    "    print(\"Aspect with no adjectives = {} \".format(data.loc[(data.new_aspect_1.notnull()) & (data.new_aspect_2.isnull())].shape[0]))\n",
    "    print(\"Aspect with adjectives = {} \".format(data.loc[(data.new_aspect_1.notnull()) & (data.new_aspect_2.notnull())].shape[0]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords1 = stopwords.words('english')\n",
    "stopwords1.extend([\"ambience\",\"price\",\"food\",\"service\",\"portion\",\"time\"])\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function for gensim fastText word embeddings in python\n",
    "# remove numbers\n",
    "def process_text(document):     \n",
    "        # Remove extra white space from text\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)     \n",
    "        # Remove all the special characters from text\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "        # Remove all single characters from text\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "        # Word tokenization       \n",
    "        tokens = document.split()\n",
    "        # Lemmatization using NLTK\n",
    "        lemma_txt = [stemmer.lemmatize(word) for word in tokens]\n",
    "        # Remove stop words\n",
    "        lemma_no_stop_txt = [word for word in lemma_txt if word not in stopwords1]\n",
    "        # Drop words \n",
    "        tokens = [word for word in tokens if len(word) > 3]            \n",
    "        clean_txt = ' '.join(lemma_no_stop_txt)\n",
    "        return clean_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wa', 10), ('great', 8), ('place', 8), ('nice', 6), ('atmosphere', 6), ('good', 4), ('sea', 3), ('view', 3)]\n",
      "[('wa', 205), ('good', 118), ('sauce', 77), ('sweet', 73), ('taste', 65), ('noodle', 54), ('fresh', 52), ('tender', 51)]\n",
      "[('generous', 57), ('small', 28), ('serving', 24), ('wa', 18), ('big', 17), ('size', 16), ('amount', 16), ('good', 13)]\n",
      "[('affordable', 19), ('worth', 16), ('cheaper', 11), ('charge', 8), ('reasonable', 8), ('tag', 7), ('value', 7), ('money', 6)]\n",
      "[('wa', 14), ('friendly', 13), ('good', 8), ('great', 6), ('staff', 5), ('excellent', 3), ('attentive', 3), ('bad', 3)]\n",
      "[('long', 18), ('queue', 17), ('wait', 8), ('lunch', 6), ('even', 6), ('waiting', 5), ('snaking', 4), ('dinner', 4)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_text = pd.concat([train_1,train_2,train_3,val,test],axis=0)\n",
    "all_text1 = all_text.loc[(all_text.new_aspect_1.notnull()) & (all_text.new_aspect_2.notnull())]\n",
    "all_text2 = all_text1.groupby([\"new_aspect_2\"]).agg({\"phrase\": \" \".join})\n",
    "all_text2 = all_text2.reset_index()\n",
    "all_text2.phrase = all_text2.phrase.map(process_text)\n",
    "count = 0 \n",
    "\n",
    "all_text2.new_aspect_2.values\n",
    "\n",
    "\n",
    "data_set = all_text2.loc[all_text2.new_aspect_2 == \"ambience\" , \"phrase\"][0]\n",
    "# split() returns list of all the words in the string\n",
    "split_it = data_set.split()\n",
    "\n",
    "# Pass the split_it list to instance of Counter class.\n",
    "Counter0 = Counter(split_it)\n",
    "#print(Counter)\n",
    "\n",
    "# most_common() produces k frequently encountered\n",
    "# input values and their respective counts.\n",
    "most_occur = Counter0.most_common(8)\n",
    "print(most_occur)\n",
    "\n",
    "data_set = all_text2.loc[all_text2.new_aspect_2 == \"food\" , \"phrase\"][1]\n",
    "# split() returns list of all the words in the string\n",
    "split_it = data_set.split()\n",
    "\n",
    "# Pass the split_it list to instance of Counter class.\n",
    "Counter1 = Counter(split_it)\n",
    "#print(Counter)\n",
    "\n",
    "# most_common() produces k frequently encountered\n",
    "# input values and their respective counts.\n",
    "most_occur = Counter1.most_common(8)\n",
    "print(most_occur)\n",
    "\n",
    "\n",
    "\n",
    "data_set = all_text2.loc[all_text2.new_aspect_2 == \"portion\" , \"phrase\"][2]\n",
    "# split() returns list of all the words in the string\n",
    "split_it = data_set.split()\n",
    "\n",
    "# Pass the split_it list to instance of Counter class.\n",
    "Counter2 = Counter(split_it)\n",
    "#print(Counter)\n",
    "\n",
    "# most_common() produces k frequently encountered\n",
    "# input values and their respective counts.\n",
    "most_occur = Counter2.most_common(8)\n",
    "print(most_occur)\n",
    "\n",
    "data_set = all_text2.loc[all_text2.new_aspect_2 == \"price\" , \"phrase\"][3]\n",
    "# split() returns list of all the words in the string\n",
    "split_it = data_set.split()\n",
    "\n",
    "# Pass the split_it list to instance of Counter class.\n",
    "Counter3 = Counter(split_it)\n",
    "#print(Counter)\n",
    "\n",
    "# most_common() produces k frequently encountered\n",
    "# input values and their respective counts.\n",
    "most_occur = Counter3.most_common(8)\n",
    "print(most_occur)\n",
    "\n",
    "data_set = all_text2.loc[all_text2.new_aspect_2 == \"service\" , \"phrase\"][4]\n",
    "# split() returns list of all the words in the string\n",
    "split_it = data_set.split()\n",
    "\n",
    "# Pass the split_it list to instance of Counter class.\n",
    "Counter4 = Counter(split_it)\n",
    "#print(Counter)\n",
    "\n",
    "# most_common() produces k frequently encountered\n",
    "# input values and their respective counts.\n",
    "most_occur = Counter4.most_common(8)\n",
    "print(most_occur)\n",
    "\n",
    "data_set = all_text2.loc[all_text2.new_aspect_2 == \"time\" , \"phrase\"][5]\n",
    "# split() returns list of all the words in the string\n",
    "split_it = data_set.split()\n",
    "\n",
    "# Pass the split_it list to instance of Counter class.\n",
    "Counter5 = Counter(split_it)\n",
    "#print(Counter)\n",
    "\n",
    "# most_common() produces k frequently encountered\n",
    "# input values and their respective counts.\n",
    "most_occur = Counter5.most_common(8)\n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing\n",
    "\n",
    "2 data sets for modelling\n",
    "\n",
    "1. Use this original data without noise\n",
    "2. Create new dataset --> Filter the phrase column (punctuations/hashtags) then combine the aspects into 1 & aggregate the sentiment scores (label)\n",
    "\n",
    "Question : when aggregating use majority? but what if i had alot of noise eg. 0 0 0 0 0 and only 1 -1 then using majority voting will only get 0 not -1? And also mean doesnt work either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3094, 11)\n",
      "(1150, 11)\n",
      "(1010, 11)\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "noise = train.loc[(train.label == 0.0) & (train.new_aspect_1.isnull()) & (train.new_aspect_2.isnull())].index\n",
    "train1 = train.loc[~train.index.isin(noise)]\n",
    "print(train1.shape)\n",
    "\n",
    "# test\n",
    "noise = val.loc[(val.label == 0.0) & (val.new_aspect_1.isnull()) & (val.new_aspect_2.isnull())].index\n",
    "val1 = val.loc[~val.index.isin(noise)]\n",
    "print(val1.shape)\n",
    "\n",
    "#val \n",
    "noise = test.loc[(test.label == 0.0) & (test.new_aspect_1.isnull()) & (test.new_aspect_2.isnull())].index\n",
    "test1 = test.loc[~test.index.isin(noise)]\n",
    "print(test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.0    2025\n",
      " 1.0     897\n",
      "-1.0     171\n",
      "Name: label, dtype: int64\n",
      " 0.0    516\n",
      " 1.0    430\n",
      "-1.0     63\n",
      "Name: label, dtype: int64\n",
      " 0    762\n",
      " 1    335\n",
      "-1     53\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train1.label.value_counts())\n",
    "print(test1.label.value_counts())\n",
    "print(val1.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aggregate sentiment again!\n",
    "\n",
    "- average then sign(>0 = 1,=0 =0 <0 = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering (Train) = 3813\n",
      "Number of rows after filtering (Test) = 1352\n",
      "Number of rows after filtering (Val) = 1388\n"
     ]
    }
   ],
   "source": [
    "# filter out phrases with hashtags & those with character length <=5 then get the majority (might not make sense)\n",
    "# train\n",
    "train2_filtered = train.loc[~train.index.isin(train.loc[(train.phrase.str.contains(\"#\")) | (train.phrase.str.len() <= 5)].index)]\n",
    "print(\"Number of rows after filtering (Train) = {}\".format(train2_filtered.shape[0]))\n",
    "train2 = train2_filtered.groupby([\"review_title\",\"account_name\",\"aspect\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                                \"label\": lambda x:x.value_counts().index[0]})\n",
    "train2 = train2.reset_index()\n",
    "\n",
    "\n",
    "# test\n",
    "test2_filtered = test.loc[~test.index.isin(test.loc[(test.phrase.str.contains(\"#\")) | (test.phrase.str.len() <= 5)].index)]\n",
    "print(\"Number of rows after filtering (Test) = {}\".format(test2_filtered.shape[0]))\n",
    "test2 = test2_filtered.groupby([\"review_title\",\"account_name\",\"aspect\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                              \"label\": lambda x:x.value_counts().index[0]})\n",
    "test2 = test2.reset_index()\n",
    "\n",
    "#val \n",
    "val2_filtered = val.loc[~val.index.isin(val.loc[(val.phrase.str.contains(\"#\")) | (val.phrase.str.len() <= 5)].index)]\n",
    "print(\"Number of rows after filtering (Val) = {}\".format(val2_filtered.shape[0]))\n",
    "val2 = val2_filtered.groupby([\"review_title\",\"account_name\",\"aspect\"]).agg({\"phrase\": \" \".join,\n",
    "                                                                              \"label\": lambda x:x.value_counts().index[0]})\n",
    "val2 = val2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1639, 5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use train & val as training\n",
    "train_val2 = pd.concat([train2,val2],axis=0)\n",
    "train_val2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.0    1120\n",
      " 1.0     434\n",
      "-1.0      85\n",
      "Name: label, dtype: int64\n",
      " 0.0    263\n",
      " 1.0    133\n",
      "-1.0     21\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_val2.label.value_counts())\n",
    "print(test2.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Text Vectorisation --> used on data2\n",
    "1. BoW\n",
    "2. Tf-idf\n",
    "3. Fasttext - can handle oov words https://www.quora.com/How-does-fastText-output-a-vector-for-a-word-that-is-not-in-the-pre-trained-model\n",
    "4. word2vec\n",
    "\n",
    "\n",
    "FastText vs word2vec\n",
    "Word2vec treats each word like an atomic entity and generates a vector for each word. Word2vec cannot provide good results for rare and out of vocabulary words.\n",
    "\n",
    "FastText (an extension of word2vec model), treats each word as composed of character n-grams. FastText word embeddings generate better word embeddings for rare and out of vocabulary words because even if words are rare their character n-grams are still shared with other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function for gensim fastText word embeddings in python\n",
    "# remove numbers\n",
    "def process_text(document):     \n",
    "        # Remove extra white space from text\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)     \n",
    "        # Remove all the special characters from text\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "        # Remove all single characters from text\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "        # Word tokenization       \n",
    "        tokens = document.split()\n",
    "        # Lemmatization using NLTK\n",
    "        lemma_txt = [stemmer.lemmatize(word) for word in tokens]\n",
    "        # Remove stop words\n",
    "        lemma_no_stop_txt = [word for word in lemma_txt if word not in stopwords]\n",
    "        # Drop words \n",
    "        tokens = [word for word in tokens if len(word) > 3]            \n",
    "        clean_txt = ' '.join(lemma_no_stop_txt)\n",
    "        return clean_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 3),preprocessor=process_text)\n",
    "bow_train = bow.fit_transform(train_val2.phrase)\n",
    "bow_test = bow.transform(test2.phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 3),preprocessor=process_text)\n",
    "tfidf_train = tfidf.fit_transform(train_val2.phrase)\n",
    "tfidf_test = tfidf.transform(test2.phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1639/1639 [00:00<00:00, 11006.79it/s]\n",
      "100%|██████████| 1639/1639 [00:00<00:00, 266627.79it/s]\n",
      "100%|██████████| 417/417 [00:00<00:00, 10243.97it/s]\n",
      "100%|██████████| 417/417 [00:00<00:00, 95247.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Preprocess data for fasttext word embeddings training\n",
    "train_clean = [process_text(sentence) for sentence in tqdm(train_val2.phrase) if sentence.strip() !='']\n",
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "train_word_tokens = [word_tokenizer.tokenize(sent) for sent in tqdm(train_clean)]\n",
    "\n",
    "# Preprocess data for fasttext word embeddings testing data\n",
    "test_clean = [process_text(sentence) for sentence in tqdm(test2.phrase) if sentence.strip() !='']\n",
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "test_word_tokens = [word_tokenizer.tokenize(sent) for sent in tqdm(test_clean)]\n",
    "\n",
    "# Train Fasttext word embeddings on training data\n",
    "# Defining values for parameters\n",
    "embedding_size = 300\n",
    "window_size = 5\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n",
    " \n",
    "\n",
    "fast_Text_model = FastText(train_word_tokens,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      workers = 4,\n",
    "                      sg=1,\n",
    "                      iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fastText gensim model\n",
    "fast_Text_model.save(\"ftmodel/ft_model_train\")\n",
    "# Load saved gensim fastText model\n",
    "fast_Text_model = Word2Vec.load(\"ftmodel/ft_model_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-0e7f7a7a4025>:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  fasttext_train = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in train_word_tokens]\n",
      "<ipython-input-33-0e7f7a7a4025>:2: RuntimeWarning: Mean of empty slice.\n",
      "  fasttext_train = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in train_word_tokens]\n",
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-33-0e7f7a7a4025>:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  fasttext_test = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in test_word_tokens]\n",
      "<ipython-input-33-0e7f7a7a4025>:4: RuntimeWarning: Mean of empty slice.\n",
      "  fasttext_test = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in test_word_tokens]\n"
     ]
    }
   ],
   "source": [
    "# convert sentence --> average(each word embedding)\n",
    "fasttext_train = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in train_word_tokens]\n",
    "# convert sentence --> average(each word embedding)\n",
    "fasttext_test = [np.array([fast_Text_model[word] for word in sentence]).mean(axis=0) for sentence in test_word_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_train_processed = []\n",
    "for row in fasttext_train:\n",
    "    try:\n",
    "        fasttext_train_processed.append(list(row))\n",
    "    except:\n",
    "        fasttext_train_processed.append([0]*300)\n",
    "\n",
    "fasttext_test_processed = []\n",
    "for row in fasttext_test:\n",
    "    try:\n",
    "        fasttext_test_processed.append(list(row))\n",
    "    except:\n",
    "        fasttext_test_processed.append([0]*300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_train_processed = np.array(fasttext_train_processed).reshape(-1,300)\n",
    "fasttext_test_processed = np.array(fasttext_test_processed).reshape(-1,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelling\n",
    "\n",
    "1. Naive Bayes/Support Vector Machines (NBSVM)/Logistic Regression\n",
    "2. Fasttext\n",
    "3. BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(Baseline) 1. Naive Bayes -Support Vector Machines (NBSVM)- Logistic Regression\n",
    "- works very well when the dataset is very small, at times it worked better than the neural networks based models. Sida Wang et al. \n",
    "- showed that the simple NB and SVM variants outperformed most published results on several sentiment analysis datasets (snippets and longer documents) sometimes providing a new state-of-the-art performance level.\n",
    "- A simple and efficient baseline for sentence classification is to represent sentences as bag of words (BoW) and train a linear classifier, e.g., a logistic regression or an SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       1.00      0.92      0.96        85\n",
      "         0.0       0.99      1.00      1.00      1120\n",
      "         1.0       1.00      0.99      0.99       434\n",
      "\n",
      "    accuracy                           0.99      1639\n",
      "   macro avg       1.00      0.97      0.98      1639\n",
      "weighted avg       0.99      0.99      0.99      1639\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.67      0.10      0.17        21\n",
      "         0.0       0.68      0.89      0.77       263\n",
      "         1.0       0.53      0.29      0.37       133\n",
      "\n",
      "    accuracy                           0.66       417\n",
      "   macro avg       0.63      0.42      0.44       417\n",
      "weighted avg       0.63      0.66      0.61       417\n",
      "\n",
      "SVM + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       1.00      0.13      0.23        85\n",
      "         0.0       0.84      1.00      0.91      1120\n",
      "         1.0       0.99      0.68      0.81       434\n",
      "\n",
      "    accuracy                           0.87      1639\n",
      "   macro avg       0.94      0.60      0.65      1639\n",
      "weighted avg       0.89      0.87      0.85      1639\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.00      0.00      0.00        21\n",
      "         0.0       0.67      0.94      0.78       263\n",
      "         1.0       0.58      0.22      0.32       133\n",
      "\n",
      "    accuracy                           0.66       417\n",
      "   macro avg       0.42      0.38      0.37       417\n",
      "weighted avg       0.61      0.66      0.59       417\n",
      "\n",
      "Naive Bayes + BoW\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.98      0.76      0.86        85\n",
      "         0.0       0.98      1.00      0.99      1120\n",
      "         1.0       0.98      0.97      0.98       434\n",
      "\n",
      "    accuracy                           0.98      1639\n",
      "   macro avg       0.98      0.91      0.94      1639\n",
      "weighted avg       0.98      0.98      0.98      1639\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.00      0.00      0.00        21\n",
      "         0.0       0.66      0.97      0.79       263\n",
      "         1.0       0.64      0.16      0.25       133\n",
      "\n",
      "    accuracy                           0.66       417\n",
      "   macro avg       0.43      0.37      0.35       417\n",
      "weighted avg       0.62      0.66      0.58       417\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       1.00      0.01      0.02        85\n",
      "         0.0       0.82      1.00      0.90      1120\n",
      "         1.0       0.99      0.61      0.75       434\n",
      "\n",
      "    accuracy                           0.84      1639\n",
      "   macro avg       0.93      0.54      0.56      1639\n",
      "weighted avg       0.87      0.84      0.81      1639\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.00      0.00      0.00        21\n",
      "         0.0       0.68      0.97      0.80       263\n",
      "         1.0       0.71      0.22      0.33       133\n",
      "\n",
      "    accuracy                           0.68       417\n",
      "   macro avg       0.46      0.40      0.38       417\n",
      "weighted avg       0.66      0.68      0.61       417\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       1.00      0.47      0.64        85\n",
      "         0.0       0.96      1.00      0.98      1120\n",
      "         1.0       1.00      0.99      1.00       434\n",
      "\n",
      "    accuracy                           0.97      1639\n",
      "   macro avg       0.99      0.82      0.87      1639\n",
      "weighted avg       0.97      0.97      0.97      1639\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       1.00      0.05      0.09        21\n",
      "         0.0       0.67      0.99      0.80       263\n",
      "         1.0       0.73      0.14      0.24       133\n",
      "\n",
      "    accuracy                           0.67       417\n",
      "   macro avg       0.80      0.39      0.38       417\n",
      "weighted avg       0.70      0.67      0.58       417\n",
      "\n",
      "Naive Bayes + TfIdf\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.00      0.00      0.00        85\n",
      "         0.0       0.78      1.00      0.87      1120\n",
      "         1.0       1.00      0.45      0.62       434\n",
      "\n",
      "    accuracy                           0.80      1639\n",
      "   macro avg       0.59      0.48      0.50      1639\n",
      "weighted avg       0.79      0.80      0.76      1639\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.00      0.00      0.00        21\n",
      "         0.0       0.65      0.99      0.78       263\n",
      "         1.0       0.62      0.06      0.11       133\n",
      "\n",
      "    accuracy                           0.65       417\n",
      "   macro avg       0.42      0.35      0.30       417\n",
      "weighted avg       0.60      0.65      0.53       417\n",
      "\n",
      "Logistic Regression + FastText\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.73      0.28      0.41        85\n",
      "         0.0       0.79      0.94      0.86      1120\n",
      "         1.0       0.75      0.46      0.57       434\n",
      "\n",
      "    accuracy                           0.78      1639\n",
      "   macro avg       0.75      0.56      0.61      1639\n",
      "weighted avg       0.77      0.78      0.76      1639\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.40      0.10      0.15        21\n",
      "         0.0       0.68      0.92      0.78       263\n",
      "         1.0       0.59      0.26      0.36       133\n",
      "\n",
      "    accuracy                           0.67       417\n",
      "   macro avg       0.56      0.42      0.43       417\n",
      "weighted avg       0.64      0.67      0.62       417\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM + FastText\n",
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       1.00      0.13      0.23        85\n",
      "         0.0       0.78      0.99      0.87      1120\n",
      "         1.0       0.91      0.46      0.61       434\n",
      "\n",
      "    accuracy                           0.80      1639\n",
      "   macro avg       0.90      0.52      0.57      1639\n",
      "weighted avg       0.83      0.80      0.77      1639\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       1.00      0.10      0.17        21\n",
      "         0.0       0.66      0.98      0.79       263\n",
      "         1.0       0.75      0.16      0.26       133\n",
      "\n",
      "    accuracy                           0.67       417\n",
      "   macro avg       0.80      0.41      0.41       417\n",
      "weighted avg       0.71      0.67      0.59       417\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-cc8d75c1fa07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_val2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Naive Bayes + \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mvectoriser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_effective_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "svm = SVC()\n",
    "\n",
    "all_train = [bow_train,tfidf_train,fasttext_train_processed]\n",
    "all_test = [bow_test,tfidf_test,fasttext_test_processed]\n",
    "all_vectorisers = [\"BoW\",\"TfIdf\",\"FastText\"]\n",
    "\n",
    "for train,test,vectoriser in zip(all_train,all_test,all_vectorisers):\n",
    "    logreg.fit(train,train_val2.label)\n",
    "    \n",
    "    print(\"Logistic Regression + \" + vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(train_val2.label, logreg.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(test2.label, logreg.predict(test))) \n",
    "    \n",
    "    svm.fit(train,train_val2.label)\n",
    "    \n",
    "    print(\"SVM + \" + vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(train_val2.label, svm.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(test2.label, svm.predict(test))) \n",
    "\n",
    "    \n",
    "    nb.fit(train,train_val2.label)\n",
    "    print(\"Naive Bayes + \"+ vectoriser)\n",
    "    print(\"Training Data\")\n",
    "    print(classification_report(train_val2.label, nb.predict(train)))\n",
    "    print(\"Test Data\")\n",
    "    print(classification_report(test2.label, nb.predict(test)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. BERT\n",
    "\n",
    "- Transfer Learning in NLP\n",
    "\n",
    "Transfer learning is a technique where a deep learning model trained on a large dataset is used to perform similar tasks on another dataset. We call such a deep learning model a pre-trained model. The most renowned examples of pre-trained models are the computer vision deep learning models trained on the ImageNet dataset. So, it is better to use a pre-trained model as a starting point to solve a problem rather than building a model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fasttext - FastText uses a simple and efficient baseline for sentence classification( represent sentences as bag of words (BoW) and train a linear classifier). It uses negative sampling, hierarchical softmax and N-gram features to reduce computational cost and improve efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation for fasttext\n",
    "with open('fasttext_input_train2.txt', 'w') as f:\n",
    "    for each_text, each_label in zip(train2['phrase'], train2['label']):\n",
    "        f.writelines(f'__label__{each_label} {each_text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation for fasttext\n",
    "with open('fasttext_input_val2.txt', 'w') as f:\n",
    "    for each_text, each_label in zip(val2['phrase'], val2['label']):\n",
    "        f.writelines(f'__label__{each_label} {each_text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation for fasttext\n",
    "with open('fasttext_input_test2.txt', 'w') as f:\n",
    "    for each_text, each_label in zip(test2['phrase'], test2['label']):\n",
    "        f.writelines(f'__label__{each_label} {each_text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised('fasttext_input_train2.txt', wordNgrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measuring performance on test data\n",
    "def print_results(sample_size, precision, recall):\n",
    "    precision   = round(precision, 2)\n",
    "    recall      = round(recall, 2)\n",
    "    print(f'{sample_size=}')\n",
    "    print(f'{precision=}')\n",
    "    print(f'{recall=}')\n",
    "\n",
    "print_results(*model.test('fasttext_input_test2.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\"great service and delectable modern chinese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems\n",
    "1. Deal with emojis/punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
