{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Aggregated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"new_labels/test_newpreproc_emoticon/train_newpreproc_emoticon.csv\")\n",
    "val = pd.read_csv(\"new_labels/test_newpreproc_emoticon/val_newpreproc_emoticon.csv\")\n",
    "test = pd.read_csv(\"new_labels/test_newpreproc_emoticon/test_newpreproc_emoticon.csv\")\n",
    "trainval = pd.concat([train,val],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((2486, 15), (775, 15))"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "trainval.shape,test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER\n",
    "\n",
    "Developed in 2014, VADER (Valence Aware Dictionary and sEntiment Reasoner) is a pre-trained model that uses rule-based values tuned to sentiments from social media. It evaluates the text of a message and gives you an assessment of not just positive and negative, but the intensity of that emotion as well.\n",
    "\n",
    "It uses a dictionary of terms that it can evaluate. From the GitHub repository this includes examples like:\n",
    "\n",
    "Negations - a modifier that reverses the meaning of a phrase (\"not great\").\n",
    "Contractions - negations, but more complex (\"wasn’t great\").\n",
    "Punctuation - increased intensity (\"It’s great!!!\").\n",
    "Slang - variations of slang words such as \"kinda\", \"sux\", or \"hella\".\n",
    "It's even able to understand acronyms (\"lol\") and emoji (❤).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/xinminaw/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.3182}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "sid.polarity_scores(\"fresh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0           restaurant_code            review_title  \\\n",
       "0           0  109-teochew-yong-tau-foo             Ampang YTF!   \n",
       "1           1  109-teochew-yong-tau-foo             Ampang YTF!   \n",
       "2           2  109-teochew-yong-tau-foo             Ampang YTF!   \n",
       "3           3  109-teochew-yong-tau-foo  Delicious Yong Tau Foo   \n",
       "4           4  109-teochew-yong-tau-foo  Delicious Yong Tau Foo   \n",
       "\n",
       "    account_name new_aspect_1  \\\n",
       "0   Pearlyn Chua     ambience   \n",
       "1   Pearlyn Chua         food   \n",
       "2   Pearlyn Chua         time   \n",
       "3  Simple Foodie         food   \n",
       "4  Simple Foodie         time   \n",
       "\n",
       "                                              phrase  \\\n",
       "0  airconditioned price point loving place ca wai...   \n",
       "1  soup dry laksa fried rice fried chicken wings ...   \n",
       "2  snaking queue intimidated long cos moves fairl...   \n",
       "3  yong tau foohidden circular road smack town sh...   \n",
       "4                                        clear queue   \n",
       "\n",
       "                                        phrase_lemma  \\\n",
       "0  airconditioned price point loving place ca wai...   \n",
       "1  soup dry laksa fried rice fried chicken wing b...   \n",
       "2  snaking queue intimidated long co move fairly ...   \n",
       "3  yong tau foohidden circular road smack town sh...   \n",
       "4                                        clear queue   \n",
       "\n",
       "                                         phrase_stem  \\\n",
       "0     aircondit price point love place ca wait visit   \n",
       "1  soup dri laksa fri rice fri chicken wing brown...   \n",
       "2       snake queue intimid long co move fairli fast   \n",
       "3  yong tau foohidden circular road smack town sh...   \n",
       "4                                        clear queue   \n",
       "\n",
       "                             phrase_emoticon_generic  \\\n",
       "0  airconditioned price point loving place ca wai...   \n",
       "1  soup dry laksa fried rice fried chicken wings ...   \n",
       "2  snaking queue intimidated long cos moves fairl...   \n",
       "3  yong tau foohidden circular road smack town sh...   \n",
       "4                                        clear queue   \n",
       "\n",
       "                              phrase_emoticon_unique  \\\n",
       "0  airconditioned price point loving place ca wai...   \n",
       "1  soup dry laksa fried rice fried chicken wings ...   \n",
       "2  snaking queue intimidated long cos moves fairl...   \n",
       "3  yong tau foohidden circular road smack town sh...   \n",
       "4                                        clear queue   \n",
       "\n",
       "                        phrase_stem_emoticon_generic  \\\n",
       "0     aircondit price point love place ca wait visit   \n",
       "1  soup dri laksa fri rice fri chicken wing brown...   \n",
       "2       snake queue intimid long co move fairli fast   \n",
       "3  yong tau foohidden circular road smack town sh...   \n",
       "4                                        clear queue   \n",
       "\n",
       "                       phrase_lemma_emoticon_generic  \\\n",
       "0  airconditioned price point loving place ca wai...   \n",
       "1  soup dry laksa fried rice fried chicken wing b...   \n",
       "2  snaking queue intimidated long co move fairly ...   \n",
       "3  yong tau foohidden circular road smack town sh...   \n",
       "4                                        clear queue   \n",
       "\n",
       "                         phrase_stem_emoticon_unique  \\\n",
       "0     aircondit price point love place ca wait visit   \n",
       "1  soup dri laksa fri rice fri chicken wing brown...   \n",
       "2       snake queue intimid long co move fairli fast   \n",
       "3  yong tau foohidden circular road smack town sh...   \n",
       "4                                        clear queue   \n",
       "\n",
       "                        phrase_lemma_emoticon_unique  label  \\\n",
       "0  airconditioned price point loving place ca wai...    1.0   \n",
       "1  soup dry laksa fried rice fried chicken wing b...    1.0   \n",
       "2  snaking queue intimidated long co move fairly ...   -1.0   \n",
       "3  yong tau foohidden circular road smack town sh...    0.0   \n",
       "4                                        clear queue    1.0   \n",
       "\n",
       "                                     polarity_scores  \n",
       "0  {'neg': 0.0, 'neu': 0.642, 'pos': 0.358, 'comp...  \n",
       "1  {'neg': 0.063, 'neu': 0.663, 'pos': 0.274, 'co...  \n",
       "2  {'neg': 0.293, 'neu': 0.707, 'pos': 0.0, 'comp...  \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "4  {'neg': 0.0, 'neu': 0.278, 'pos': 0.722, 'comp...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>restaurant_code</th>\n      <th>review_title</th>\n      <th>account_name</th>\n      <th>new_aspect_1</th>\n      <th>phrase</th>\n      <th>phrase_lemma</th>\n      <th>phrase_stem</th>\n      <th>phrase_emoticon_generic</th>\n      <th>phrase_emoticon_unique</th>\n      <th>phrase_stem_emoticon_generic</th>\n      <th>phrase_lemma_emoticon_generic</th>\n      <th>phrase_stem_emoticon_unique</th>\n      <th>phrase_lemma_emoticon_unique</th>\n      <th>label</th>\n      <th>polarity_scores</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>109-teochew-yong-tau-foo</td>\n      <td>Ampang YTF!</td>\n      <td>Pearlyn Chua</td>\n      <td>ambience</td>\n      <td>airconditioned price point loving place ca wai...</td>\n      <td>airconditioned price point loving place ca wai...</td>\n      <td>aircondit price point love place ca wait visit</td>\n      <td>airconditioned price point loving place ca wai...</td>\n      <td>airconditioned price point loving place ca wai...</td>\n      <td>aircondit price point love place ca wait visit</td>\n      <td>airconditioned price point loving place ca wai...</td>\n      <td>aircondit price point love place ca wait visit</td>\n      <td>airconditioned price point loving place ca wai...</td>\n      <td>1.0</td>\n      <td>{'neg': 0.0, 'neu': 0.642, 'pos': 0.358, 'comp...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>109-teochew-yong-tau-foo</td>\n      <td>Ampang YTF!</td>\n      <td>Pearlyn Chua</td>\n      <td>food</td>\n      <td>soup dry laksa fried rice fried chicken wings ...</td>\n      <td>soup dry laksa fried rice fried chicken wing b...</td>\n      <td>soup dri laksa fri rice fri chicken wing brown...</td>\n      <td>soup dry laksa fried rice fried chicken wings ...</td>\n      <td>soup dry laksa fried rice fried chicken wings ...</td>\n      <td>soup dri laksa fri rice fri chicken wing brown...</td>\n      <td>soup dry laksa fried rice fried chicken wing b...</td>\n      <td>soup dri laksa fri rice fri chicken wing brown...</td>\n      <td>soup dry laksa fried rice fried chicken wing b...</td>\n      <td>1.0</td>\n      <td>{'neg': 0.063, 'neu': 0.663, 'pos': 0.274, 'co...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>109-teochew-yong-tau-foo</td>\n      <td>Ampang YTF!</td>\n      <td>Pearlyn Chua</td>\n      <td>time</td>\n      <td>snaking queue intimidated long cos moves fairl...</td>\n      <td>snaking queue intimidated long co move fairly ...</td>\n      <td>snake queue intimid long co move fairli fast</td>\n      <td>snaking queue intimidated long cos moves fairl...</td>\n      <td>snaking queue intimidated long cos moves fairl...</td>\n      <td>snake queue intimid long co move fairli fast</td>\n      <td>snaking queue intimidated long co move fairly ...</td>\n      <td>snake queue intimid long co move fairli fast</td>\n      <td>snaking queue intimidated long co move fairly ...</td>\n      <td>-1.0</td>\n      <td>{'neg': 0.293, 'neu': 0.707, 'pos': 0.0, 'comp...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>109-teochew-yong-tau-foo</td>\n      <td>Delicious Yong Tau Foo</td>\n      <td>Simple Foodie</td>\n      <td>food</td>\n      <td>yong tau foohidden circular road smack town sh...</td>\n      <td>yong tau foohidden circular road smack town sh...</td>\n      <td>yong tau foohidden circular road smack town sh...</td>\n      <td>yong tau foohidden circular road smack town sh...</td>\n      <td>yong tau foohidden circular road smack town sh...</td>\n      <td>yong tau foohidden circular road smack town sh...</td>\n      <td>yong tau foohidden circular road smack town sh...</td>\n      <td>yong tau foohidden circular road smack town sh...</td>\n      <td>yong tau foohidden circular road smack town sh...</td>\n      <td>0.0</td>\n      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>109-teochew-yong-tau-foo</td>\n      <td>Delicious Yong Tau Foo</td>\n      <td>Simple Foodie</td>\n      <td>time</td>\n      <td>clear queue</td>\n      <td>clear queue</td>\n      <td>clear queue</td>\n      <td>clear queue</td>\n      <td>clear queue</td>\n      <td>clear queue</td>\n      <td>clear queue</td>\n      <td>clear queue</td>\n      <td>clear queue</td>\n      <td>1.0</td>\n      <td>{'neg': 0.0, 'neu': 0.278, 'pos': 0.722, 'comp...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "trainval[\"polarity_scores\"] = trainval.phrase_emoticon_generic.map(lambda phrase : sid.polarity_scores(phrase))\n",
    "test[\"polarity_scores\"] = test.phrase_emoticon_generic.map(lambda phrase : sid.polarity_scores(phrase))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval[\"compound\"] = trainval[\"polarity_scores\"].map(lambda score_dict : score_dict[\"compound\"])\n",
    "test[\"compound\"] = test[\"polarity_scores\"].map(lambda score_dict : score_dict[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval[\"prediction\"] = trainval[\"compound\"].map(lambda c: 1 if c >0 else 0 if c == 0 else -1)\n",
    "test[\"prediction\"] = test[\"compound\"].map(lambda c: 1 if c >0 else 0 if c == 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "VADER Sentiment Analysis Model\nTrainingValidation Data\n              precision    recall  f1-score   support\n\n        -1.0     0.4911    0.3303    0.3950       333\n         0.0     0.6190    0.6467    0.6325       917\n         1.0     0.7500    0.7913    0.7701      1236\n\n    accuracy                         0.6762      2486\n   macro avg     0.6200    0.5894    0.5992      2486\nweighted avg     0.6670    0.6762    0.6691      2486\n\nTest Data\n              precision    recall  f1-score   support\n\n        -1.0     0.5263    0.3846    0.4444       104\n         0.0     0.5532    0.7156    0.6240       218\n         1.0     0.8129    0.7483    0.7793       453\n\n    accuracy                         0.6903       775\n   macro avg     0.6308    0.6162    0.6159       775\nweighted avg     0.7014    0.6903    0.6907       775\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print(\"VADER Sentiment Analysis Model\")\n",
    "print(\"TrainingValidation Data\")\n",
    "print(classification_report(trainval.label,trainval.prediction,digits=4))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(test.label,test.prediction,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TrainingValidation Data\n[[110 150  73]\n [ 71 593 253]\n [ 43 215 978]]\nTest Data\n[[ 40  33  31]\n [ 15 156  47]\n [ 21  93 339]]\n"
     ]
    }
   ],
   "source": [
    "print(\"TrainingValidation Data\")\n",
    "print(confusion_matrix(trainval.label,trainval.prediction))\n",
    "print(\"Test Data\")\n",
    "print(confusion_matrix(test.label,test.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['laksa approx small kind laksa eat',\n",
       "       'laksa lemak rice noodles cut small pieces feels weird genius bowl otah otah laksa',\n",
       "       'expect laksa taste good not exceptional sadly waste money taste good not exceptional sadly waste money',\n",
       "       'small portion', 'felt kinda pricey kinda pricey portion',\n",
       "       'small small bowl', 'air conditioning coild barely felt',\n",
       "       'katong laksa bowl mediocre katong laksa', 'laksa laksa small',\n",
       "       'expensive icecream'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# Examine wrong class -1\n",
    "trainval.loc[(trainval.label == -1) & (trainval.prediction != -1)].phrase.values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update Lexicon Dictionary (Round 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_food = {\n",
    "    \"tender\" : 4,\n",
    "    \"fresh\" : 4,\n",
    "    \"soggy\" : -4,\n",
    "    \"jelat\" : -4,\n",
    "    \"oily\" : -4,\n",
    "    \"overcooked\" :-4,\n",
    "    \"dry\" : -2,\n",
    "    \"disappointed\" : -4  \n",
    "}\n",
    "\n",
    "new_time = {\n",
    "    \"long queue\" : -4,\n",
    "    \"queue\" : -4,\n",
    "    \"wait\" : -2,\n",
    "    \"slow\" : -4,\n",
    "    \"crowd\" : -4\n",
    "}\n",
    "\n",
    "new_price = {\n",
    "    \"pricey\" : -4,\n",
    "    \"expensive\" : -4,\n",
    "    \"cheap\" : 4,\n",
    "    \"worth\" : 4,\n",
    "    \"overpriced\" : -4,\n",
    "    \"not worth\" : -4,\n",
    "    \"value for money\" : 4\n",
    "    \n",
    "}\n",
    "\n",
    "new_portion = {\n",
    "    \"small\" : -4,\n",
    "    \"large\" : 4,\n",
    "    \"generous\" : 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.lexicon.update(new_food)\n",
    "sid.lexicon.update(new_time)\n",
    "sid.lexicon.update(new_price)\n",
    "sid.lexicon.update(new_portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval[\"polarity_scores1\"] = trainval.phrase_emoticon_generic.map(lambda phrase : sid.polarity_scores(phrase))\n",
    "test[\"polarity_scores1\"] = test.phrase_emoticon_generic.map(lambda phrase : sid.polarity_scores(phrase))\n",
    "trainval[\"compound1\"] = trainval[\"polarity_scores1\"].map(lambda score_dict : score_dict[\"compound\"])\n",
    "test[\"compound1\"] = test[\"polarity_scores1\"].map(lambda score_dict : score_dict[\"compound\"])\n",
    "trainval[\"prediction1\"] = trainval[\"compound1\"].map(lambda c: 1 if c >0 else 0 if c == 0 else -1)\n",
    "test[\"prediction1\"] = test[\"compound1\"].map(lambda c: 1 if c >0 else 0 if c == 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "VADER Sentiment Analysis Model (Tuned with new words)\nTrainingValidation Data\n              precision    recall  f1-score   support\n\n        -1.0     0.5450    0.6366    0.5873       333\n         0.0     0.6824    0.6140    0.6464       917\n         1.0     0.7649    0.7872    0.7759      1236\n\n    accuracy                         0.7031      2486\n   macro avg     0.6641    0.6793    0.6699      2486\nweighted avg     0.7050    0.7031    0.7029      2486\n\nTest Data\n              precision    recall  f1-score   support\n\n        -1.0     0.5766    0.6154    0.5953       104\n         0.0     0.5961    0.6972    0.6427       218\n         1.0     0.8337    0.7528    0.7912       453\n\n    accuracy                         0.7187       775\n   macro avg     0.6688    0.6885    0.6764       775\nweighted avg     0.7324    0.7187    0.7231       775\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print(\"VADER Sentiment Analysis Model (Tuned with new words)\")\n",
    "print(\"TrainingValidation Data\")\n",
    "print(classification_report(trainval.label,trainval.prediction1,digits=4))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(test.label,test.prediction1,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['instead went pig congee lo mai kai crystal dumplings total damage food photo rice normal sticky glutinous like not salty lomaikai porridge time stick congee smooth criminal bonus not salty',\n",
       "       'laksadelicious laksa', 'cravings laksa goes signature katong',\n",
       "       'laksa onsen egglovely edition egg', 'big laksa',\n",
       "       'waiting time no waiting waiting time no waiting',\n",
       "       'friday pig intestines big', 'good coffee sg good coffee sg',\n",
       "       'affordable utilising burpple',\n",
       "       'clay pot medium sauce dried chili slices ginger cloves mushy garlic heavenly supposed soggy decent dish overall'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Examine wrong class 1\n",
    "trainval.loc[(trainval.label == 1) & (trainval.prediction1 != 1)].phrase.values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update Lexicon Dictionary (Round 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_food = {\n",
    "    \"tender\" : 2,\n",
    "    \"fresh\" : 2,\n",
    "    \"soggy\" : -2,\n",
    "    \"jelat\" : -2,\n",
    "    \"oily\" : -2,\n",
    "    \"overcooked\" :-2,\n",
    "    \"dry\" : -2,\n",
    "    \"disappointed\" : -2,\n",
    "    \"cravings satisfied\" : 2,\n",
    "    \"crispy\" : 2,\n",
    "    \"sinful\" : 2,\n",
    "    \"tough\" : -2,\n",
    "    \"cold\" : -2\n",
    "}\n",
    "\n",
    "new_time = {\n",
    "    \"long queue\" : -2,\n",
    "    \"queue\" : -2,\n",
    "    \"wait\" : -2,\n",
    "    \"slow\" : -2,\n",
    "    \"crowd\" : -2,\n",
    "    \"crowded\" : -2,\n",
    "    \"no waiting time\" : 2,\n",
    "    \"fast\" : 2,\n",
    "}\n",
    "\n",
    "new_price = {\n",
    "    \"pricey\" : -2,\n",
    "    \"expensive\" : -2,\n",
    "    \"cheap\" : 2,\n",
    "    \"worth\" : 2,\n",
    "    \"overpriced\" : -2,\n",
    "    \"not worth\" : -2,\n",
    "    \"value for money\" : 2,\n",
    "    \"reasonable\" : 2,\n",
    "    \"reasonably\" : 2,\n",
    "    \"affordable\" : 2,\n",
    "    \"steal\" : 2   \n",
    "}\n",
    "\n",
    "new_portion = {\n",
    "    \"small\" : -2,\n",
    "    \"large\" : 2,\n",
    "    \"generous\" : 2,\n",
    "    \"sufficient\" : 1,\n",
    "    \"enough\" : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.lexicon.update(new_food)\n",
    "sid.lexicon.update(new_time)\n",
    "sid.lexicon.update(new_price)\n",
    "sid.lexicon.update(new_portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval[\"polarity_scores2\"] = trainval.phrase_emoticon_generic.map(lambda phrase : sid.polarity_scores(phrase))\n",
    "test[\"polarity_scores2\"] = test.phrase_emoticon_generic.map(lambda phrase : sid.polarity_scores(phrase))\n",
    "trainval[\"compound2\"] = trainval[\"polarity_scores2\"].map(lambda score_dict : score_dict[\"compound\"])\n",
    "test[\"compound2\"] = test[\"polarity_scores2\"].map(lambda score_dict : score_dict[\"compound\"])\n",
    "trainval[\"prediction2\"] = trainval[\"compound2\"].map(lambda c: 1 if c >0 else 0 if c == 0 else -1)\n",
    "test[\"prediction2\"] = test[\"compound2\"].map(lambda c: 1 if c >0 else 0 if c == 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "VADER Sentiment Analysis Model (Tuned with new words 2)\nTrainingValidation Data\n              precision    recall  f1-score   support\n\n        -1.0     0.5562    0.6096    0.5817       333\n         0.0     0.7009    0.6031    0.6483       917\n         1.0     0.7605    0.8196    0.7889      1236\n\n    accuracy                         0.7116      2486\n   macro avg     0.6725    0.6774    0.6730      2486\nweighted avg     0.7111    0.7116    0.7093      2486\n\nTest Data\n              precision    recall  f1-score   support\n\n        -1.0     0.6400    0.6154    0.6275       104\n         0.0     0.6234    0.6835    0.6521       218\n         1.0     0.8349    0.8035    0.8189       453\n\n    accuracy                         0.7445       775\n   macro avg     0.6994    0.7008    0.6995       775\nweighted avg     0.7492    0.7445    0.7463       775\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print(\"VADER Sentiment Analysis Model (Tuned with new words 2)\")\n",
    "print(\"TrainingValidation Data\")\n",
    "print(classification_report(trainval.label,trainval.prediction2,digits=4))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(test.label,test.prediction2,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold CV For Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_k_fold_VADER(column, data, model_name):\n",
    "    \n",
    "    # Generate fold predictions\n",
    "    fold_num = 1\n",
    "    for tf_combi in data:\n",
    "        train = tf_combi[0].copy() # no training needed for VADER \n",
    "        predict_on = tf_combi[1] \n",
    "\n",
    "        # Get Labels\n",
    "        train_label = train.label # not required\n",
    "        \n",
    "        # Fit Model\n",
    "        train[\"polarity_scores\"] = train[column].map(lambda phrase : sid.polarity_scores(phrase))\n",
    "        train[\"pos\"] = train[\"polarity_scores\"].map(lambda score_dict : score_dict[\"pos\"])\n",
    "        train[\"neg\"] = train[\"polarity_scores\"].map(lambda score_dict : score_dict[\"neg\"])\n",
    "\n",
    "        # Create Dataframe and output\n",
    "        df = pd.DataFrame(data=train[[\"neg\",\"pos\",\"label\",\"new_aspect_1\"]].values, columns = [model_name+'_prob_neg', model_name+'_prob_pos',\"label\",\"aspect\"])\n",
    "        # df.drop(columns= [model_name+'_prob_neu'])\n",
    "        ordered_cols = [model_name+'_prob_pos',model_name+'_prob_neg',\"label\",\"aspect\"]\n",
    "        df=df[ordered_cols]\n",
    "        if fold_num <=5:\n",
    "            path = \"kfold/\" + model_name + '_fold' + str(fold_num) +'.csv'\n",
    "        else:\n",
    "            path = \"kfold/\" + model_name + '_test.csv'\n",
    "        \n",
    "        df.to_csv(path, index=False)\n",
    "        \n",
    "        fold_num +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "fold1 = pd.read_csv('stacking_folds/fold1.csv')\n",
    "fold2 = pd.read_csv('stacking_folds/fold2.csv')\n",
    "fold3 = pd.read_csv('stacking_folds/fold3.csv')\n",
    "fold4 = pd.read_csv('stacking_folds/fold4.csv')\n",
    "fold5 = pd.read_csv('stacking_folds/fold5.csv')\n",
    "\n",
    "# VADER DONT NEED\n",
    "train1 = pd.read_csv('stacking_folds/train1.csv')\n",
    "train2 = pd.read_csv('stacking_folds/train2.csv')\n",
    "train3 = pd.read_csv('stacking_folds/train3.csv')\n",
    "train4 = pd.read_csv('stacking_folds/train4.csv')\n",
    "train5 = pd.read_csv('stacking_folds/train5.csv')\n",
    "\n",
    "# VADER DONT NEED\n",
    "train_all = pd.read_csv('stacking_folds/train_all.csv')\n",
    "test = pd.read_csv('stacking_folds/test.csv')\n",
    "\n",
    "# store in suitable data structure\n",
    "data = [(fold1, fold1), (fold2, fold2),(fold3, fold3), (fold4, fold4), (fold5, fold5), (test, test)]\n",
    "\n",
    "column = \"phrase_emoticon_generic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_k_fold_VADER(column=column, data=data, model_name=\"VADER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full model\n",
    "full_df = pd.read_csv(\"new_labels/ALL_LABELLED_DATA.csv\")\n",
    "\n",
    "full_df[\"polarity_scores\"] = full_df[column].map(lambda phrase : sid.polarity_scores(phrase))\n",
    "full_df[\"pos\"] = full_df[\"polarity_scores\"].map(lambda score_dict : score_dict[\"pos\"])\n",
    "full_df[\"neg\"] = full_df[\"polarity_scores\"].map(lambda score_dict : score_dict[\"neg\"])\n",
    "\n",
    "model_name = \"VADER\"\n",
    "# Create Dataframe and output\n",
    "df = pd.DataFrame(data=full_df[[\"neg\",\"pos\",\"label\"]].values, columns = [model_name+'_prob_neg', model_name+'_prob_pos',\"label\"])\n",
    "# df.drop(columns= [model_name+'_prob_neu'])\n",
    "ordered_cols = [model_name+'_prob_pos',model_name+'_prob_neg',\"label\"]\n",
    "df=df[ordered_cols]\n",
    "\n",
    "df.to_csv(\"fold_predictions/VADER/VADER_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}