{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_csv(\"train_newpreproc.csv\",index_col=0)\n",
    "#val = pd.read_csv(\"val_newpreproc.csv\",index_col=0)\n",
    "#test = pd.read_csv(\"test_newpreproc.csv\",index_col=0)\n",
    "train = pd.read_csv(\"train_oldpreproc.csv\",index_col=0)\n",
    "val = pd.read_csv(\"val_oldpreproc.csv\",index_col=0)\n",
    "test = pd.read_csv(\"test_oldpreproc.csv\",index_col=0)\n",
    "trainval = pd.concat([train,val],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER\n",
    "\n",
    "Developed in 2014, VADER (Valence Aware Dictionary and sEntiment Reasoner) is a pre-trained model that uses rule-based values tuned to sentiments from social media. It evaluates the text of a message and gives you an assessment of not just positive and negative, but the intensity of that emotion as well.\n",
    "\n",
    "It uses a dictionary of terms that it can evaluate. From the GitHub repository this includes examples like:\n",
    "\n",
    "Negations - a modifier that reverses the meaning of a phrase (\"not great\").\n",
    "Contractions - negations, but more complex (\"wasn’t great\").\n",
    "Punctuation - increased intensity (\"It’s great!!!\").\n",
    "Slang - variations of slang words such as \"kinda\", \"sux\", or \"hella\".\n",
    "It's even able to understand acronyms (\"lol\") and emoji (❤).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/xinminaw/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_code</th>\n",
       "      <th>review_title</th>\n",
       "      <th>account_name</th>\n",
       "      <th>new_aspect_1</th>\n",
       "      <th>phrase</th>\n",
       "      <th>phrase_lemma</th>\n",
       "      <th>phrase_stem</th>\n",
       "      <th>label</th>\n",
       "      <th>polarity_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>328-katong-laksa-united-square</td>\n",
       "      <td>328 Katong Laksa; Now you don't have to travel...</td>\n",
       "      <td>Qing Xiang</td>\n",
       "      <td>food</td>\n",
       "      <td>katong laksa defeated gordon ramsay version la...</td>\n",
       "      <td>katong laksa defeated gordon ramsay version la...</td>\n",
       "      <td>katong laksa defeat gordon ramsay version laks...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.163, 'neu': 0.684, 'pos': 0.153, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>328-katong-laksa-united-square</td>\n",
       "      <td>Best Laksa In Singapore</td>\n",
       "      <td>Clara Choo</td>\n",
       "      <td>food</td>\n",
       "      <td>best laksa prawns fish cakes</td>\n",
       "      <td>best laksa prawn fish cake</td>\n",
       "      <td>best laksa prawn fish cake</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.488, 'pos': 0.512, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>328-katong-laksa-united-square</td>\n",
       "      <td>Decided to settle at [328 KATONG LAKSA] for lu...</td>\n",
       "      <td>Natalie Tan</td>\n",
       "      <td>food</td>\n",
       "      <td>nowhere comparable original east coast rd got ...</td>\n",
       "      <td>nowhere comparable original east coast rd got ...</td>\n",
       "      <td>nowher compar origin east coast rd got ta admi...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'neg': 0.125, 'neu': 0.511, 'pos': 0.364, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7thheavenktvandcafe</td>\n",
       "      <td>For 1-for-1 Main Dish (save ~$20)</td>\n",
       "      <td>Burpple Guides</td>\n",
       "      <td>food</td>\n",
       "      <td>sing heart content dining hearty western pulle...</td>\n",
       "      <td>sing heart content dining hearty western pulle...</td>\n",
       "      <td>sing heart content dine hearti western pull po...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7thheavenktvandcafe</td>\n",
       "      <td>Grilled Chicken With Truffled Mash</td>\n",
       "      <td>E S</td>\n",
       "      <td>food</td>\n",
       "      <td>dine th heaven nicely grilled juicy tender</td>\n",
       "      <td>dine th heaven nicely grilled juicy tender</td>\n",
       "      <td>dine th heaven nice grill juici tender</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.446, 'pos': 0.554, 'comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  restaurant_code  \\\n",
       "0  328-katong-laksa-united-square   \n",
       "1  328-katong-laksa-united-square   \n",
       "2  328-katong-laksa-united-square   \n",
       "3             7thheavenktvandcafe   \n",
       "4             7thheavenktvandcafe   \n",
       "\n",
       "                                        review_title    account_name  \\\n",
       "0  328 Katong Laksa; Now you don't have to travel...      Qing Xiang   \n",
       "1                            Best Laksa In Singapore      Clara Choo   \n",
       "2  Decided to settle at [328 KATONG LAKSA] for lu...     Natalie Tan   \n",
       "3                  For 1-for-1 Main Dish (save ~$20)  Burpple Guides   \n",
       "4                 Grilled Chicken With Truffled Mash             E S   \n",
       "\n",
       "  new_aspect_1                                             phrase  \\\n",
       "0         food  katong laksa defeated gordon ramsay version la...   \n",
       "1         food                       best laksa prawns fish cakes   \n",
       "2         food  nowhere comparable original east coast rd got ...   \n",
       "3         food  sing heart content dining hearty western pulle...   \n",
       "4         food         dine th heaven nicely grilled juicy tender   \n",
       "\n",
       "                                        phrase_lemma  \\\n",
       "0  katong laksa defeated gordon ramsay version la...   \n",
       "1                         best laksa prawn fish cake   \n",
       "2  nowhere comparable original east coast rd got ...   \n",
       "3  sing heart content dining hearty western pulle...   \n",
       "4         dine th heaven nicely grilled juicy tender   \n",
       "\n",
       "                                         phrase_stem  label  \\\n",
       "0  katong laksa defeat gordon ramsay version laks...      1   \n",
       "1                         best laksa prawn fish cake      1   \n",
       "2  nowher compar origin east coast rd got ta admi...      0   \n",
       "3  sing heart content dine hearti western pull po...      1   \n",
       "4             dine th heaven nice grill juici tender      1   \n",
       "\n",
       "                                     polarity_scores  \n",
       "0  {'neg': 0.163, 'neu': 0.684, 'pos': 0.153, 'co...  \n",
       "1  {'neg': 0.0, 'neu': 0.488, 'pos': 0.512, 'comp...  \n",
       "2  {'neg': 0.125, 'neu': 0.511, 'pos': 0.364, 'co...  \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "4  {'neg': 0.0, 'neu': 0.446, 'pos': 0.554, 'comp...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainval[\"polarity_scores\"] = trainval.phrase.map(lambda phrase : sid.polarity_scores(phrase))\n",
    "test[\"polarity_scores\"] = test.phrase.map(lambda phrase : sid.polarity_scores(phrase))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval[\"compound\"] = trainval[\"polarity_scores\"].map(lambda score_dict : score_dict[\"compound\"])\n",
    "test[\"compound\"] = test[\"polarity_scores\"].map(lambda score_dict : score_dict[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval[\"prediction\"] = trainval[\"compound\"].map(lambda c: 1 if c >0 else 0 if c == 0 else -1)\n",
    "test[\"prediction\"] = test[\"compound\"].map(lambda c: 1 if c >0 else 0 if c == 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER Sentiment Analysis Model\n",
      "TrainingValidation Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.23      0.21      0.22       125\n",
      "           0       0.64      0.64      0.64       677\n",
      "           1       0.72      0.73      0.73       762\n",
      "\n",
      "    accuracy                           0.65      1564\n",
      "   macro avg       0.53      0.53      0.53      1564\n",
      "weighted avg       0.65      0.65      0.65      1564\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.44      0.44      0.44        34\n",
      "           0       0.47      0.63      0.54       104\n",
      "           1       0.82      0.70      0.76       245\n",
      "\n",
      "    accuracy                           0.66       383\n",
      "   macro avg       0.58      0.59      0.58       383\n",
      "weighted avg       0.69      0.66      0.67       383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print(\"VADER Sentiment Analysis Model\")\n",
    "print(\"TrainingValidation Data\")\n",
    "print(classification_report(trainval.label,trainval.prediction))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(test.label,test.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingValidation Data\n",
      "[[ 26  71  28]\n",
      " [ 56 432 189]\n",
      " [ 29 173 560]]\n",
      "Test Data\n",
      "[[ 15  11   8]\n",
      " [  8  66  30]\n",
      " [ 11  62 172]]\n"
     ]
    }
   ],
   "source": [
    "print(\"TrainingValidation Data\")\n",
    "print(confusion_matrix(trainval.label,trainval.prediction))\n",
    "print(\"Test Data\")\n",
    "print(confusion_matrix(test.label,test.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['popular coffee stall high expectations tad stale dry kaya slightly sweet',\n",
       "       'set came pretty small pot soup rice', 'tender chicken meat dry',\n",
       "       'promising taste cheese looking forward hoping nice stick ice cream',\n",
       "       'pricey size',\n",
       "       'got ala carte piece garlicky chicken time tad salty liking fried chicken time tad salty liking',\n",
       "       'long waiting time', 'longer time', 'ham',\n",
       "       'bound gripe waiting time pretty long min gripe waiting time pretty long min',\n",
       "       'little', 'not half', 'minimal protein curries stews',\n",
       "       'extra seven bucks', 'small bowl', 'gripe portion',\n",
       "       'little bit sweeter preferred',\n",
       "       'tasted greasy tasted like ketchup unnaturally flavourful soup nice sweet savoury sauce drizzled rice alongside',\n",
       "       'small portions', 'plain plus sauce garnish', 'price',\n",
       "       'look queue basic ingredients', 'little pricey',\n",
       "       'portion small small', 'slight min wait ish min wait',\n",
       "       'pork belly plus peppered chilli powder chinese spanish style order dish',\n",
       "       'snaking queue city hall', 'price lunch ouch price lunch ouch',\n",
       "       'snaking queue', 'long ish wait lunchtime',\n",
       "       'sweet potato bingsu ran green tea shaved ice cream melted fast condensed milk',\n",
       "       'eating snow flakes took long time serve abt mins long time serve abt mins',\n",
       "       'unbearably spicy ordered regular level spice',\n",
       "       'packed dinner time',\n",
       "       'korean style beef shrimp combined pizza double cheese twist asked recommendation bite proved wrong strong flavors',\n",
       "       'additional',\n",
       "       'lovers probably little cheese ai black rice dough stuffed mozzarella cream mousse diced chicken cheesy experience possible jelatness',\n",
       "       'becareful lunch time crowd', 'wary price portions',\n",
       "       'not spicy based sauce spice', 'issue service slow quiet',\n",
       "       'smallest portion waiting time close min sat night smallest portion waiting time close min sat night',\n",
       "       'romaine lettuce pcs dressed parmesan cheese thousand island dressing dissapointed type salad',\n",
       "       'long queue', 'long queue', 'pork meat bit small buy portion',\n",
       "       'sat evening waiting time super long',\n",
       "       'try lunch sets felt creamy saltier like', 'run pasta',\n",
       "       'time splurge starbucks drink', 'small portion shredded',\n",
       "       'small bowl small bowl', 'petite herb mix',\n",
       "       'monkfish liver slight fish pungent flavour sushi aburisushi',\n",
       "       'smallish size offers respectable selection dishes standard serving',\n",
       "       'industrial not taste', 'higher price higher price',\n",
       "       'base cents veg toppings onions sweet corn chicken breast bit tough complaint rich',\n",
       "       'added tea egg sadly tasteless pretty satisfying',\n",
       "       'typical fish chips hybrid dory chicken fillet fries option change hot veggies sunny nestled crispy tortilla melted cheese runny yolk bowl unfortunately bland',\n",
       "       'wished tea kick stronger', 'charge kind pricey',\n",
       "       'sweet vanilla cotton candy white grapes green apple bitter oranges',\n",
       "       'sweet taste bacon cheese balls like potato',\n",
       "       'pieces bread small slice', 'kinda small',\n",
       "       'belittle small size small size best',\n",
       "       'small serving scallops small serving scallops felt minced pork instead',\n",
       "       'crowded lunch dinner time crowded lunch dinner time crowded lunch dinner time',\n",
       "       'bit little', 'additional percent service charge added',\n",
       "       'long queue', 'extra charge', 'albeit pricey',\n",
       "       'snaking queue people queue opening pm',\n",
       "       'snaking queue forms opens pm', 'portions little', 'tiny bowl',\n",
       "       'pricey size', 'abit mushy',\n",
       "       'real onion batter tasted like overfried refried serving hard getting burger set refried serving hard bite',\n",
       "       'bigger small', 'small price',\n",
       "       'small salad regular portion smaller comes small salad',\n",
       "       'okay portion size small', 'staff brusque', 'smaller plumper',\n",
       "       'kebabs amd pretty expensive',\n",
       "       'ordered tomato soup base stuff like pig brain liver cow stomach ball hard not nice quail egg sweet potato noodle',\n",
       "       'lose mr coconut ice cream slightly acquired ppl wld like wldnt like strong tasting try flavours tropical coco damn coconutty real tasting taste slight bitterness come frm real coconuts',\n",
       "       'creamy herb chicken pie interesting crust bit',\n",
       "       'thai milk tea taste justify chendol sundae cone',\n",
       "       'grilled chicken salted egg yolk interesting combination grill meat local favourite sauce dishes sweet gets bit gelat garlic rice lacking seasoning flavour suckling pig amazing day chinese sausage',\n",
       "       'eat monster curry katsu felt level spice gelat ness comes soup drink dessert pork katsu felt heavy',\n",
       "       'dropped drastically', 'chewy small',\n",
       "       'romaine lettuce choice carbs chicken breast roasted bell peppers cauliflower mushroom medley ingredients particularly oily toasted coconut flakes romesco sauce oil',\n",
       "       'including service charge pricey',\n",
       "       'brown sugar taste clearly non existent flavour oatly milk'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine wrong class -1\n",
    "trainval.loc[(trainval.label == -1) & (trainval.prediction != -1)].phrase.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update Lexicon Dictionary (Round 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_food = {\n",
    "    \"tender\" : 4,\n",
    "    \"fresh\" : 4,\n",
    "    \"soggy\" : -4,\n",
    "    \"jelat\" : -4,\n",
    "    \"oily\" : -4,\n",
    "    \"overcooked\" :-4,\n",
    "    \"dry\" : -2,\n",
    "    \"disappointed\" : -4  \n",
    "}\n",
    "\n",
    "new_time = {\n",
    "    \"long queue\" : -4,\n",
    "    \"queue\" : -4,\n",
    "    \"wait\" : -2,\n",
    "    \"slow\" : -4,\n",
    "    \"crowd\" : -4\n",
    "}\n",
    "\n",
    "new_price = {\n",
    "    \"pricey\" : -4,\n",
    "    \"expensive\" : -4,\n",
    "    \"cheap\" : 4,\n",
    "    \"worth\" : 4,\n",
    "    \"overpriced\" : -4,\n",
    "    \"not worth\" : -4,\n",
    "    \"value for money\" : 4\n",
    "    \n",
    "}\n",
    "\n",
    "new_portion = {\n",
    "    \"small\" : -4,\n",
    "    \"large\" : 4,\n",
    "    \"generous\" : 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.lexicon.update(new_food)\n",
    "sid.lexicon.update(new_time)\n",
    "sid.lexicon.update(new_price)\n",
    "sid.lexicon.update(new_portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval[\"polarity_scores1\"] = trainval.phrase.map(lambda phrase : sid.polarity_scores(phrase))\n",
    "test[\"polarity_scores1\"] = test.phrase.map(lambda phrase : sid.polarity_scores(phrase))\n",
    "trainval[\"compound1\"] = trainval[\"polarity_scores1\"].map(lambda score_dict : score_dict[\"compound\"])\n",
    "test[\"compound1\"] = test[\"polarity_scores1\"].map(lambda score_dict : score_dict[\"compound\"])\n",
    "trainval[\"prediction1\"] = trainval[\"compound1\"].map(lambda c: 1 if c >0 else 0 if c == 0 else -1)\n",
    "test[\"prediction1\"] = test[\"compound1\"].map(lambda c: 1 if c >0 else 0 if c == 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER Sentiment Analysis Model (Tuned with new words)\n",
      "TrainingValidation Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.36      0.50      0.41       125\n",
      "           0       0.68      0.60      0.64       677\n",
      "           1       0.72      0.75      0.73       762\n",
      "\n",
      "    accuracy                           0.67      1564\n",
      "   macro avg       0.59      0.62      0.60      1564\n",
      "weighted avg       0.68      0.67      0.67      1564\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.42      0.59      0.49        34\n",
      "           0       0.52      0.62      0.57       104\n",
      "           1       0.82      0.71      0.76       245\n",
      "\n",
      "    accuracy                           0.67       383\n",
      "   macro avg       0.59      0.64      0.61       383\n",
      "weighted avg       0.71      0.67      0.68       383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print(\"VADER Sentiment Analysis Model (Tuned with new words)\")\n",
    "print(\"TrainingValidation Data\")\n",
    "print(classification_report(trainval.label,trainval.prediction1))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(test.label,test.prediction1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['went pig congee lo mai kai crystal dumplings total damage food photo rice normal sticky glutinous sourness smooth not salty sghawker sgfoodie time stick congee smooth not salty',\n",
       "       'sizzling pop chicken available limited order tom yum chicken combined soy garlic hot sauce options form wingettes drumettes boneless bites friedchicken fingerlickinggood fourfingers',\n",
       "       'stark resembelance packaging papa pahelta version ones espresso league',\n",
       "       'big piece not',\n",
       "       'black sauce chilli flavorful skin nicely crisped yellow rice touch dry',\n",
       "       'fragrant feel generally refreshing serves quality craft beer',\n",
       "       'affordable option',\n",
       "       'scooping oil chili dark soya bowl ungracefully stained liquid taste way prawn noodles unassuming',\n",
       "       'fried potato cubes nice bite oily duck meat',\n",
       "       'omurice look photos reviews fried rice tomato disappointed serving not look photos',\n",
       "       'affordable', 'sufficient portion sufficient portion',\n",
       "       'spicy chipotle sauce array gourmet sausages cook steak preferred doneness',\n",
       "       'roasted coffee right balance spice', 'extra mile serving',\n",
       "       'luxurious sauce not overly sweet',\n",
       "       'little oily overall enjoyed wonderful traditional taste missing wanton noodles qq slightly soft',\n",
       "       'guess no flour', 'cooked flavourful not easy dish right',\n",
       "       'serving course dinner menu priced affordably adult',\n",
       "       'value money evening charge weight shocking pay bowl',\n",
       "       'decent featured favourite cauliflower fried rice curried cauliflower',\n",
       "       'mussels sliced fish variety meat beef pork chicken',\n",
       "       'premium beef equal ratio fats meat slice gets cooked seconds',\n",
       "       'order', 'flavourful chicken rice big portion fragrant',\n",
       "       'big portion fragrant rice',\n",
       "       'oil achieve crispy burnt fried effect fried kailan pork floss point non oily leaves juicy crunchy stems convenient alternative crab add chilli',\n",
       "       'no additional cost additional cost', 'affordable prices',\n",
       "       'marinated shrimp paste extra savory kick',\n",
       "       'milky smooth touch xo going', 'faced steaming soup efforts paid',\n",
       "       'tasted authentic',\n",
       "       'banmian foodporn instafood instafoodie sgfood foodstagram hawkerfood chinesefood foodporn instafood instafoodie sgfood',\n",
       "       'suffused braising flavour snappy',\n",
       "       'dim sum background bbq pork pastry yummm', 'waiting long',\n",
       "       'satiated till late evening', 'attentive',\n",
       "       'sinful chilli crab situation',\n",
       "       'splash sesame oil enhanced flavour soup sliced pork minced',\n",
       "       'roasted meat instead sliced thinly grain duck ticks nice savoury dimension plan beeline popular ban mian stall implore try stall specialises charcoal roasted meat instead',\n",
       "       'roasted duck char siew noodles roasted meat wanton noodle enjoyed oily juicy slices duck',\n",
       "       'lot ingredients serving portion',\n",
       "       'regular size plate char siew rice regular size plate char siew rice',\n",
       "       'fluffy rice signature dark sauce braised duck drumstick chicken breast meat',\n",
       "       'big portion', 'fluffy rice',\n",
       "       'humble plate roast duck meat fluffy rice', 'decent',\n",
       "       'meatball fish cakes lot', 'fast', 'appetizing',\n",
       "       'know bouncy fishball noodle stall orchard ion food court',\n",
       "       'flavoured strawberry milk foam refreshing iced black tea literal tip opt sugar',\n",
       "       'bad carrot cake crispy',\n",
       "       'fried chicken wing added slice luncheon meat infused fluffy rice fried egg brings ingredients tasty',\n",
       "       'fried rice normmmmm fragrant pour laksa gravy chilli',\n",
       "       'unagi don appetizer spinach right fat japanese rice',\n",
       "       'kaisen hitsumabushi kaisen tendon studded chawanmushi',\n",
       "       'arnd pm queue yay',\n",
       "       'add egg molten yolk typical crispy type turns soggy nice touch',\n",
       "       'crunchy bits similar green tea matcha hot fudge sundae',\n",
       "       'double samurai beef burger sinful fries nomnomnom',\n",
       "       'need meat smith lunch platter dreary work day afternoons',\n",
       "       'got meat platters price got meat platters price',\n",
       "       'similar croissant texture smoked nori butter sesame seaweed furikake oozed softly bread',\n",
       "       'big piece cod',\n",
       "       'taste coconut water neutralises gelat effect makes refreshing flesh nom',\n",
       "       'filled tasty lean meat pau skin dry trying traditional handmade pau authentic recipe jb malaysia',\n",
       "       'copious amounts nutella crispy toasted ciabatta',\n",
       "       'looking like noodle fool noodles actually look chinese version mee kia springy al dente cooked egg going thai place trying tom yum dishes',\n",
       "       'handmade noodles smooth chewy black soybean paste starchy sauce not grainy',\n",
       "       'ethiopian gemandro coffee beans jeju island soft alkaline spring water neutralize acidity served tap nitrogen gas smooth creamy texture available yah chicken tikka smoked duck bacon',\n",
       "       'curry rice mid week lunch plate reasonable priced lots ingredients',\n",
       "       'goes peanuts bean sprouts', 'unique dry bak kut disappoint',\n",
       "       'set exterior revealed gooey luscious',\n",
       "       'affiliated otoko ramen different broths shoyu tonkotsu miso tsukemen spicy minced pork spring onions poached egg noodles laced punchy umami flavour',\n",
       "       'place chinatown point sells affordable reliable ramen mazesoba',\n",
       "       'cozy little joint', 'fanciful decoration inside',\n",
       "       'salted egg savoury creaminess',\n",
       "       'spicy beef soup served balls tendon dish drink',\n",
       "       'famous dough balls served garlic butter pesto siciliano dipping base',\n",
       "       'flavourful garlic aglio olio base juicy prawns', 'lots liao',\n",
       "       'balanced meal olive rice flavorful need dressing corn black shredded cabbage not',\n",
       "       'distinct banana taste topped nuttiness bite cake',\n",
       "       'flavourful broth paired chewy rice noodles topped sprinkle fried garlic beef slices noodles',\n",
       "       'authentic boat noodles pick beef pork protein broths taste different usual chilli fish sauce',\n",
       "       'lots selections lots selections',\n",
       "       'reasonable price reasonable price', 'wait long wait long',\n",
       "       'succulent meat sandwiched focaccia spicy remoulade sauce goes seafood dishes mayonnaise',\n",
       "       'stall favourite pink',\n",
       "       'dig bowl bcm soup oops dry version wanton mee nice bite overcooked like stalls pretty tasty wait dig bowl bcm soup oops dry version',\n",
       "       'filled ingredients', 'weather literally melting',\n",
       "       'pork slices minced boiled hours extract extra depth umami fish maw soaking soup burst flavor mouthful',\n",
       "       'big chunks',\n",
       "       'ordinary wanton noodle diet size serawak kolo mee tasty',\n",
       "       'picked hotdog bun option topped crispy shakemeister ale marinated shallots shakemeister burger',\n",
       "       'chew satay bee hoon red sauce eat yong tau foo bean sprouts creamier gravy pungent',\n",
       "       'chew satay bee hoon red sauce eat yong tau foo bean sprouts creamier gravy pungent',\n",
       "       'buttermilk chocolate batter tasty country chicken',\n",
       "       'cheaper price price', 'canned fruits match waffle',\n",
       "       'thai chicken refreshing',\n",
       "       'fluffy cake fragrance milk tea thicker',\n",
       "       'favourite croissant spot singapore crisp croissant watching creamy hokkaidomilkcroissant delish eat',\n",
       "       'freshly fried decent number prawns mushrooms beansprouts bamboo shoot miso soup shredded cabbage salad',\n",
       "       'chicken ball egg pork curry rice meatballs bed slathered aromatic japanese',\n",
       "       'quick service',\n",
       "       'short ribs wobbly onsen egg truffle yakiniku sure crowd pleaser garlic butter chicken',\n",
       "       'big place', 'vibe vibe',\n",
       "       'nutritious soup chinese cabbage fish maw mee sua wheat vermicelli',\n",
       "       'local heritage hawker signature dishes available',\n",
       "       'aromatic broth spice notes',\n",
       "       'seasoned truffle mayonnaise dip ordering rounds fries need',\n",
       "       'add anymore salt salmon gave balanced taste', 'famous duck pizza',\n",
       "       'tall glass mocha iced signature chocolate tart mango yuzu millefeuille',\n",
       "       'noodle soup beef ball available toktoksg somerset seasoning looked simple noodles addictive tasty',\n",
       "       'meepok sungei road laksa stewed pig trotter braised chicken feet ter hoong pig intestines mini fishcake minced pork balanced mix chilli sambal black vinegar',\n",
       "       'affordable price probably reason', 'flavourful sauce',\n",
       "       'favourite dish night bursting umami bowl incredibly flavourful paired naan',\n",
       "       'favourite cookie sauce favourite cookie sauce',\n",
       "       'favourite cookie sauce', 'big portion',\n",
       "       'long fast come early queue avoid dissapointment',\n",
       "       'handmade soup flavourful',\n",
       "       'satay ages definitely old school awesomeness high scores meats mutton chicken beef tripe rarely oh good old school awesomeness high scores meats mutton beef tripe rarely oh good',\n",
       "       'place occasions', 'liberal amounts pineapple',\n",
       "       'sliced halves savoury unami foodporn', 'big',\n",
       "       'blue vanilla ice cream pea seller caramel flavor',\n",
       "       'light curry not forgetting crunchy french goes rice porridge',\n",
       "       'goes porridge cabbage salted fish steamed pork', 'shorter queue',\n",
       "       'decent price',\n",
       "       'soft succulent fish filet aromatic bite potato salad mayo sauce meal chunks pretty tough think overcooked main star crispy',\n",
       "       'affordable price', 'individual portion reasonable size',\n",
       "       'mushroom savory garlic bread', 'belly big',\n",
       "       'pig skin curry sauce ccf silky smooth runny not spicy expect gravy pig skin curry sauce ccf silky smooth',\n",
       "       'reasonable price', 'filling right not weak strong melts mouth',\n",
       "       'affordable cheese tart', 'manage tempura batter light oily',\n",
       "       'seasoned slab egg rice mackerel sushi vinegared fish complements',\n",
       "       'affordable price tag', 'affordable breakfast midday tea snack',\n",
       "       'mexican taco chorizo sgig instagood foodporn foodies',\n",
       "       'grilled chicken wrap way health conscious strapped time',\n",
       "       'molten chocolate inside crispy pie crust instant',\n",
       "       'crisp lettuce', 'premium ingredients abalone crabs',\n",
       "       'combines expertise head bartender',\n",
       "       'balanced oolong syrup served tiao started serving cocktails',\n",
       "       'way', 'half price',\n",
       "       'spice usually person absolute favourite green earth steamboat buffet',\n",
       "       'usual ones cheaper price cheaper price cheapest porridge usual ones cheaper price',\n",
       "       'affordable lunch sets ofc situation telok ayer cbd crowd',\n",
       "       'trying time actual shop small cosy korean ish vibe rly felt like overseas ofc convos ppl night damn hilarious points company luh tried',\n",
       "       'kfc likely army stew spicy kfc not punch sauced soy seafood pancake likely army stew',\n",
       "       'bibimbap bomb not pic mixed liao korean fusion food tasting',\n",
       "       'appetizing sourly laksa soup dark soy sauce expected wok hei missing',\n",
       "       'salted yolk lava bun spare ribs black bean sauce rice roll chicken lotus leaf prawns fluffy mini egg tart fried radish cake xo sauce roasted crispy pork belly skin tossed fried vemicelli seafood',\n",
       "       'thai dish silky coated right sauce order', 'cheaper tiger sugar',\n",
       "       'popular sushi bar offers omakase sets different prices lunch hijiki seaweed salad truffled onsen egg ikura pieces assorted sushi small chirashi bowl hand roll torched scallop sea salt',\n",
       "       'use angoli fish threadfin bream high grade firm flaky time curryfishhead curry',\n",
       "       'favourite peanut butter malted milkshake incredibly addictive',\n",
       "       'try nitro green tea latte frap smoother', 'affordable affordable',\n",
       "       'bowl salmon wo waste money not finishing',\n",
       "       'foie gras belly crispy skin little salty depending preference ice cream nuts',\n",
       "       'reasonable price glass',\n",
       "       'pancakes maple syrup scrambled eggs tasty bacon',\n",
       "       'pancakes maple syrup scrambled eggs tasty bacon',\n",
       "       'trying seasonal delicacy',\n",
       "       'tart lemon meringue tart total burpple unique',\n",
       "       'quick time urging',\n",
       "       'black squiggly noodle like object hidden chinese noodles known silver needle rat noodle distinct flavours basil chilli blended create fiery unforgettable sensation glides throat no time mouthful',\n",
       "       'tasty japanese rice'], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine wrong class 1\n",
    "trainval.loc[(trainval.label == 1) & (trainval.prediction1 != 1)].phrase.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update Lexicon Dictionary (Round 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_food = {\n",
    "    \"tender\" : 2,\n",
    "    \"fresh\" : 2,\n",
    "    \"soggy\" : -2,\n",
    "    \"jelat\" : -2,\n",
    "    \"oily\" : -2,\n",
    "    \"overcooked\" :-2,\n",
    "    \"dry\" : -2,\n",
    "    \"disappointed\" : -2,\n",
    "    \"cravings satisfied\" : 2,\n",
    "    \"crispy\" : 2,\n",
    "    \"sinful\" : 2,\n",
    "    \"tough\" : -2,\n",
    "    \"cold\" : -2\n",
    "}\n",
    "\n",
    "new_time = {\n",
    "    \"long queue\" : -2,\n",
    "    \"queue\" : -2,\n",
    "    \"wait\" : -2,\n",
    "    \"slow\" : -2,\n",
    "    \"crowd\" : -2,\n",
    "    \"crowded\" : -2,\n",
    "    \"no waiting time\" : 2,\n",
    "    \"fast\" : 2,\n",
    "}\n",
    "\n",
    "new_price = {\n",
    "    \"pricey\" : -2,\n",
    "    \"expensive\" : -2,\n",
    "    \"cheap\" : 2,\n",
    "    \"worth\" : 2,\n",
    "    \"overpriced\" : -2,\n",
    "    \"not worth\" : -2,\n",
    "    \"value for money\" : 2,\n",
    "    \"reasonable\" : 2,\n",
    "    \"reasonably\" : 2,\n",
    "    \"affordable\" : 2,\n",
    "    \"steal\" : 2\n",
    "    \n",
    "}\n",
    "\n",
    "new_portion = {\n",
    "    \"small\" : -2,\n",
    "    \"large\" : 2,\n",
    "    \"generous\" : 2,\n",
    "    \"sufficient\" : 1,\n",
    "    \"enough\" : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.lexicon.update(new_food)\n",
    "sid.lexicon.update(new_time)\n",
    "sid.lexicon.update(new_price)\n",
    "sid.lexicon.update(new_portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval[\"polarity_scores2\"] = trainval.phrase.map(lambda phrase : sid.polarity_scores(phrase))\n",
    "test[\"polarity_scores2\"] = test.phrase.map(lambda phrase : sid.polarity_scores(phrase))\n",
    "trainval[\"compound2\"] = trainval[\"polarity_scores2\"].map(lambda score_dict : score_dict[\"compound\"])\n",
    "test[\"compound2\"] = test[\"polarity_scores2\"].map(lambda score_dict : score_dict[\"compound\"])\n",
    "trainval[\"prediction2\"] = trainval[\"compound2\"].map(lambda c: 1 if c >0 else 0 if c == 0 else -1)\n",
    "test[\"prediction2\"] = test[\"compound2\"].map(lambda c: 1 if c >0 else 0 if c == 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER Sentiment Analysis Model (Tuned with new words 2)\n",
      "TrainingValidation Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.36      0.48      0.41       125\n",
      "           0       0.70      0.59      0.64       677\n",
      "           1       0.72      0.78      0.75       762\n",
      "\n",
      "    accuracy                           0.68      1564\n",
      "   macro avg       0.60      0.62      0.60      1564\n",
      "weighted avg       0.69      0.68      0.68      1564\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.44      0.59      0.51        34\n",
      "           0       0.55      0.62      0.58       104\n",
      "           1       0.83      0.74      0.78       245\n",
      "\n",
      "    accuracy                           0.70       383\n",
      "   macro avg       0.61      0.65      0.62       383\n",
      "weighted avg       0.72      0.70      0.71       383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print(\"VADER Sentiment Analysis Model (Tuned with new words 2)\")\n",
    "print(\"TrainingValidation Data\")\n",
    "print(classification_report(trainval.label,trainval.prediction2))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(test.label,test.prediction2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7017232375979112"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(34/383)*0.51 + (104/383)*0.58 + (245/383)*0.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
