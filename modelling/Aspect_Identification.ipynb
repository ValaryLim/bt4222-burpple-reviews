{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from textblob import TextBlob\n",
    "#from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1 : Extracting aspects using lexicon matching using a corpus from yelp \n",
    "Reference : https://medium.com/@borgesalkan/yelp-popular-dishes-with-aspect-based-sentiment-analysis-796c191245bf\n",
    "\n",
    "Lexicon : the vocabulary of a person, language, or branch of knowledge. This can include the technical terms of a particular subject or field / how the terms are usually used as well. \n",
    "\n",
    "Domain specific lexicons extracted (using NER) from the yelp review dataset was used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEXICONS from yelp corpus so might not include local food names eg zi char etc\n",
    "FOOD_LEXICONS = set(pd.read_csv(\"../data/rule_mining/yelp_lexicon/yelpnlg-lexicons/food.csv\", header=None, names = [\"food\"])[\"food\"].str.lower())\n",
    "AMBIENCE_LEXICONS = set(pd.read_csv(\"../data/rule_mining/yelp_lexicon/yelpnlg-lexicons/ambience.csv\", header=None, names = [\"ambience\"])[\"ambience\"].str.lower())\n",
    "PRICE_LEXICONS = set(pd.read_csv(\"../data/rule_mining/yelp_lexicon/yelpnlg-lexicons/price.csv\", header=None, names = [\"price\"])[\"price\"].str.lower())\n",
    "SERVICE_LEXICONS = set(pd.read_csv(\"../data/rule_mining/yelp_lexicon/yelpnlg-lexicons/service.csv\", header=None, names = [\"service\"])[\"service\"].str.lower())\n",
    "TIME_LEXICONS = set([\"time\",\"queue\",\"wait\"]) # aspects extracted from topic modelling\n",
    "PORTION_LEXICONS = set([\"portion\",\"size\",\"serving\"]) # aspects extracted from topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_ngrams(data, num):\n",
    "   n_grams = TextBlob(data).ngrams(num)\n",
    "   return [' '.join(grams).lower() for grams in n_grams]\n",
    "\n",
    "def _delete_duplicate_food_n_grams(text, foods):\n",
    "   foods.sort(key=lambda x: -len(x.split()))  # Sort desc by number of words\n",
    "   result_foods = []\n",
    "   for food in foods:\n",
    "       if food in text:\n",
    "           text = text.replace(food, '')\n",
    "           result_foods.append(food)\n",
    "   \n",
    "   return result_foods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = [\"food\",\"ambience\",\"price\",\"service\",\"time\",\"portion\"]\n",
    "ASPECTS_LEXICONS = [FOOD_LEXICONS,AMBIENCE_LEXICONS,PRICE_LEXICONS,SERVICE_LEXICONS,TIME_LEXICONS,PORTION_LEXICONS]\n",
    "all_text = []\n",
    "\n",
    "def extract_aspects(text):\n",
    "    text = text.lower()   #lower review text\n",
    "    all_aspects = {}\n",
    "    for aspect, ASPECT_LEXICONS in zip(aspects,ASPECTS_LEXICONS):\n",
    "           aspect_list = set()\n",
    "           for n in range(3, 0, -1):\n",
    "               n_grams = _extract_ngrams(text, n)\n",
    "               #n_grams_stemmed = [stemmer.stem(n_gram) for n_gram in n_grams]\n",
    "               n_grams_set = set(n_grams).union(n_grams)\n",
    "               aspect_list = aspect_list.union(n_grams_set.intersection(ASPECT_LEXICONS))\n",
    "           aspect_list = list(aspect_list)\n",
    "           aspect_list = _delete_duplicate_food_n_grams(text, aspect_list)\n",
    "           if len(aspect_list) != 0 :\n",
    "               all_text.extend(aspect_list)\n",
    "               all_aspects[aspect] = aspect_list\n",
    "    return all_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('../data/train_test/reviews_nontest.csv')\n",
    "reviews = reviews.loc[reviews.review_body.notnull(),\"review_body\"]\n",
    "reviews[\"aspects\"] = reviews.map(extract_aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"review\": reviews, \"aspects\": reviews.aspects})\n",
    "df.to_csv(\"../data/rule_mining/aspects_yelp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add in Burple reviews specific food names into lexicon list\n",
    "\n",
    "Use POStagging to extract out food names (usually nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pprint\n",
    "from nltk import Tree\n",
    "import pdb\n",
    "\n",
    "# to extract NounPhrases - food names\n",
    "# Penn Treebank Tag Set differentiates between four different types of nouns: NN, NNS, NNP, NNPS. We will want to consider all types of nouns:\n",
    "# proper and common, singular/mass and plural. Rather than writing separate rules for each case, we can use regular expressions to include them all.\n",
    "patterns=\"\"\".    \n",
    "    NP: {<NN*>+}\n",
    "    {<NN*><CC>*<NN*>+}\n",
    "    {<NP><CC><NP>}\n",
    "    {<RB><NN*>+}\n",
    "    \"\"\"\n",
    "# Chunking breaks a text up into user-defined units ('chunks') that contain certain types of words (nouns, adjectives, verbs) or\n",
    "# phrases (noun phrases, verb phrases, prepositional phrases). What makes chunking with the NLTK different from using a built-in string\n",
    "# method like split is the NLTK's ability to analyze the text and tag each word with its part of speech\n",
    "NPChunker = nltk.RegexpParser(patterns)\n",
    "\n",
    "def prepare_text(input):\n",
    "    sentences = nltk.sent_tokenize(input)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences] \n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    sentences = [NPChunker.parse(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "# combine nouns to noun phrases\n",
    "def parsed_text_to_NP(sentences):\n",
    "    nps = []\n",
    "    for sent in sentences:\n",
    "        tree = NPChunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'NP':  # only extract noun phrases\n",
    "                t = subtree\n",
    "                t = ' '.join(word for word, tag in t.leaves())\n",
    "                nps.append(t)\n",
    "    return nps\n",
    "\n",
    "def sent_parse(input):\n",
    "    sentences = prepare_text(input)\n",
    "    nps = parsed_text_to_NP(sentences)\n",
    "    return nps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"food\"] = reviews.map(sent_parse)\n",
    "df = pd.DataFrame({\"review\": reviews, \"Food\": reviews.food})\n",
    "df.to_csv(\"../data/rule_mining/food_extraction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df1 = pd.read_csv(\"../data/rule_mining/aspects_yelp.csv\",index_col=0)\n",
    "reviews_new_food = pd.read_csv('./data/rule_mining/food_extraction.csv') # code in drive\n",
    "df1[\"new_food\"] = reviews_new_food.Food\n",
    "df1.new_food.fillna(\"[]\",inplace=True)\n",
    "df1.aspects.fillna(\"{}\",inplace=True)\n",
    "df1[\"new_food\"] = df1.new_food.map(lambda x : [text.lower() for text in ast.literal_eval(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"../data/rule_mining/aspects_yelp_with_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../data/rule_mining/aspects_yelp_with_new.csv\",index_col=0)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(row):\n",
    "    foods_ner = ast.literal_eval(row[\"new_food\"])\n",
    "    aspects = ast.literal_eval(row[\"aspects\"])\n",
    "    for word in foods_ner:\n",
    "        if word not in all_text:\n",
    "            if \"food\" not in aspects.keys() or aspects[\"food\"] is None :\n",
    "                aspects[\"food\"] = []               \n",
    "            aspects[\"food\"].append(word)\n",
    "            print(aspects)\n",
    "    print(aspects)\n",
    "    return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"aspects_new\"] = df1.apply(remove_duplicates,axis=1)\n",
    "df1.to_csv(\"../data/rule_mining/final_aspects_terms.csv\") # for rule mining to extract out adjectives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
