{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Rules\n",
    "train_old = pd.read_csv(\"../data/labelled_data/train_oldpreproc.csv\")\n",
    "val_old = pd.read_csv(\"../data/labelled_data/val_oldpreproc.csv\")\n",
    "test_old = pd.read_csv(\"../data/labelled_data/test_oldpreproc.csv\")\n",
    "\n",
    "trainval_old =pd.concat([train_old, val_old])\n",
    "\n",
    "# New rules\n",
    "train_new = pd.read_csv(\"../data/labelled_data/train_newpreproc.csv\")\n",
    "val_new = pd.read_csv(\"../data/labelled_data/val_newpreproc.csv\")\n",
    "test_new = pd.read_csv(\"../data/labelled_data/test_newpreproc.csv\")\n",
    "\n",
    "trainval_new = pd.concat([train_new, val_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>restaurant_code</th>\n",
       "      <th>review_title</th>\n",
       "      <th>account_name</th>\n",
       "      <th>new_aspect_1</th>\n",
       "      <th>phrase</th>\n",
       "      <th>phrase_lemma</th>\n",
       "      <th>phrase_stem</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>03 Jan 2018 my comfort food..</td>\n",
       "      <td>Bobcatsysop YK Chan</td>\n",
       "      <td>time</td>\n",
       "      <td>sold lunch time noon</td>\n",
       "      <td>sold lunch time noon</td>\n",
       "      <td>sold lunch time noon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>How do people take clean #Instafood shots at a...</td>\n",
       "      <td>Lynn Kwek</td>\n",
       "      <td>food</td>\n",
       "      <td>went pig congee lo mai kai crystal dumplings t...</td>\n",
       "      <td>went pig congee lo mai kai crystal dumpling to...</td>\n",
       "      <td>went pig conge lo mai kai crystal dumpl total ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>How do people take clean #Instafood shots at a...</td>\n",
       "      <td>Lynn Kwek</td>\n",
       "      <td>service</td>\n",
       "      <td>friendly owner</td>\n",
       "      <td>friendly owner</td>\n",
       "      <td>friendli owner</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>My Saturday morning started with breakfast at ...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>food</td>\n",
       "      <td>cantonese style congee</td>\n",
       "      <td>cantonese style congee</td>\n",
       "      <td>cantones style conge</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>So glad the weekend is here and I can finally ...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>food</td>\n",
       "      <td>gen shu glutinous rice</td>\n",
       "      <td>gen shu glutinous rice</td>\n",
       "      <td>gen shu glutin rice</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 restaurant_code  \\\n",
       "0           0        2lhaZp7B   \n",
       "1           1        2lhaZp7B   \n",
       "2           2        2lhaZp7B   \n",
       "3           3        2lhaZp7B   \n",
       "4           4        2lhaZp7B   \n",
       "\n",
       "                                        review_title         account_name  \\\n",
       "0                      03 Jan 2018 my comfort food..  Bobcatsysop YK Chan   \n",
       "1  How do people take clean #Instafood shots at a...            Lynn Kwek   \n",
       "2  How do people take clean #Instafood shots at a...            Lynn Kwek   \n",
       "3  My Saturday morning started with breakfast at ...           Maureen Ow   \n",
       "4  So glad the weekend is here and I can finally ...           Maureen Ow   \n",
       "\n",
       "  new_aspect_1                                             phrase  \\\n",
       "0         time                               sold lunch time noon   \n",
       "1         food  went pig congee lo mai kai crystal dumplings t...   \n",
       "2      service                                     friendly owner   \n",
       "3         food                             cantonese style congee   \n",
       "4         food                             gen shu glutinous rice   \n",
       "\n",
       "                                        phrase_lemma  \\\n",
       "0                               sold lunch time noon   \n",
       "1  went pig congee lo mai kai crystal dumpling to...   \n",
       "2                                     friendly owner   \n",
       "3                             cantonese style congee   \n",
       "4                             gen shu glutinous rice   \n",
       "\n",
       "                                         phrase_stem  label  \n",
       "0                               sold lunch time noon      0  \n",
       "1  went pig conge lo mai kai crystal dumpl total ...      1  \n",
       "2                                     friendli owner      1  \n",
       "3                               cantones style conge      0  \n",
       "4                                gen shu glutin rice      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_old.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_params = {\n",
    "    \"analyzer\": [\"word\"],\n",
    "    \"lowercase\": [True],\n",
    "    \"ngram_range\": [(1,1), (1,2), (1,3)],\n",
    "    \"max_df\": [0.25, 0.5, 1.0],\n",
    "    \"min_df\": [1, 10, 20]\n",
    "}\n",
    "bow_paramgrid = list(ParameterGrid(bow_params))\n",
    "\n",
    "type_proc_params = {\n",
    "    \"type\": [\"original\", \"lemma\", \"stem\"], # normal, lemma, stem\n",
    "    \"processing\": [\"old\", \"new\"] # old, new phrase splitting\n",
    "}\n",
    "type_proc_paramgrid = list(ParameterGrid(type_proc_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "logreg_params = {\n",
    "    \"C\": [0.5, 1.0, 1.5],\n",
    "    \"solver\": [\"lbfgs\", \"newton-cg\"],\n",
    "    \"penalty\": [\"l2\", \"none\"],\n",
    "    \"class_weight\": [\"balanced\", None] \n",
    "}\n",
    "logreg_paramgrid = list(ParameterGrid(logreg_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "nb_params = {\n",
    "    \"alpha\": [0, 0.001, 0.01, 0.1, 0.25, 0.5, 1]\n",
    "}\n",
    "nb_paramgrid = list(ParameterGrid(nb_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "svm_params = {\n",
    "    \"C\": [0.5, 1.0, 1.5, 10, 100],\n",
    "    \"kernel\": [\"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"],\n",
    "    \"class_weight\": [\"balanced\", None] \n",
    "}\n",
    "\n",
    "svm_paramgrid = list(ParameterGrid(svm_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = [\n",
    "    {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"class_weight\": [\"balanced\", \"balanced_subsample\", None],\n",
    "        \"bootstrap\": [True, False],\n",
    "        \"max_features\": [\"auto\",\"sqrt\"],\n",
    "        \"n_estimators\": [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)],\n",
    "        \"min_samples_leaf\": [1, 2, 4]\n",
    "    }\n",
    "]\n",
    "\n",
    "rf_paramgrid = list(ParameterGrid(rf_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_params = { \"strategy\": [\"prior\"] }\n",
    "dummy_paramgrid = list(ParameterGrid(dummy_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(bow_param):\n",
    "    # original\n",
    "    bow = CountVectorizer(**bow_param)\n",
    "    bow_train_old = bow.fit_transform(train_old.phrase)\n",
    "    bow_val_old = bow.transform(val_old.phrase)\n",
    "    bow_test_old = bow.transform(test_old.phrase)\n",
    "    bow_trainval_old = bow.transform(trainval_old.phrase)\n",
    "    \n",
    "    bow = CountVectorizer(**bow_param)\n",
    "    bow_train_new = bow.fit_transform(train_new.phrase)\n",
    "    bow_val_new = bow.transform(val_new.phrase)\n",
    "    bow_test_new = bow.transform(test_new.phrase)\n",
    "    bow_trainval_new = bow.transform(trainval_new.phrase)\n",
    "    \n",
    "    # lemmatize\n",
    "    bow = CountVectorizer(**bow_param)\n",
    "    bow_train_old_lemma = bow.fit_transform(train_old.phrase_lemma)\n",
    "    bow_val_old_lemma = bow.transform(val_old.phrase_lemma)\n",
    "    bow_test_old_lemma = bow.transform(test_old.phrase_lemma)\n",
    "    bow_trainval_old_lemma = bow.transform(trainval_old.phrase_lemma)\n",
    "\n",
    "    bow = CountVectorizer(**bow_param)\n",
    "    bow_train_new_lemma = bow.fit_transform(train_new.phrase_lemma)\n",
    "    bow_val_new_lemma = bow.transform(val_new.phrase_lemma)\n",
    "    bow_test_new_lemma = bow.transform(test_new.phrase_lemma)\n",
    "    bow_trainval_new_lemma = bow.transform(trainval_new.phrase_lemma)\n",
    "    \n",
    "    # stem\n",
    "    bow = CountVectorizer(**bow_param)\n",
    "    bow_train_old_stem = bow.fit_transform(train_old.phrase_stem)\n",
    "    bow_val_old_stem = bow.transform(val_old.phrase_stem)\n",
    "    bow_test_old_stem = bow.transform(test_old.phrase_stem)\n",
    "    bow_trainval_old_stem = bow.transform(trainval_old.phrase_stem)\n",
    "    \n",
    "    bow = CountVectorizer(**bow_param)\n",
    "    bow_train_new_stem = bow.fit_transform(train_new.phrase_stem)\n",
    "    bow_val_new_stem = bow.transform(val_new.phrase_stem)\n",
    "    bow_test_new_stem = bow.transform(test_new.phrase_stem)\n",
    "    bow_trainval_new_stem = bow.transform(trainval_new.phrase_stem)\n",
    "    \n",
    "    return {\n",
    "        \"original\": {\n",
    "            \"old\": [bow_train_old, bow_val_old, bow_test_old, bow_trainval_old],\n",
    "            \"new\": [bow_train_new, bow_val_new, bow_test_new, bow_trainval_new]\n",
    "        },\n",
    "        \"lemma\": {\n",
    "            \"old\": [bow_train_old_lemma, bow_val_old_lemma, bow_test_old_lemma, bow_trainval_old_lemma],\n",
    "            \"new\": [bow_train_new_lemma, bow_val_new_lemma, bow_test_new_lemma, bow_trainval_new_lemma]\n",
    "        },\n",
    "        \"stem\": {\n",
    "            \"old\": [bow_train_old_stem, bow_val_old_stem, bow_test_old_stem, bow_trainval_old_stem],\n",
    "            \"new\": [bow_train_new_stem, bow_val_new_stem, bow_test_new_stem, bow_trainval_new_stem]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"old\": [train_old.label, val_old.label, test_old.label, trainval_old.label],\n",
    "    \"new\": [train_new.label, val_new.label, test_new.label, trainval_new.label]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "60\n",
      "90\n",
      "120\n",
      "150\n",
      "180\n",
      "210\n",
      "240\n",
      "270\n",
      "300\n",
      "330\n",
      "360\n",
      "390\n",
      "420\n",
      "450\n",
      "480\n",
      "510\n",
      "540\n",
      "570\n",
      "600\n",
      "630\n",
      "660\n",
      "690\n",
      "720\n",
      "750\n",
      "780\n",
      "810\n",
      "840\n",
      "870\n",
      "900\n",
      "930\n",
      "960\n",
      "990\n",
      "1020\n",
      "1050\n",
      "1080\n",
      "1110\n",
      "1140\n",
      "1170\n",
      "1200\n",
      "1230\n",
      "1260\n",
      "1290\n",
      "1320\n",
      "1350\n",
      "1380\n",
      "1410\n",
      "1440\n",
      "1470\n",
      "1500\n",
      "1530\n",
      "1560\n",
      "1590\n",
      "1620\n",
      "1650\n",
      "1680\n",
      "1710\n",
      "1740\n",
      "1770\n",
      "1800\n",
      "1830\n",
      "1860\n",
      "1890\n",
      "1920\n",
      "1950\n",
      "1980\n",
      "2010\n",
      "2040\n",
      "2070\n",
      "2100\n",
      "2130\n",
      "2160\n",
      "2190\n",
      "2220\n",
      "2250\n",
      "2280\n",
      "2310\n",
      "2340\n",
      "2370\n",
      "2400\n",
      "2430\n",
      "2460\n",
      "2490\n",
      "2520\n",
      "2550\n",
      "2580\n",
      "2610\n",
      "2640\n",
      "2670\n",
      "2700\n",
      "2730\n",
      "2760\n",
      "2790\n",
      "2820\n",
      "2850\n",
      "2880\n",
      "2910\n",
      "2940\n",
      "2970\n",
      "3000\n",
      "3030\n",
      "3060\n",
      "3090\n",
      "3120\n",
      "3150\n",
      "3180\n",
      "3210\n",
      "3240\n",
      "3270\n",
      "3300\n",
      "3330\n",
      "3360\n",
      "3390\n",
      "3420\n",
      "3450\n",
      "3480\n",
      "3510\n",
      "3540\n",
      "3570\n",
      "3600\n",
      "3630\n",
      "3660\n",
      "3690\n",
      "3720\n",
      "3750\n",
      "3780\n",
      "3810\n",
      "3840\n",
      "3870\n"
     ]
    }
   ],
   "source": [
    "model_name = \"logreg\"\n",
    "model_fn = LogisticRegression\n",
    "model_paramgrid = logreg_paramgrid\n",
    "\n",
    "ind = 0 \n",
    "gridsearch_results = []\n",
    "for bow_param in bow_paramgrid:\n",
    "    datasets = prepare_datasets(bow_param)\n",
    "    \n",
    "    for type_proc_param in type_proc_paramgrid:\n",
    "        data_type = type_proc_param[\"type\"]\n",
    "        data_proc = type_proc_param[\"processing\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_proc][0]\n",
    "        val_set = datasets[data_type][data_proc][1]\n",
    "        test_set = datasets[data_type][data_proc][2]\n",
    "        trainval_set = datasets[data_type][data_proc][3]\n",
    "        \n",
    "        train_label = labels[data_proc][0]\n",
    "        val_label = labels[data_proc][1]\n",
    "        test_label = labels[data_proc][2]\n",
    "        trainval_label = labels[data_proc][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # predict\n",
    "            val_f1 = f1_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_recall = recall_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_precision = precision_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_accuracy = accuracy_score(val_label, val_pred)\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # predict\n",
    "            test_f1 = f1_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_recall = recall_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_precision = precision_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_accuracy = accuracy_score(test_label, test_pred)\n",
    "            \n",
    "            results = { \"model\": model_name }\n",
    "            results.update(bow_param)\n",
    "            results.update(type_proc_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1\": val_f1, \"val_recall\": val_recall, \"val_precision\": val_precision, \n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1\": test_f1, \"test_recall\": test_recall, \"test_precision\": test_precision, \n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            ind += 1\n",
    "            if (ind % 50 == 0):\n",
    "                print(ind)\n",
    "            \n",
    "final_logreg_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_logreg_results = final_logreg_results.sort_values(by=[\"val_f1\", \"test_f1\"], ascending=False)\n",
    "final_logreg_results.to_csv(\"model_results/bow/logreg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nb\"\n",
    "model_fn = MultinomialNB\n",
    "model_paramgrid = nb_paramgrid\n",
    "\n",
    "ind = 0 \n",
    "gridsearch_results = []\n",
    "for bow_param in bow_paramgrid:\n",
    "    datasets = prepare_datasets(bow_param)\n",
    "    \n",
    "    for type_proc_param in type_proc_paramgrid:\n",
    "        data_type = type_proc_param[\"type\"]\n",
    "        data_proc = type_proc_param[\"processing\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_proc][0]\n",
    "        val_set = datasets[data_type][data_proc][1]\n",
    "        test_set = datasets[data_type][data_proc][2]\n",
    "        trainval_set = datasets[data_type][data_proc][3]\n",
    "        \n",
    "        train_label = labels[data_proc][0]\n",
    "        val_label = labels[data_proc][1]\n",
    "        test_label = labels[data_proc][2]\n",
    "        trainval_label = labels[data_proc][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # predict\n",
    "            val_f1 = f1_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_recall = recall_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_precision = precision_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_accuracy = accuracy_score(val_label, val_pred)\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # predict\n",
    "            test_f1 = f1_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_recall = recall_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_precision = precision_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_accuracy = accuracy_score(test_label, test_pred)\n",
    "            \n",
    "            results = { \"model\": model_name }\n",
    "            results.update(bow_param)\n",
    "            results.update(type_proc_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1\": val_f1, \"val_recall\": val_recall, \"val_precision\": val_precision, \n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1\": test_f1, \"test_recall\": test_recall, \"test_precision\": test_precision, \n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            ind += 1\n",
    "            if (ind % 50 == 0):\n",
    "                print(ind)\n",
    "            \n",
    "            \n",
    "final_nb_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_nb_results = final_nb_results.sort_values(by=[\"val_f1\", \"test_f1\"], ascending=False)\n",
    "final_nb_results.to_csv(\"model_results/bow/nb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"rf\"\n",
    "model_fn = RandomForestClassifier\n",
    "model_paramgrid = rf_paramgrid\n",
    "\n",
    "ind = 0 \n",
    "gridsearch_results = []\n",
    "for bow_param in bow_paramgrid:\n",
    "    datasets = prepare_datasets(bow_param)\n",
    "    \n",
    "    for type_proc_param in type_proc_paramgrid:\n",
    "        data_type = type_proc_param[\"type\"]\n",
    "        data_proc = type_proc_param[\"processing\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_proc][0]\n",
    "        val_set = datasets[data_type][data_proc][1]\n",
    "        test_set = datasets[data_type][data_proc][2]\n",
    "        trainval_set = datasets[data_type][data_proc][3]\n",
    "        \n",
    "        train_label = labels[data_proc][0]\n",
    "        val_label = labels[data_proc][1]\n",
    "        test_label = labels[data_proc][2]\n",
    "        trainval_label = labels[data_proc][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # predict\n",
    "            val_f1 = f1_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_recall = recall_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_precision = precision_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_accuracy = accuracy_score(val_label, val_pred)\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # predict\n",
    "            test_f1 = f1_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_recall = recall_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_precision = precision_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_accuracy = accuracy_score(test_label, test_pred)\n",
    "            \n",
    "            results = { \"model\": model_name }\n",
    "            results.update(bow_param)\n",
    "            results.update(type_proc_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1\": val_f1, \"val_recall\": val_recall, \"val_precision\": val_precision, \n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1\": test_f1, \"test_recall\": test_recall, \"test_precision\": test_precision, \n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            ind += 1\n",
    "            if (ind % 50 == 0):\n",
    "                print(ind)\n",
    "            \n",
    "            \n",
    "final_rf_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_rf_results = final_rf_results.sort_values(by=[\"val_f1\", \"test_f1\"], ascending=False)\n",
    "final_rf_results.to_csv(\"model_results/bow/rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-4b2d7148475b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;31m# train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mmodel_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mval_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;31m# predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_sparse_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    288\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m                 \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshrinking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m                 random_seed)\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\svm\\libsvm_sparse.pyx\u001b[0m in \u001b[0;36msklearn.svm.libsvm_sparse.libsvm_sparse_train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;34m\"\"\"base matrix class for compressed row and column oriented matrices\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0m_data_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = \"svm\"\n",
    "model_fn = SVC\n",
    "model_paramgrid = svm_paramgrid\n",
    "\n",
    "ind = 0 \n",
    "gridsearch_results = []\n",
    "for bow_param in bow_paramgrid:\n",
    "    datasets = prepare_datasets(bow_param)\n",
    "    \n",
    "    for type_proc_param in type_proc_paramgrid:\n",
    "        data_type = type_proc_param[\"type\"]\n",
    "        data_proc = type_proc_param[\"processing\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_proc][0]\n",
    "        val_set = datasets[data_type][data_proc][1]\n",
    "        test_set = datasets[data_type][data_proc][2]\n",
    "        trainval_set = datasets[data_type][data_proc][3]\n",
    "        \n",
    "        train_label = labels[data_proc][0]\n",
    "        val_label = labels[data_proc][1]\n",
    "        test_label = labels[data_proc][2]\n",
    "        trainval_label = labels[data_proc][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # predict\n",
    "            val_f1 = f1_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_recall = recall_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_precision = precision_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_accuracy = accuracy_score(val_label, val_pred)\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # predict\n",
    "            test_f1 = f1_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_recall = recall_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_precision = precision_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_accuracy = accuracy_score(test_label, test_pred)\n",
    "            \n",
    "            results = { \"model\": model_name }\n",
    "            results.update(bow_param)\n",
    "            results.update(type_proc_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1\": val_f1, \"val_recall\": val_recall, \"val_precision\": val_precision, \n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1\": test_f1, \"test_recall\": test_recall, \"test_precision\": test_precision, \n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            ind += 1\n",
    "            if (ind % 50 == 0):\n",
    "                print(ind)\n",
    "\n",
    "final_svm_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_svm_results = final_svm_results.sort_values(by=[\"val_f1\", \"test_f1\"], ascending=False)\n",
    "final_svm_results.to_csv(\"model_results/bow/svm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dummy\"\n",
    "model_fn = DummyClassifier\n",
    "model_paramgrid = dummy_paramgrid\n",
    "\n",
    "ind = 0 \n",
    "gridsearch_results = []\n",
    "for bow_param in bow_paramgrid:\n",
    "    datasets = prepare_datasets(bow_param)\n",
    "    \n",
    "    for type_proc_param in type_proc_paramgrid:\n",
    "        data_type = type_proc_param[\"type\"]\n",
    "        data_proc = type_proc_param[\"processing\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_proc][0]\n",
    "        val_set = datasets[data_type][data_proc][1]\n",
    "        test_set = datasets[data_type][data_proc][2]\n",
    "        trainval_set = datasets[data_type][data_proc][3]\n",
    "        \n",
    "        train_label = labels[data_proc][0]\n",
    "        val_label = labels[data_proc][1]\n",
    "        test_label = labels[data_proc][2]\n",
    "        trainval_label = labels[data_proc][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # predict\n",
    "            val_f1 = f1_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_recall = recall_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_precision = precision_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_accuracy = accuracy_score(val_label, val_pred)\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # predict\n",
    "            test_f1 = f1_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_recall = recall_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_precision = precision_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_accuracy = accuracy_score(test_label, test_pred)\n",
    "            \n",
    "            results = { \"model\": model_name }\n",
    "            results.update(bow_param)\n",
    "            results.update(type_proc_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1\": val_f1, \"val_recall\": val_recall, \"val_precision\": val_precision, \n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1\": test_f1, \"test_recall\": test_recall, \"test_precision\": test_precision, \n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            ind += 1\n",
    "            if (ind % 50 == 0):\n",
    "                print(ind)\n",
    "                \n",
    "final_dummy_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_dummy_results = final_dummy_results.sort_values(by=[\"val_f1\", \"test_f1\"], ascending=False)\n",
    "final_dummy_results.to_csv(\"model_results/bow/dummy.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([final_logreg_results, final_nb_results, final_svm_results, final_rf_results, \\\n",
    "                        final_dummy_results])\n",
    "combined_df = combined_df.sort_values(by=[\"val_f1\", \"test_f1\"], ascending=False)\n",
    "combined_df.to_csv(\"model_results/bow/combined.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
