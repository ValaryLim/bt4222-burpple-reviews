{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Rules\n",
    "train_old = pd.read_csv(\"../data/labelled_data/train_oldpreproc.csv\")\n",
    "val_old = pd.read_csv(\"../data/labelled_data/val_oldpreproc.csv\")\n",
    "test_old = pd.read_csv(\"../data/labelled_data/test_oldpreproc.csv\")\n",
    "\n",
    "trainval_old =pd.concat([train_old, val_old])\n",
    "\n",
    "# New rules\n",
    "train_new = pd.read_csv(\"../data/labelled_data/train_newpreproc.csv\")\n",
    "val_new = pd.read_csv(\"../data/labelled_data/val_newpreproc.csv\")\n",
    "test_new = pd.read_csv(\"../data/labelled_data/test_newpreproc.csv\")\n",
    "\n",
    "trainval_new = pd.concat([train_new, val_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>restaurant_code</th>\n",
       "      <th>review_title</th>\n",
       "      <th>account_name</th>\n",
       "      <th>new_aspect_1</th>\n",
       "      <th>phrase</th>\n",
       "      <th>phrase_lemma</th>\n",
       "      <th>phrase_stem</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>03 Jan 2018 my comfort food..</td>\n",
       "      <td>Bobcatsysop YK Chan</td>\n",
       "      <td>time</td>\n",
       "      <td>sold lunch time noon</td>\n",
       "      <td>sold lunch time noon</td>\n",
       "      <td>sold lunch time noon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>How do people take clean #Instafood shots at a...</td>\n",
       "      <td>Lynn Kwek</td>\n",
       "      <td>food</td>\n",
       "      <td>went pig congee lo mai kai crystal dumplings t...</td>\n",
       "      <td>went pig congee lo mai kai crystal dumpling to...</td>\n",
       "      <td>went pig conge lo mai kai crystal dumpl total ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>How do people take clean #Instafood shots at a...</td>\n",
       "      <td>Lynn Kwek</td>\n",
       "      <td>service</td>\n",
       "      <td>friendly owner</td>\n",
       "      <td>friendly owner</td>\n",
       "      <td>friendli owner</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>My Saturday morning started with breakfast at ...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>food</td>\n",
       "      <td>cantonese style congee</td>\n",
       "      <td>cantonese style congee</td>\n",
       "      <td>cantones style conge</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2lhaZp7B</td>\n",
       "      <td>So glad the weekend is here and I can finally ...</td>\n",
       "      <td>Maureen Ow</td>\n",
       "      <td>food</td>\n",
       "      <td>gen shu glutinous rice</td>\n",
       "      <td>gen shu glutinous rice</td>\n",
       "      <td>gen shu glutin rice</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 restaurant_code  \\\n",
       "0           0        2lhaZp7B   \n",
       "1           1        2lhaZp7B   \n",
       "2           2        2lhaZp7B   \n",
       "3           3        2lhaZp7B   \n",
       "4           4        2lhaZp7B   \n",
       "\n",
       "                                        review_title         account_name  \\\n",
       "0                      03 Jan 2018 my comfort food..  Bobcatsysop YK Chan   \n",
       "1  How do people take clean #Instafood shots at a...            Lynn Kwek   \n",
       "2  How do people take clean #Instafood shots at a...            Lynn Kwek   \n",
       "3  My Saturday morning started with breakfast at ...           Maureen Ow   \n",
       "4  So glad the weekend is here and I can finally ...           Maureen Ow   \n",
       "\n",
       "  new_aspect_1                                             phrase  \\\n",
       "0         time                               sold lunch time noon   \n",
       "1         food  went pig congee lo mai kai crystal dumplings t...   \n",
       "2      service                                     friendly owner   \n",
       "3         food                             cantonese style congee   \n",
       "4         food                             gen shu glutinous rice   \n",
       "\n",
       "                                        phrase_lemma  \\\n",
       "0                               sold lunch time noon   \n",
       "1  went pig congee lo mai kai crystal dumpling to...   \n",
       "2                                     friendly owner   \n",
       "3                             cantonese style congee   \n",
       "4                             gen shu glutinous rice   \n",
       "\n",
       "                                         phrase_stem  label  \n",
       "0                               sold lunch time noon      0  \n",
       "1  went pig conge lo mai kai crystal dumpl total ...      1  \n",
       "2                                     friendli owner      1  \n",
       "3                               cantones style conge      0  \n",
       "4                                gen shu glutin rice      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_old.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_params = {\n",
    "    \"analyzer\": [\"word\"],\n",
    "    \"lowercase\": [True],\n",
    "    \"ngram_range\": [(1,1), (1,2), (1,3)],\n",
    "    \"max_df\": [0.25, 0.5, 1.0],\n",
    "    \"min_df\": [1, 10, 20]\n",
    "}\n",
    "bow_paramgrid = list(ParameterGrid(bow_params))\n",
    "\n",
    "type_proc_params = {\n",
    "    \"type\": [\"original\", \"lemma\", \"stem\"], # normal, lemma, stem\n",
    "    \"processing\": [\"old\", \"new\"] # old, new phrase splitting\n",
    "}\n",
    "type_proc_paramgrid = list(ParameterGrid(type_proc_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "logreg_params = {\n",
    "    \"C\": [0.5, 1.0, 1.5],\n",
    "    \"solver\": [\"lbfgs\", \"newton-cg\"],\n",
    "    \"penalty\": [\"l2\", \"none\"],\n",
    "    \"class_weight\": [\"balanced\", None] \n",
    "}\n",
    "logreg_paramgrid = list(ParameterGrid(logreg_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "nb_params = {\n",
    "    \"alpha\": [0, 0.01, 0.1, 0.25, 0.5, 1]\n",
    "}\n",
    "nb_paramgrid = list(ParameterGrid(nb_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "svm_params = {\n",
    "    \"C\": [0.5, 1.0, 1.5, 10, 100],\n",
    "    \"kernel\": [\"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"],\n",
    "    \"class_weight\": [\"balanced\", None] \n",
    "}\n",
    "\n",
    "svm_paramgrid = list(ParameterGrid(svm_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_list = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "depth_list.append(None)\n",
    "\n",
    "rf_params = [\n",
    "    {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"class_weight\": [\"balanced\", \"balanced_subsample\", None],\n",
    "        \"bootstrap\": [True, False],\n",
    "        \"max_features\": [\"auto\",\"sqrt\"],\n",
    "        \"n_estimators\": [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "        \"max_depth\": depth_list,\n",
    "        \"min_samples_leaf\": [1, 2, 4]\n",
    "    }\n",
    "]\n",
    "\n",
    "rf_paramgrid = list(ParameterGrid(rf_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_params = { \"strategy\": [\"prior\"] }\n",
    "dummy_paramgrid = list(ParameterGrid(dummy_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(bow_param):\n",
    "    # original\n",
    "    bow = CountVectorizer(**bow_param)\n",
    "    bow_train_old = bow.fit_transform(train_old.phrase)\n",
    "    bow_val_old = bow.transform(val_old.phrase)\n",
    "    bow_test_old = bow.transform(test_old.phrase)\n",
    "    bow_trainval_old = bow.transform(trainval_old.phrase)\n",
    "    \n",
    "    bow = CountVectorizer(**bow_param)\n",
    "    bow_train_new = bow.fit_transform(train_new.phrase)\n",
    "    bow_val_new = bow.transform(val_new.phrase)\n",
    "    bow_test_new = bow.transform(test_new.phrase)\n",
    "    bow_trainval_new = bow.transform(trainval_new.phrase)\n",
    "    \n",
    "    # lemmatize\n",
    "    bow = CountVectorizer(**bow_param)\n",
    "    bow_train_old_lemma = bow.fit_transform(train_old.phrase_lemma)\n",
    "    bow_val_old_lemma = bow.transform(val_old.phrase_lemma)\n",
    "    bow_test_old_lemma = bow.transform(test_old.phrase_lemma)\n",
    "    bow_trainval_old_lemma = bow.transform(trainval_old.phrase_lemma)\n",
    "\n",
    "    bow = CountVectorizer(**bow_param)\n",
    "    bow_train_new_lemma = bow.fit_transform(train_new.phrase_lemma)\n",
    "    bow_val_new_lemma = bow.transform(val_new.phrase_lemma)\n",
    "    bow_test_new_lemma = bow.transform(test_new.phrase_lemma)\n",
    "    bow_trainval_new_lemma = bow.transform(trainval_new.phrase_lemma)\n",
    "    \n",
    "    # stem\n",
    "    bow = CountVectorizer(**bow_param)\n",
    "    bow_train_old_stem = bow.fit_transform(train_old.phrase_stem)\n",
    "    bow_val_old_stem = bow.transform(val_old.phrase_stem)\n",
    "    bow_test_old_stem = bow.transform(test_old.phrase_stem)\n",
    "    bow_trainval_old_stem = bow.transform(trainval_old.phrase_stem)\n",
    "    \n",
    "    bow = CountVectorizer(**bow_param)\n",
    "    bow_train_new_stem = bow.fit_transform(train_new.phrase_stem)\n",
    "    bow_val_new_stem = bow.transform(val_new.phrase_stem)\n",
    "    bow_test_new_stem = bow.transform(test_new.phrase_stem)\n",
    "    bow_trainval_new_stem = bow.transform(trainval_new.phrase_stem)\n",
    "    \n",
    "    return {\n",
    "        \"original\": {\n",
    "            \"old\": [bow_train_old, bow_val_old, bow_test_old, bow_trainval_old],\n",
    "            \"new\": [bow_train_new, bow_val_new, bow_test_new, bow_trainval_new]\n",
    "        },\n",
    "        \"lemma\": {\n",
    "            \"old\": [bow_train_old_lemma, bow_val_old_lemma, bow_test_old_lemma, bow_trainval_old_lemma],\n",
    "            \"new\": [bow_train_new_lemma, bow_val_new_lemma, bow_test_new_lemma, bow_trainval_new_lemma]\n",
    "        },\n",
    "        \"stem\": {\n",
    "            \"old\": [bow_train_old_stem, bow_val_old_stem, bow_test_old_stem, bow_trainval_old_stem],\n",
    "            \"new\": [bow_train_new_stem, bow_val_new_stem, bow_test_new_stem, bow_trainval_new_stem]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"old\": [train_old.label, val_old.label, test_old.label, trainval_old.label],\n",
    "    \"new\": [train_new.label, val_new.label, test_new.label, trainval_new.label]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "60\n",
      "90\n",
      "120\n",
      "150\n",
      "180\n",
      "210\n",
      "240\n",
      "270\n",
      "300\n",
      "330\n",
      "360\n",
      "390\n",
      "420\n",
      "450\n",
      "480\n",
      "510\n",
      "540\n",
      "570\n",
      "600\n",
      "630\n",
      "660\n",
      "690\n",
      "720\n",
      "750\n",
      "780\n",
      "810\n",
      "840\n",
      "870\n",
      "900\n",
      "930\n"
     ]
    }
   ],
   "source": [
    "model_name = \"logreg\"\n",
    "model_fn = LogisticRegression\n",
    "model_paramgrid = logreg_paramgrid\n",
    "\n",
    "ind = 0 \n",
    "gridsearch_results = []\n",
    "for bow_param in bow_paramgrid:\n",
    "    datasets = prepare_datasets(bow_param)\n",
    "    \n",
    "    for type_proc_param in type_proc_paramgrid:\n",
    "        data_type = type_proc_param[\"type\"]\n",
    "        data_proc = type_proc_param[\"processing\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_proc][0]\n",
    "        val_set = datasets[data_type][data_proc][1]\n",
    "        test_set = datasets[data_type][data_proc][2]\n",
    "        trainval_set = datasets[data_type][data_proc][3]\n",
    "        \n",
    "        train_label = labels[data_proc][0]\n",
    "        val_label = labels[data_proc][1]\n",
    "        test_label = labels[data_proc][2]\n",
    "        trainval_label = labels[data_proc][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # predict\n",
    "            val_f1 = f1_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_recall = recall_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_precision = precision_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_accuracy = accuracy_score(val_label, val_pred)\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # predict\n",
    "            test_f1 = f1_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_recall = recall_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_precision = precision_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_accuracy = accuracy_score(test_label, test_pred)\n",
    "            \n",
    "            results = { \"model\": model_name }\n",
    "            results.update(bow_param)\n",
    "            results.update(type_proc_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1\": val_f1, \"val_recall\": val_recall, \"val_precision\": val_precision, \n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1\": test_f1, \"test_recall\": test_recall, \"test_precision\": test_precision, \n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            ind += 1\n",
    "            if (ind % 30 == 0):\n",
    "                print(ind)\n",
    "            \n",
    "final_logreg_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_logreg_results = final_logreg_results.sort_values(by=[\"val_f1\", \"test_f1\"], ascending=False)\n",
    "final_logreg_results.to_csv(\"model_results/bow/logreg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nb\"\n",
    "model_fn = MultinomialNB\n",
    "model_paramgrid = nb_paramgrid\n",
    "\n",
    "ind = 0 \n",
    "gridsearch_results = []\n",
    "for bow_param in bow_paramgrid:\n",
    "    datasets = prepare_datasets(bow_param)\n",
    "    \n",
    "    for type_proc_param in type_proc_paramgrid:\n",
    "        data_type = type_proc_param[\"type\"]\n",
    "        data_proc = type_proc_param[\"processing\"]\n",
    "        \n",
    "        # extract datasets\n",
    "        train_set = datasets[data_type][data_proc][0]\n",
    "        val_set = datasets[data_type][data_proc][1]\n",
    "        test_set = datasets[data_type][data_proc][2]\n",
    "        trainval_set = datasets[data_type][data_proc][3]\n",
    "        \n",
    "        train_label = labels[data_proc][0]\n",
    "        val_label = labels[data_proc][1]\n",
    "        test_label = labels[data_proc][2]\n",
    "        trainval_label = labels[data_proc][3]\n",
    "        \n",
    "        # train models\n",
    "        for model_param in model_paramgrid:\n",
    "            # train model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set, train_label)\n",
    "            val_pred = model.predict(val_set)\n",
    "            # predict\n",
    "            val_f1 = f1_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_recall = recall_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_precision = precision_score(val_label, val_pred, average=\"weighted\")\n",
    "            val_accuracy = accuracy_score(val_label, val_pred)\n",
    "            \n",
    "            # train test_val model\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set, trainval_label)\n",
    "            test_pred = model.predict(test_set)\n",
    "            # predict\n",
    "            test_f1 = f1_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_recall = recall_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_precision = precision_score(test_label, test_pred, average=\"weighted\")\n",
    "            test_accuracy = accuracy_score(test_label, test_pred)\n",
    "            \n",
    "            results = { \"model\": model_name }\n",
    "            results.update(bow_param)\n",
    "            results.update(type_proc_param)\n",
    "            results.update(model_param)\n",
    "            results.update({\"val_f1\": val_f1, \"val_recall\": val_recall, \"val_precision\": val_precision, \n",
    "                            \"val_accuracy\": val_accuracy})\n",
    "            results.update({\"test_f1\": test_f1, \"test_recall\": test_recall, \"test_precision\": test_precision, \n",
    "                            \"test_accuracy\": test_accuracy})\n",
    "            gridsearch_results.append(results)\n",
    "            ind += 1\n",
    "            print(ind)\n",
    "            \n",
    "            \n",
    "final_nb_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "final_nb_results = final_nb_results.sort_values(by=[\"val_f1\", \"test_f1\"], ascending=False)\n",
    "final_nb_results.to_csv(\"model_results/bow/nb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
