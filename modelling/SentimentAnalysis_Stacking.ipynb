{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in predictions from all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = [\"BERT\",\"fasttext\", \"logreg\", \"NB\", \"RF\", \"SVM\",\"VADER\"]\n",
    "meta_model_train = pd.DataFrame()\n",
    "meta_model_test = pd.DataFrame()\n",
    "meta_model_fold_1 = pd.DataFrame()\n",
    "meta_model_fold_2 = pd.DataFrame()\n",
    "meta_model_fold_3 = pd.DataFrame()\n",
    "meta_model_fold_4 = pd.DataFrame()\n",
    "meta_model_fold_5 = pd.DataFrame()\n",
    "\n",
    "for model in models:\n",
    "    fold1pred = pd.read_csv(f'data/fold_predictions/{model}/{model}_fold1.csv')\n",
    "    meta_model_fold_1 = pd.concat([meta_model_fold_1,fold1pred], axis=1)\n",
    "    \n",
    "    fold2pred = pd.read_csv(f'data/fold_predictions/{model}/{model}_fold2.csv')\n",
    "    meta_model_fold_2 = pd.concat([meta_model_fold_2,fold2pred], axis=1)\n",
    "\n",
    "    fold3pred = pd.read_csv(f'data/fold_predictions/{model}/{model}_fold3.csv')\n",
    "    meta_model_fold_3 = pd.concat([meta_model_fold_3,fold3pred], axis=1)\n",
    "\n",
    "    fold4pred = pd.read_csv(f'data/fold_predictions/{model}/{model}_fold4.csv')\n",
    "    meta_model_fold_4 = pd.concat([meta_model_fold_4,fold4pred], axis=1)\n",
    "\n",
    "    fold5pred = pd.read_csv(f'data/fold_predictions/{model}/{model}_fold5.csv')    \n",
    "    meta_model_fold_5 = pd.concat([meta_model_fold_5,fold5pred], axis=1)\n",
    "\n",
    "    all_pred_train = pd.concat([fold1pred,fold2pred,fold3pred,fold4pred,fold5pred],axis = 0)\n",
    "    meta_model_train = pd.concat([meta_model_train,all_pred_train], axis=1)\n",
    "    \n",
    "    testpred = pd.read_csv(f'data/fold_predictions/{model}/{model}_test.csv')\n",
    "    meta_model_test = pd.concat([meta_model_test,testpred], axis=1)\n",
    "\n",
    "    \n",
    "data = [meta_model_fold_1,meta_model_fold_2,meta_model_fold_3,meta_model_fold_4,meta_model_fold_5,meta_model_train]\n",
    "meta_model_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "logreg_params = {\n",
    "    \"C\": [0.1, 0.5, 1.0, 1.5, 5],\n",
    "    \"solver\": [\"lbfgs\", \"newton-cg\"],\n",
    "    \"penalty\": [\"l2\", \"none\"],\n",
    "    \"class_weight\": [\"balanced\", None] \n",
    "}\n",
    "logreg_paramgrid = list(ParameterGrid(logreg_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "nb_params = {\n",
    "    \"alpha\": [0, 0.001, 0.01, 0.1, 0.25, 0.5, 1]\n",
    "}\n",
    "nb_paramgrid = list(ParameterGrid(nb_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "svm_params = {\n",
    "    \"C\": [0.1, 0.5, 1.0, 1.5, 5],\n",
    "    \"kernel\": [\"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"],\n",
    "    \"class_weight\": [\"balanced\", None] \n",
    "}\n",
    "\n",
    "svm_paramgrid = list(ParameterGrid(svm_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = [\n",
    "    {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"class_weight\": [\"balanced\", \"balanced_subsample\", None],\n",
    "        \"max_features\": [\"auto\",\"sqrt\"],\n",
    "        \"min_samples_leaf\": [1, 2, 4]\n",
    "    }\n",
    "]\n",
    "\n",
    "rf_paramgrid = list(ParameterGrid(rf_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_params = { \"strategy\": [\"prior\"] }\n",
    "dummy_paramgrid = list(ParameterGrid(dummy_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression/NB/SVM/DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_grid_search(model_name, model_fn, model_paramgrid,data, test) : \n",
    "    ind = 0 \n",
    "    gridsearch_results = []      \n",
    "    \n",
    "    # train models\n",
    "    for model_param in model_paramgrid:\n",
    "\n",
    "        # 5 fold cross val\n",
    "        val_accuracy = []\n",
    "        val_f1_weighted = []\n",
    "        val_f1_neg = []\n",
    "        val_f1_zero = []\n",
    "        val_f1_pos = []\n",
    "    \n",
    "        test_accuracy = []\n",
    "        test_f1_weighted = []\n",
    "        test_f1_neg = []\n",
    "        test_f1_zero = []\n",
    "        test_f1_pos = []\n",
    "        \n",
    "        for i in range(5):\n",
    "            print(f\"fold {i}\")\n",
    "            train_set = pd.DataFrame()\n",
    "            for x in range(5):\n",
    "                if i != x :\n",
    "                    train_set = pd.concat([train_set,data[x]],axis=0)\n",
    "            val_set = data[i]\n",
    "            test_set = test\n",
    "            trainval_set = data[5]\n",
    "\n",
    "            train_label = train_set.label\n",
    "            val_label = val_set.label\n",
    "            test_label = test_set.label\n",
    "            trainval_label = trainval_set.label\n",
    "\n",
    "            # train on train model test on val\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(train_set.iloc[:,:-1], train_label)\n",
    "            val_pred = model.predict(val_set.iloc[:,:-1])\n",
    "            # scoring\n",
    "            val_metrics = classification_report(val_label, val_pred, output_dict=True)\n",
    "            val_accuracy.append(val_metrics[\"accuracy\"])\n",
    "            val_f1_weighted.append(val_metrics[\"weighted avg\"][\"f1-score\"])\n",
    "            val_f1_neg.append(val_metrics[\"-1.0\"][\"f1-score\"])\n",
    "            val_f1_zero.append(val_metrics[\"0.0\"][\"f1-score\"])\n",
    "            val_f1_pos.append(val_metrics[\"1.0\"][\"f1-score\"])\n",
    "\n",
    "            # train on train_val model test on test\n",
    "            model = model_fn(**model_param)\n",
    "            model.fit(trainval_set.iloc[:,:-1], trainval_label)\n",
    "            test_pred = model.predict(test_set.iloc[:,:-1])\n",
    "\n",
    "            # scoring\n",
    "            test_metrics = classification_report(test_label, test_pred, output_dict=True)\n",
    "            test_accuracy.append(test_metrics[\"accuracy\"])\n",
    "            test_f1_weighted.append(test_metrics[\"weighted avg\"][\"f1-score\"])\n",
    "            test_f1_neg.append(test_metrics[\"-1.0\"][\"f1-score\"])\n",
    "            test_f1_zero.append(test_metrics[\"0.0\"][\"f1-score\"])\n",
    "            test_f1_pos.append(test_metrics[\"1.0\"][\"f1-score\"])\n",
    "\n",
    "        results = { \"model\": model_name }\n",
    "        results.update(model_param)\n",
    "        results.update({\"val_f1_weighted\": np.mean(val_f1_weighted), \"val_f1_neg\": np.mean(val_f1_neg), \n",
    "                        \"val_f1_zero\": np.mean(val_f1_zero), \"val_f1_pos\": np.mean(val_f1_pos),\n",
    "                        \"val_accuracy\": np.mean(val_accuracy)})\n",
    "        results.update({\"test_f1_weighted\": np.mean(test_f1_weighted), \"test_f1_neg\": np.mean(test_f1_neg), \n",
    "                        \"test_f1_zero\": np.mean(test_f1_zero), \"test_f1_pos\": np.mean(test_f1_pos),\n",
    "                        \"test_accuracy\": np.mean(test_accuracy)})\n",
    "        print(results)\n",
    "        gridsearch_results.append(results)\n",
    "        ind += 1\n",
    "    return gridsearch_results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model_name = \"logreg\"\n",
    "model_fn = LogisticRegression\n",
    "model_paramgrid = logreg_paramgrid\n",
    "final_logreg_results = pd.DataFrame.from_records(models_grid_search(model_name,model_fn,model_paramgrid, data, meta_model_test))\n",
    "\n",
    "model_name = \"nb\"\n",
    "model_fn = MultinomialNB\n",
    "model_paramgrid = nb_paramgrid\n",
    "final_nb_results = pd.DataFrame.from_records(models_grid_search(model_name,model_fn,model_paramgrid, data, meta_model_test))\n",
    "\n",
    "model_name = \"svm\"\n",
    "model_fn = SVC\n",
    "model_paramgrid = svm_paramgrid\n",
    "final_svm_results = pd.DataFrame.from_records(models_grid_search(model_name,model_fn,model_paramgrid, data, meta_model_test))\n",
    "\n",
    "model_name = \"dummy\"\n",
    "model_fn = DummyClassifier\n",
    "model_paramgrid = dummy_paramgrid\n",
    "final_dummy_results = pd.DataFrame.from_records(models_grid_search(model_name,model_fn,model_paramgrid, data, meta_model_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_logreg_results = final_logreg_results.sort_values(by=[\"val_f1_weighted\", \"test_f1_weighted\"], ascending=False)\n",
    "final_nb_results = final_nb_results.sort_values(by=[\"val_f1_weighted\", \"test_f1_weighted\"], ascending=False)\n",
    "final_svm_results = final_svm_results.sort_values(by=[\"val_f1_weighted\", \"test_f1_weighted\"], ascending=False)\n",
    "final_dummy_results = final_dummy_results.sort_values(by=[\"val_f1_weighted\", \"test_f1_weighted\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model\n",
    "final_logreg_results.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_nb_results.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_svm_results.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dummy_results.iloc[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on all data & save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.concat([meta_model_train,meta_model_test],axis=0)\n",
    "alldata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model trained on all data\n",
    "import pickle\n",
    "best_param = {\"C\": final_logreg_results.iloc[0,:].to_dict()[\"C\"],\n",
    "\"class_weight\" : final_logreg_results.iloc[0,:].to_dict()[\"class_weight\"] , \"penalty\" : final_logreg_results.iloc[0,:].to_dict()[\"penalty\"], \"solver\" : final_logreg_results.iloc[0,:].to_dict()[\"solver\"]}\n",
    "final_model = LogisticRegression(**best_param)\n",
    "final_model.fit(alldata.iloc[:,:-1], alldata.label)\n",
    "\n",
    "model_pkl_filename = \"saved_models/model_meta.pkl\"\n",
    "with open(model_pkl_filename, 'wb') as file:\n",
    "    pickle.dump(final_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get F1-Score by Aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = meta_model_train.aspect.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_grid_search_aspect(model_name,train, test) : \n",
    "    gridsearch_results = []\n",
    "\n",
    "\n",
    "    # train on train_val model test on test\n",
    "    # best params determined previously\n",
    "    model = LogisticRegression(C=0.1,class_weight=\"balanced\",penalty=\"l2\", solver=\"lbfgs\")\n",
    "    model.fit(train.iloc[:,:-2], train.label)\n",
    "    test_pred = model.predict(test.iloc[:,:-2])\n",
    "    df= pd.DataFrame({\"Aspects\":test.aspect,\"Labels\":test.label,\"Predictions\":test_pred})    \n",
    "\n",
    "    print(\"Train on Training-Val (all folds) test on Test Data\")\n",
    "    for aspect in aspects:\n",
    "        print(f\"Aspect = {aspect}\")\n",
    "        test_label_aspect = df.loc[df.Aspects == aspect,\"Labels\"]\n",
    "        test_pred_aspect = df.loc[df.Aspects == aspect,\"Predictions\"]\n",
    "        print(classification_report(test_label_aspect, test_pred_aspect))\n",
    "\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"logreg\"\n",
    "output = models_grid_search_aspect(model_name, meta_model_train, meta_model_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "# define the mask to set the values in the upper triangle to True\n",
    "mask = np.triu(np.ones_like(alldata.corr(), dtype=np.bool))\n",
    "heatmap = sns.heatmap(alldata.corr(), mask=mask, vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation of Predictions of Base Models', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "final_model = pickle.load(open(\"saved_models/model_meta.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get importance\n",
    "importance = final_model.coef_\n",
    "classes = final_model.classes_\n",
    "features = alldata.columns.tolist()[:-1]\n",
    "features.append(\"intercept\")\n",
    "dictionary = {\"Class\":[],\"Feature\":[],\"Score\":[]}\n",
    "# summarize feature importance\n",
    "for j in classes:\n",
    "    print(f\"Class = {j}\")\n",
    "    print(final_model.intercept_[int(j)])\n",
    "    coeff = list(importance[int(j)]) + [final_model.intercept_[int(j)]]\n",
    "    print(f\"Intercept: {final_model.intercept_[int(j)]}\")\n",
    "    for i,v in zip(features,coeff):\n",
    "        dictionary[\"Class\"].append(j)\n",
    "        dictionary[\"Feature\"].append(i)\n",
    "        dictionary[\"Score\"].append(v)\n",
    "        print(f'Feature: {i}, Score: {v}')\n",
    "\n",
    "    print()\n",
    "feature_importance_dictionary = pd.DataFrame(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,10))\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "### CHANGE THIS PART ####\n",
    "plt.barh(features, importance[-1], height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in importance[-1]])\n",
    "plt.title('Class = -1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "########################## \n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,10))\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "plt.barh(features, importance[0], height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in importance[-1]])\n",
    "plt.title('Class = 0.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,10))\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "plt.barh(features, importance[1], height=0.8,\n",
    "         color=[\"#E3242B\" if x<0 else \"#00AB6B\" for x in importance[-1]])\n",
    "plt.title('Class = 1.0 top features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Weight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}