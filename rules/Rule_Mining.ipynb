{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize  \n",
    "import string\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aspects\n",
    "aspects_df = pd.read_csv(\"../data/rule_mining/final_aspects_terms.csv\")\n",
    "aspects_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aspect lists\n",
    "food_list = list(pd.read_csv(\"../utils/aspects/food.csv\")['food'].astype(str))\n",
    "time_list = list(pd.read_csv(\"../utils/aspects/time.csv\")['time'].astype(str))\n",
    "price_list = list(pd.read_csv(\"../utils/aspects/price.csv\")['price'].astype(str))\n",
    "portion_list = list(pd.read_csv(\"../utils/aspects/portion.csv\")['portion'].astype(str))\n",
    "service_list = list(pd.read_csv(\"../utils/aspects/service.csv\")['service'].astype(str))\n",
    "ambience_list = list(pd.read_csv(\"../utils/aspects/ambience.csv\")['ambience'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aspects = food_list + time_list + price_list + portion_list + service_list + ambience_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "description: get the unique synonyms for each aspect \n",
    "input: dataframe\n",
    "output: dataframe (saved as csv)\n",
    "'''\n",
    "def get_all_aspects(df) :\n",
    "    food = []\n",
    "    time = []\n",
    "    price = []\n",
    "    portion = []\n",
    "    service = []\n",
    "    ambience = []\n",
    "    for row in range(0, len(df)) :\n",
    "        # turn each aspects string into dictionary\n",
    "        row_aspects_dict = ast.literal_eval(df['aspects'][row])\n",
    "        # get the aspect keys\n",
    "        row_aspects_keys = list(row_aspects_dict.keys())\n",
    "        for key in row_aspects_keys :\n",
    "            if key == 'food' :\n",
    "                for value in row_aspects_dict['food'] :\n",
    "                    if value not in food :\n",
    "                        food.append(str(value))\n",
    "            elif key == 'time' :\n",
    "                for value in row_aspects_dict['time'] :\n",
    "                    if value not in time :\n",
    "                        time.append(str(value))\n",
    "            elif key == 'price' :\n",
    "                for value in row_aspects_dict['price'] :\n",
    "                    if value not in price :\n",
    "                        price.append(str(value))\n",
    "            elif key == 'portion' :\n",
    "                for value in row_aspects_dict['portion'] :\n",
    "                    if value not in portion :\n",
    "                        portion.append(str(value))\n",
    "            elif key == 'service' :\n",
    "                for value in row_aspects_dict['service'] :\n",
    "                    if value not in service :\n",
    "                        service.append(str(value))\n",
    "            elif key == 'ambience' :\n",
    "                for value in row_aspects_dict['ambience'] :\n",
    "                    if value not in ambience :\n",
    "                        ambience.append(str(value))\n",
    "        print(row, \"out of\", len(df), \"done\")\n",
    "    # create dataframe\n",
    "    food_df = pd.DataFrame({'food': food})\n",
    "    time_df = pd.DataFrame({'time': time})\n",
    "    price_df = pd.DataFrame({'price': price})\n",
    "    portion_df = pd.DataFrame({'portion': portion})\n",
    "    service_df = pd.DataFrame({'service': service})\n",
    "    ambience_df = pd.DataFrame({'ambience': ambience})\n",
    "    print(\"dataframes created\")\n",
    "    # create new folder\n",
    "    if not os.path.exists('./aspects') :\n",
    "        os.makedirs('./aspects')\n",
    "    # save as csv\n",
    "    food_df.to_csv(\"./aspects/food.csv\", index=False)\n",
    "    time_df.to_csv(\"./aspects/time.csv\", index=False)\n",
    "    price_df.to_csv(\"./aspects/price.csv\", index=False)\n",
    "    portion_df.to_csv(\"./aspects/portion.csv\", index=False)\n",
    "    service_df.to_csv(\"./aspects/service.csv\", index=False)\n",
    "    ambience_df.to_csv(\"./aspects/ambience.csv\", index=False)\n",
    "    print(\"dataframes saved\")\n",
    "    \n",
    "'''\n",
    "description: remove numbers, empty strings, new lines from phrases\n",
    "input: string\n",
    "output: string\n",
    "'''\n",
    "def pre_processing(review) : \n",
    "    # remove numbers\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    review = review.translate(remove_digits)\n",
    "    # remove new lines\n",
    "    review = review.replace('\\n', ' ')\n",
    "    return review\n",
    "\n",
    "'''\n",
    "description: get the text, pos and tag of each word\n",
    "input: text\n",
    "output: dataframe\n",
    "'''\n",
    "def get_pos(review) :\n",
    "    text = []\n",
    "    pos = []\n",
    "    tag = []\n",
    "    doc = nlp(review)\n",
    "    for token in doc:\n",
    "        text.append(token.text)\n",
    "        pos.append(token.pos_)\n",
    "        tag.append(token.tag_)\n",
    "    pos_df = pd.DataFrame({'text': text, 'pos': pos, 'tag': tag})\n",
    "    return pos_df\n",
    "\n",
    "'''\n",
    "description: get aspects\n",
    "input: string, list\n",
    "output: list, list\n",
    "'''\n",
    "def get_aspects(review, pos_df, aspect_list) :\n",
    "    # get aspects present in review\n",
    "    aspects = list(pos_df.loc[pos_df['text'].isin(aspect_list)]['text'])\n",
    "    # get sentences with aspects\n",
    "    aspect_sentences = []\n",
    "    # split into sentences\n",
    "    sentences = review.split('.')\n",
    "    for i in range(0, len(sentences)) :\n",
    "        if any(aspect in sentences[i] for aspect in aspect_list) :\n",
    "            aspect_sentences.append(sentences[i])\n",
    "    return aspects, aspect_sentences\n",
    "\n",
    "'''\n",
    "description: get the ranges\n",
    "input: list\n",
    "output: list\n",
    "'''\n",
    "def ranges(nums):\n",
    "    nums = sorted(set(nums))\n",
    "    gaps = [[s, e] for s, e in zip(nums, nums[1:]) if s+1 < e]\n",
    "    edges = iter(nums[:1] + sum(gaps, []) + nums[-1:])\n",
    "    return list(zip(edges, edges))\n",
    "\n",
    "'''\n",
    "description: get the text where first word is target_pos and last word is aspect and vice versa\n",
    "input: dataframe, list, str\n",
    "output: list\n",
    "'''\n",
    "def pos_before_after_aspect(pos_df, aspect_list, target_pos) :\n",
    "    start, end = 0, 0\n",
    "    sentences = []\n",
    "    positions_start = []\n",
    "    positions_end = []\n",
    "    filtered = []\n",
    "    aspect_index = list(pos_df.loc[pos_df['text'].isin(aspect_list)].index)\n",
    "    \n",
    "    # adjectives before\n",
    "    for i in range(0, len(aspect_index)) :\n",
    "        index = aspect_index[i]\n",
    "        for j in reversed(range(0, index)) :\n",
    "            if pos_df['pos'][j] == target_pos or pos_df['pos'][j] == 'VERB':\n",
    "                # check if there is an adv or det before then add that\n",
    "                if j>1 :\n",
    "                    if pos_df['pos'][j-1] == 'ADV' or pos_df['pos'][j-1] == 'DET' :\n",
    "                        positions_start.append(j-1)\n",
    "                        positions_end.append(index+1)\n",
    "                    else :\n",
    "                        positions_start.append(j)\n",
    "                        positions_end.append(index+1)\n",
    "                else :\n",
    "                    positions_start.append(j)\n",
    "                    positions_end.append(index+1)  \n",
    "\n",
    "    # adjectives after\n",
    "    for i in range(0, len(aspect_index)) :\n",
    "        index = aspect_index[i]\n",
    "        for j in range(index, len(pos_df)) :\n",
    "            if pos_df['pos'][j] == target_pos :\n",
    "                positions_start.append(index+1)\n",
    "                positions_end.append(j)\n",
    "            \n",
    "    positions_df = pd.DataFrame({'start': positions_start, 'end': positions_end})\n",
    "    positions_df = positions_df.drop_duplicates().reset_index(drop=True)\n",
    "    positions_df = positions_df.sort_values(by=['start', 'end'])\n",
    "    filtered_df_1 = positions_df.drop_duplicates(subset=['end'], keep='last').reset_index(drop=True)\n",
    "    filtered_df = filtered_df_1.drop_duplicates(subset=['start'], keep='last').reset_index(drop=True)\n",
    "    \n",
    "    for i in range(0, len(filtered_df)) :\n",
    "        start = filtered_df['start'][i]\n",
    "        end = filtered_df['end'][i]\n",
    "        if start < end and start!=0 and end!=0:\n",
    "            review = pos_df['text'][start:end+1].apply(lambda x:x + ' ').sum()\n",
    "            if '.' in review :\n",
    "                review = review.split(\".\")[0]\n",
    "            elif '!' in review :\n",
    "                review = review.split(\"!\")[0]\n",
    "            elif '?' in review :\n",
    "                review = review.split(\"?\")[0]\n",
    "            if len(review) > 0 :\n",
    "                filtered.append(review)\n",
    "    return filtered\n",
    "\n",
    "'''\n",
    "description: get the start and end indexes of the sentence\n",
    "input: string, dataframe\n",
    "output: dataframe\n",
    "'''\n",
    "def get_sentence_indexes(sentence, pos_df) :\n",
    "    sentence_split = sentence.split(\" \")\n",
    "    target_pos = pos_df.loc[pos_df['text'].isin(sentence_split)]\n",
    "    target_index = list(target_pos.index)\n",
    "    ranges_list = ranges(target_index)\n",
    "    for i in ranges_list :\n",
    "        if i[0] != i[1] :\n",
    "            start = i[0]\n",
    "            end = i[1] + 1\n",
    "            return pos_df[start:end]\n",
    "\n",
    "def get_sentences_indexes(sentence, pos_df) :\n",
    "    all_indexes = pd.DataFrame({'text':[], 'pos':[],'tag':[]})\n",
    "    for i in range(0, len(sentence)) :\n",
    "        pos_df_target = get_sentence_indexes(sentence[i], pos_df)\n",
    "        all_indexes = pd.concat([all_indexes, pos_df_target])\n",
    "    return all_indexes\n",
    "        \n",
    "'''\n",
    "description: get the sentences from indexes\n",
    "input: dataframe, dataframe\n",
    "output: list\n",
    "'''\n",
    "def get_sentences(pos_df, pos_df_original) :\n",
    "    pos_df_phrases = []\n",
    "    pos_df_index = list(pos_df.index)\n",
    "    ranges_list = ranges(pos_df_index)\n",
    "    for i in ranges_list :\n",
    "        if i[0] != i[1] :\n",
    "            start = i[0]\n",
    "            end = i[1] + 1\n",
    "            pos_df_phrases.append(pos_df_original['text'][start:end].apply(lambda x:x + ' ').sum())\n",
    "    return pos_df_phrases\n",
    "\n",
    "'''\n",
    "description: remove nouns, stopwords and punctuations from phrases\n",
    "input: list, list\n",
    "output: list\n",
    "'''\n",
    "\n",
    "def post_processing(phrase_list, to_remove) : \n",
    "    new_phrase_list = []\n",
    "    for phrase in phrase_list :\n",
    "        pos_df = get_pos(phrase)\n",
    "        nouns = list(pos_df.loc[pos_df['pos']=='NOUN']['text'])\n",
    "        #to_remove += nouns\n",
    "        word_tokens = word_tokenize(phrase)\n",
    "        new_phrase_list.append([w for w in word_tokens if not w in to_remove])\n",
    "    return new_phrase_list\n",
    "\n",
    "'''\n",
    "description: apply functions to aspect\n",
    "input: str, dataframe, list\n",
    "output: list\n",
    "'''\n",
    "def process_review_aspect(review, pos_df, aspect_list, to_remove) :\n",
    "    aspects, sentence = get_aspects(review, pos_df, aspect_list)\n",
    "    sentence_new = pos_before_after_aspect(pos_df, aspects, 'ADJ')\n",
    "    aspect_pos = get_sentences_indexes(sentence_new, pos_df)\n",
    "    aspect_pos = aspect_pos.sort_index().drop_duplicates()\n",
    "    phrase_list = get_sentences(aspect_pos, pos_df)\n",
    "    phrase_list_cleaned = post_processing(phrase_list, to_remove)\n",
    "    return phrase_list_cleaned\n",
    "\n",
    "'''\n",
    "description: add phrase_no_noun and phrase_no_aspect\n",
    "input: dataframe, list\n",
    "output: dataframe\n",
    "'''\n",
    "def add_phrases(df, all_aspects) :\n",
    "    new_phrase_no_aspect = []\n",
    "    new_phrase_no_noun = []\n",
    "    for i in range(0, len(df)) :\n",
    "        no_aspect = []\n",
    "        no_noun = []\n",
    "        phrase_list = df['phrase'][i].split(', ')\n",
    "        for phrase in phrase_list:\n",
    "            if phrase not in all_aspects :\n",
    "                no_aspect.append(phrase)\n",
    "            if get_pos(phrase)['pos'][0] != 'NOUN' :\n",
    "                no_noun.append(phrase)\n",
    "        new_phrase_no_aspect.append(no_aspect)\n",
    "        new_phrase_no_noun.append(no_noun)\n",
    "    df['phrase_no_aspect'] = new_phrase_no_aspect\n",
    "    df['phrase_no_noun'] = new_phrase_no_noun\n",
    "    return df\n",
    "\n",
    "'''\n",
    "description: apply functions to review\n",
    "input: path to dataframe\n",
    "output: dataframe\n",
    "'''\n",
    "def process_reviews(df_path) :\n",
    "    \n",
    "    # for post-processing\n",
    "    stop_words_to_remove = set(stopwords.words('english'))\n",
    "    # negated terms to not remove\n",
    "    stop_words_dont_remove = set(['no', 'not', 'nor']) \n",
    "    # stop_words_dont_remove = set(['no', 'not', 'nor', 'don', \"don't\", 'should', \"should've\", 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]) \n",
    "    stop_words = list(stop_words_to_remove-stop_words_dont_remove)\n",
    "    punctuations = list(string.punctuation)\n",
    "    to_remove = stop_words + punctuations + [\"'s\"]\n",
    "    \n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    food_phrases = []\n",
    "    time_phrases = []\n",
    "    price_phrases = []\n",
    "    portion_phrases = []\n",
    "    service_phrases = []\n",
    "    ambience_phrases = []\n",
    "    \n",
    "    aspect_list = []\n",
    "    phrase_list = []\n",
    "    restaurant_code = []\n",
    "    review_title = []\n",
    "    review_body = []\n",
    "    account_name = []\n",
    "    account_id = []\n",
    "    \n",
    "    for i in range(0, len(df)) :\n",
    "        \n",
    "        review = (str(df['review_title'][i])+str(df['review_body'][i])).lower()\n",
    "        # for pre-processing\n",
    "        review = pre_processing(review)\n",
    "        pos_df = get_pos(review)\n",
    "        \n",
    "        # get phrases for each aspect\n",
    "        food_phrases = process_review_aspect(review, pos_df, food_list, to_remove)\n",
    "        time_phrases = process_review_aspect(review, pos_df, time_list, to_remove)\n",
    "        price_phrases = process_review_aspect(review, pos_df, price_list, to_remove)\n",
    "        portion_phrases = process_review_aspect(review, pos_df, portion_list, to_remove)\n",
    "        service_phrases = process_review_aspect(review, pos_df, service_list, to_remove)\n",
    "        ambience_phrases = process_review_aspect(review, pos_df, ambience_list, to_remove)\n",
    "\n",
    "        # add aspects together\n",
    "        if len(food_phrases) > 0 :\n",
    "            for j in range(0, len(food_phrases)) :\n",
    "                aspect_list.append('food')\n",
    "                phrase_list.append(food_phrases[j])\n",
    "                restaurant_code.append(df['restaurant_code'][i])\n",
    "                review_title.append(df['review_title'][i])\n",
    "                review_body.append(df['review_body'][i])\n",
    "                account_name.append(df['account_name'][i])\n",
    "                account_id.append(df['account_id'][i])\n",
    "        if len(time_phrases) > 0 :\n",
    "            for j in range(0, len(time_phrases)) :\n",
    "                aspect_list.append('time')\n",
    "                phrase_list.append(time_phrases[j])\n",
    "                restaurant_code.append(df['restaurant_code'][i])\n",
    "                review_title.append(df['review_title'][i])\n",
    "                review_body.append(df['review_body'][i])\n",
    "                account_name.append(df['account_name'][i])\n",
    "                account_id.append(df['account_id'][i])\n",
    "        if len(price_phrases) > 0 :\n",
    "            for j in range(0, len(price_phrases)) :\n",
    "                aspect_list.append('price')\n",
    "                phrase_list.append(price_phrases[j])\n",
    "                restaurant_code.append(df['restaurant_code'][i])\n",
    "                review_title.append(df['review_title'][i])\n",
    "                review_body.append(df['review_body'][i])\n",
    "                account_name.append(df['account_name'][i])\n",
    "                account_id.append(df['account_id'][i])\n",
    "        if len(portion_phrases) > 0 :\n",
    "            for j in range(0, len(portion_phrases)) :\n",
    "                aspect_list.append('portion')\n",
    "                phrase_list.append(portion_phrases[j])\n",
    "                restaurant_code.append(df['restaurant_code'][i])\n",
    "                review_title.append(df['review_title'][i])\n",
    "                review_body.append(df['review_body'][i])\n",
    "                account_name.append(df['account_name'][i])\n",
    "                account_id.append(df['account_id'][i])\n",
    "        if len(service_phrases) > 0 :\n",
    "            for j in range(0, len(service_phrases)) :\n",
    "                aspect_list.append('service')\n",
    "                phrase_list.append(service_phrases[j])\n",
    "                restaurant_code.append(df['restaurant_code'][i])\n",
    "                review_title.append(df['review_title'][i])\n",
    "                review_body.append(df['review_body'][i])\n",
    "                account_name.append(df['account_name'][i])\n",
    "                account_id.append(df['account_id'][i])\n",
    "        if len(ambience_phrases) > 0 :\n",
    "            for j in range(0, len(ambience_phrases)) :\n",
    "                aspect_list.append('ambience')\n",
    "                phrase_list.append(ambience_phrases[j])\n",
    "                restaurant_code.append(df['restaurant_code'][i])\n",
    "                review_title.append(df['review_title'][i])\n",
    "                review_body.append(df['review_body'][i])\n",
    "                account_name.append(df['account_name'][i])\n",
    "                account_id.append(df['account_id'][i])\n",
    "\n",
    "        #print(i, \"out of\", len(df), \"done\")\n",
    "    \n",
    "    # create df\n",
    "    output = pd.DataFrame({'restaurant_code': restaurant_code, 'review_title': review_title, 'review_body': review_body, 'account_name': account_name, 'account_id': account_id, 'aspect': aspect_list, 'phrase': phrase_list})\n",
    "    # remove empty phrases\n",
    "    output = output.loc[output['phrase'].apply(lambda x: len(x)>0)]\n",
    "    # set phrases to str\n",
    "    output['phrase'] = output['phrase'].apply(lambda x: \", \".join(x))\n",
    "    # remove duplicated rows\n",
    "    output = output.drop_duplicates().reset_index(drop=True)\n",
    "    # add phrase_no_noun and phrase_no_aspect\n",
    "    output = add_phrases(output, all_aspects)\n",
    "    \n",
    "    # save as csv\n",
    "    path_split = df_path.split(\"/\")\n",
    "    new_path = \"/\".join(path_split[:-1]) + \"/unlabelled/unlabelled_\" + path_split[-1]\n",
    "    output.to_csv(new_path)\n",
    "    \n",
    "    print(\"saved to\", new_path)\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_reviews(\"./data/train_test/reviews_test_subset.csv\")\n",
    "process_reviews(\"./data/train_test/reviews_train_subset.csv\")\n",
    "process_reviews(\"./data/train_test/reviews_validation_subset.csv\")\n",
    "process_reviews(\"./data/train_test/test_negative.csv\")\n",
    "process_reviews(\"./data/train_test/train_negative.csv\")\n",
    "process_reviews(\"./data/train_test/val_negative.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./data/train_test/unlabelled/unlabelled_reviews_test_subset.csv\")\n",
    "train = pd.read_csv(\"./data/train_test/unlabelled/unlabelled_reviews_train_subset.csv\")\n",
    "val = pd.read_csv(\"./data/train_test/unlabelled/unlabelled_reviews_validation_subset.csv\")\n",
    "test_neg = pd.read_csv(\"./data/train_test/unlabelled/unlabelled_test_negative.csv\")\n",
    "train_neg = pd.read_csv(\"./data/train_test/unlabelled/unlabelled_train_negative.csv\")\n",
    "val_neg = pd.read_csv(\"./data/train_test/unlabelled/unlabelled_val_negative.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test', len(test))\n",
    "print('train', len(train))\n",
    "print('val', len(val))\n",
    "print('test neg', len(test_neg))\n",
    "print('train neg', len(train_neg))\n",
    "print('val neg', len(val_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_first = train[:3108]\n",
    "train_second = train[3108:]\n",
    "extra_neg = pd.concat([test_neg, train_neg, val_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"./data/train_test/to_label/test_subset.csv\", index=False)\n",
    "train_first.to_csv(\"./data/train_test/to_label/train_first_subset.csv\", index=False)\n",
    "train_second.to_csv(\"./data/train_test/to_label/train_second_subset.csv\", index=False)\n",
    "val.to_csv(\"./data/train_test/to_label/val_subset.csv\", index=False)\n",
    "extra_neg.to_csv(\"./data/train_test/to_label/extra_neg.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
